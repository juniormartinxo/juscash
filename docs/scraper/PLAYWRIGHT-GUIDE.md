# DJE Scraper com Playwright

Scraper completo implementado do zero usando Playwright para extrair informa√ß√µes de publica√ß√µes de RPV e pagamentos pelo INSS do Di√°rio da Justi√ßa Eletr√¥nico de S√£o Paulo (DJE-SP).

## üöÄ Funcionalidades

- ‚úÖ Scraping por per√≠odo de datas (data in√≠cio at√© data fim)
- ‚úÖ Busca autom√°tica por termos "RPV" e "pagamento pelo INSS"
- ‚úÖ Download e processamento de PDFs das publica√ß√µes
- ‚úÖ Extra√ß√£o autom√°tica de n√∫meros de processo dos PDFs
- ‚úÖ Consulta autom√°tica no ESAJ para dados complementares
- ‚úÖ Extra√ß√£o de advogados, valores monet√°rios e datas
- ‚úÖ Sanitiza√ß√£o de conte√∫do para prevenir SQL injection
- ‚úÖ Salvamento autom√°tico em arquivos JSON
- ‚úÖ Limpeza autom√°tica de arquivos tempor√°rios

## üìã Pr√©-requisitos

- Python 3.8+
- Pip para instala√ß√£o de depend√™ncias

## üîß Instala√ß√£o

1. **Execute o script de instala√ß√£o:**
```bash
cd backend/scraper
chmod +x install-scraper.sh
./install-scraper.sh
```

2. **Ou instale manualmente:**
```bash
pip install -r requirements-scraper.txt
playwright install chromium
```

## üìñ Como usar

### Comando b√°sico
```bash
python scraping.py run --start-date YYYY-MM-DD --end-date YYYY-MM-DD
```

### Op√ß√µes dispon√≠veis
- `--start-date`: Data de in√≠cio no formato YYYY-MM-DD
- `--end-date`: Data de fim no formato YYYY-MM-DD  
- `--headless`: For√ßa execu√ß√£o sem interface gr√°fica (padr√£o em Docker)
- `--no-headless`: For√ßa execu√ß√£o com interface gr√°fica (apenas fora do Docker)

### Exemplos

**Scraping de um √∫nico dia (modo autom√°tico):**
```bash
python scraping.py run --start-date 2025-01-15 --end-date 2025-01-15
```

**Scraping de um per√≠odo:**
```bash
python scraping.py run --start-date 2025-01-15 --end-date 2025-01-20
```

**Scraping da semana passada:**
```bash
python scraping.py run --start-date 2025-01-08 --end-date 2025-01-14
```

**For√ßar modo headless:**
```bash
python scraping.py run --start-date 2025-01-15 --end-date 2025-01-15 --headless
```

**For√ßar interface gr√°fica (apenas fora do Docker):**
```bash
python scraping.py run --start-date 2025-01-15 --end-date 2025-01-15 --no-headless
```

## üîÑ Fluxo de Execu√ß√£o

Para cada data no per√≠odo informado, o scraper executa:

1. **Acesso ao DJE-SP** - Vai para a p√°gina de busca avan√ßada
2. **Preenchimento do formul√°rio** - Define datas usando o padr√£o do projeto (JavaScript com eventos)
3. **Sele√ß√£o de caderno** - Detecta automaticamente op√ß√µes dispon√≠veis
4. **Busca de publica√ß√µes** - Executa a pesquisa com termos "RPV" e "pagamento pelo INSS"
5. **Download de PDFs** - Baixa cada publica√ß√£o encontrada
6. **Extra√ß√£o de dados** - Retira n√∫mero do processo e autores do PDF
7. **Consulta no ESAJ** - Busca informa√ß√µes complementares
8. **Extra√ß√£o completa** - Obt√©m advogados, valores e datas
9. **Salvamento** - Cria arquivo JSON para cada publica√ß√£o

## üìÑ Dados Extra√≠dos

Para cada publica√ß√£o, o scraper extrai:

- **process_number** - N√∫mero do processo (ex: 0003901-40.2025.8.26.0053)
- **publication_date** - Data de publica√ß√£o (do ESAJ)
- **availability_date** - Data de disponibiliza√ß√£o (da busca)
- **authors** - Lista de autores (do PDF)
- **lawyers** - Lista de advogados com nome e OAB (do ESAJ)
- **gross_value** - Valor bruto em centavos (do ESAJ)
- **net_value** - Valor l√≠quido em centavos (do ESAJ)
- **interest_value** - Valor dos juros em centavos (do ESAJ)
- **attorney_fees** - Honor√°rios advocat√≠cios em centavos (do ESAJ)
- **content** - Conte√∫do completo sanitizado (do ESAJ)

## üìÅ Arquivos de Sa√≠da

Os arquivos JSON s√£o salvos em `backend/scraper/reports/json/` com o nome baseado no n√∫mero do processo:

```
0003901-40_2025_8_26_0053.json
0004123-55_2025_8_26_0100.json
```

Cada arquivo cont√©m todos os dados estruturados da publica√ß√£o.

## üîç Exemplo de Sa√≠da JSON

```json
{
  "process_number": "0003901-40.2025.8.26.0053",
  "publication_date": "2025-01-15",
  "availability_date": "2025-01-16",
  "authors": ["Antonio Elias dos Santos"],
  "defendant": "Instituto Nacional do Seguro Social - INSS",
  "lawyers": [
    {
      "name": "Vladimir Renato de Aquino Lopes",
      "oab": "94932/SP"
    }
  ],
  "gross_value": 1242337,
  "net_value": 1242337,
  "interest_value": 808905,
  "attorney_fees": 215976,
  "content": "Conte√∫do completo da movimenta√ß√£o sanitizado...",
  "status": "NOVA",
  "scraping_source": "DJE-SP",
  "caderno": "12",
  "instancia": "1",
  "local": "Capital",
  "parte": "1"
}
```

## ‚öôÔ∏è Configura√ß√µes

### Browser
- **Modo autom√°tico**: Detecta automaticamente se deve executar em modo headless
- **Docker**: Sempre executa em modo headless (sem interface gr√°fica)
- **Ambiente local**: Por padr√£o executa em modo headless
- **For√ßar modo**: Use `--headless` ou `--no-headless` para controlar manualmente

### Detec√ß√£o autom√°tica de ambiente
O scraper detecta automaticamente:
- Se est√° executando em container Docker (arquivo `/.dockerenv`)
- Se h√° servidor X dispon√≠vel (vari√°vel `$DISPLAY`)
- Se est√° em ambiente CI/CD
- For√ßa modo headless nesses cen√°rios

### Timeouts
- Timeout padr√£o: 30 segundos
- Pause entre datas: 2 segundos
- Aguardo ap√≥s cliques: 2-3 segundos

### Arquivos Tempor√°rios
- PDFs s√£o baixados em diret√≥rio tempor√°rio
- Limpeza autom√°tica ao final da execu√ß√£o

## üõ†Ô∏è Manuten√ß√£o

### Logs
O scraper gera logs detalhados mostrando:
- Progresso do processamento
- Erros encontrados
- Estat√≠sticas de cada dia

### Monitoramento
Ao final da execu√ß√£o, s√£o exibidas estat√≠sticas:
- Dias processados
- Publica√ß√µes encontradas
- Publica√ß√µes salvas com sucesso
- Falhas ocorridas

### Tratamento de Erros
- Continua execu√ß√£o mesmo com falhas pontuais
- Registra todos os erros para an√°lise
- Limpa recursos automaticamente

## üêõ Solu√ß√£o de Problemas

Para problemas espec√≠ficos, consulte o [**Guia de Troubleshooting**](TROUBLESHOOTING.md).

### Debug r√°pido do campo caderno:
```bash
python debug-caderno.py
```

### Problemas comuns:

**Erro "playwright not found":**
```bash
playwright install chromium
```

**Erro "PyPDF2 not found":**
```bash
pip install PyPDF2==3.0.1
```

**Timeout na sele√ß√£o do caderno:**
- Execute `python debug-caderno.py` para ver op√ß√µes dispon√≠veis
- O scraper agora detecta automaticamente as op√ß√µes corretas
- Verifique logs para ver qual caderno foi selecionado

**Timeout nos sites:**
- Verifique conex√£o com internet
- Sites podem estar temporariamente lentos
- Tente novamente mais tarde

**PDFs n√£o encontrados:**
- Pode n√£o haver publica√ß√µes para a data
- Verifique se os termos de busca est√£o corretos
- Execute debug para verificar se a busca est√° funcionando

**Erro de permiss√£o:**
```bash
chmod +x scraping.py
chmod +x install-scraper.sh
chmod +x debug-caderno.py
```

## üìù Notas Importantes

1. **Compatibilidade**: Implementado seguindo os padr√µes j√° existentes no projeto (dje_scraper_adapter.py)
2. **Responsabilidade**: Use o scraper de forma respons√°vel, respeitando os termos de uso dos sites
3. **Rate Limiting**: O scraper inclui pausas para n√£o sobrecarregar os servidores
4. **Backup**: Os arquivos JSON s√£o a √∫nica persist√™ncia dos dados
5. **Atualiza√ß√£o**: Atualize o Playwright regularmente para manter compatibilidade

## üîó Integra√ß√£o com o Projeto

Este scraper usa os mesmos padr√µes de:
- Manipula√ß√£o de datas via JavaScript com eventos
- Estrutura de entidades (Publication, Lawyer, MonetaryValue)
- Sistema de logs da infraestrutura
- Formato JSON compat√≠vel com ReportJsonSaver 