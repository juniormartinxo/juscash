# Multi-Date Multi-Worker Scraper para DJE-SP

Este sistema permite buscar publicaÃ§Ãµes do DJE-SP em mÃºltiplas datas de forma concorrente, com controle de progresso e capacidade de retomar execuÃ§Ãµes interrompidas.

## ğŸ“‹ Funcionalidades

- âœ… **Busca em mÃºltiplas datas**: De 17/03/2025 atÃ© hoje
- âœ… **Multi-worker**: 3 workers processando datas simultaneamente
- âœ… **Controle de progresso**: Salva progresso em arquivo JSON
- âœ… **Retomada automÃ¡tica**: Continua de onde parou se interrompido
- âœ… **Tratamento de erros**: Retry automÃ¡tico para falhas temporÃ¡rias
- âœ… **Monitoramento**: Scripts para acompanhar o progresso em tempo real

## ğŸš€ Como Usar

### 1. Executar o Scraper

```bash
# Navegar para o diretÃ³rio do scraper
cd backend/scraper

# Executar o scraper multi-data
python run_multi_date_scraper.py
```

### 2. Monitorar Progresso

Em outro terminal:

```bash
# Monitor contÃ­nuo (atualiza a cada 30 segundos)
python monitor_progress.py

# Monitor com intervalo personalizado (60 segundos)
python monitor_progress.py --interval 60

# VerificaÃ§Ã£o Ãºnica (sem loop)
python monitor_progress.py --once

# Monitor com arquivo personalizado
python monitor_progress.py --file /caminho/para/arquivo.json
```

### 3. Parar e Retomar

Para parar o scraper:

- Pressione `Ctrl+C` para shutdown graceful
- O progresso serÃ¡ salvo automaticamente

Para retomar:

- Execute novamente o script `run_multi_date_scraper.py`
- O sistema detectarÃ¡ automaticamente onde parou e continuarÃ¡

## ğŸ“ Estrutura dos Arquivos

### Arquivo de Progresso (`backend/scraper/src/scrap_workrs.json`)

```json
{
  "metadata": {
    "start_date": "17/03/2025",
    "end_date": "17/06/2025",
    "num_workers": 3,
    "last_updated": "2025-06-17T14:30:00",
    "total_dates": 92,
    "processed_dates": 15,
    "total_publications": 1250
  },
  "dates": {
    "17/03/2025": {
      "date": "17/03/2025",
      "processed": true,
      "worker_id": "worker_1",
      "start_time": "2025-06-17T14:00:00",
      "end_time": "2025-06-17T14:05:00",
      "publications_found": 85,
      "error": null,
      "retry_count": 0
    }
  },
  "workers": {
    "worker_1": {
      "worker_id": "worker_1",
      "current_date": "18/03/2025",
      "dates_processed": 5,
      "total_publications": 425,
      "status": "working"
    }
  }
}
```

### Estrutura de Status

#### Status de Data

- `processed`: `true` se a data foi processada com sucesso
- `worker_id`: ID do worker que processou a data
- `start_time`/`end_time`: Timestamps de inÃ­cio e fim do processamento
- `publications_found`: NÃºmero de publicaÃ§Ãµes encontradas
- `error`: Mensagem de erro (se houver)
- `retry_count`: NÃºmero de tentativas realizadas

#### Status de Worker

- `idle`: Worker aguardando nova tarefa
- `working`: Worker processando uma data
- `completed`: Worker finalizou todas as tarefas
- `error`: Worker teve erro crÃ­tico

## âš™ï¸ ConfiguraÃ§Ã£o

### ParÃ¢metros Principais

Os parÃ¢metros podem ser modificados no arquivo `multi_date_scraper.py`:

```python
# ParÃ¢metros de entrada
START_DATE = "17/03/2025"        # Data inicial
END_DATE = "NOW"                 # Data final (NOW = hoje)
JSON_FILE_PATH = "src/scrap_workrs.json"  # Arquivo de progresso
NUM_WORKERS = 3                  # NÃºmero de workers
```

### ConfiguraÃ§Ãµes do Scraper

As configuraÃ§Ãµes do scraper estÃ£o em `src/infrastructure/config/settings.py`:

```python
class ScraperSettings:
    max_retries: int = 3           # Tentativas por pÃ¡gina
    retry_delay: int = 5           # Delay entre tentativas
    max_pages: int = 20            # PÃ¡ginas mÃ¡ximas por data
    search_terms: List[str] = ["RPV", "pagamento pelo INSS"]
```

## ğŸ› ï¸ Scripts DisponÃ­veis

### 1. `multi_date_scraper.py`

Script principal com a implementaÃ§Ã£o completa do sistema multi-worker.

### 2. `run_multi_date_scraper.py`

Script executÃ¡vel simples para iniciar o scraping.

### 3. `monitor_progress.py`

Monitor de progresso com interface de linha de comando.

## ğŸ“Š Monitoramento

### Exemplo de SaÃ­da do Monitor

```
================================================================================
ğŸ•·ï¸  MONITOR DE PROGRESSO - MULTI-DATE SCRAPER
================================================================================

ğŸ“Š RESUMO GERAL:
----------------------------------------
ğŸ“… PerÃ­odo: 17/03/2025 atÃ© 17/06/2025
ğŸ‘¥ Workers: 3
ğŸ”„ Ãšltima atualizaÃ§Ã£o: 2025-06-17T14:30:00
ğŸ“ˆ Total de datas: 92
âœ… Datas processadas: 15
ğŸ“„ Total de publicaÃ§Ãµes: 1250
ğŸ¯ Progresso: 16.3%

ğŸ‘¥ STATUS DOS WORKERS:
----------------------------------------
ğŸ”¥ worker_1: working
   ğŸ“… Data atual: 18/03/2025
   âœ… Datas processadas: 5
   ğŸ“„ PublicaÃ§Ãµes: 425

ğŸ˜´ worker_2: idle
   ğŸ“… Data atual: N/A
   âœ… Datas processadas: 4
   ğŸ“„ PublicaÃ§Ãµes: 320

ğŸ”¥ worker_3: working
   ğŸ“… Data atual: 20/03/2025
   âœ… Datas processadas: 6
   ğŸ“„ PublicaÃ§Ãµes: 505

ğŸ“… ÃšLTIMAS 5 DATAS PROCESSADAS:
----------------------------------------
ğŸ“… 19/03/2025 | ğŸ‘· worker_2 | ğŸ“„ 75 pub | â±ï¸ 4m32s
ğŸ“… 17/03/2025 | ğŸ‘· worker_1 | ğŸ“„ 85 pub | â±ï¸ 5m15s
ğŸ“… 21/03/2025 | ğŸ‘· worker_3 | ğŸ“„ 92 pub | â±ï¸ 3m45s
```

## ğŸ”§ Tratamento de Erros

### Tipos de Erro

1. **Erros de ConexÃ£o**: Timeout, DNS, etc.
2. **Erros de Scraping**: Estrutura da pÃ¡gina mudou
3. **Erros de Sistema**: MemÃ³ria, disco, etc.

### EstratÃ©gias de RecuperaÃ§Ã£o

1. **Retry AutomÃ¡tico**: AtÃ© 3 tentativas por data
2. **Backoff Exponencial**: Delay aumenta a cada tentativa
3. **Isolamento de Erros**: Erro em uma data nÃ£o afeta outras
4. **Logging Detalhado**: Todos os erros sÃ£o registrados

### Logs

Os logs sÃ£o salvos em:

- `src/logs/scraper.log`: Log principal
- `src/logs/error.log`: Logs de erro
- `reports/json/`: PublicaÃ§Ãµes extraÃ­das

## ğŸš¨ LimitaÃ§Ãµes e ConsideraÃ§Ãµes

### Rate Limiting

- Delay de 2 segundos entre pÃ¡ginas
- Delay de 1 segundo entre requests
- MÃ¡ximo de 3 workers simultÃ¢neos

### Recursos do Sistema

- Cada worker usa um browser Chrome
- Consumo de memÃ³ria: ~200MB por worker
- Uso de CPU: Moderado

### Dados ExtraÃ­dos

- PublicaÃ§Ãµes sÃ£o salvas em arquivos JSON
- Backup automÃ¡tico Ã© mantido
- NÃ£o hÃ¡ verificaÃ§Ã£o de duplicatas local (feita na API)

## ğŸ“ Logs e Debugging

### NÃ­veis de Log

- `INFO`: Progresso normal
- `WARNING`: SituaÃ§Ãµes inesperadas
- `ERROR`: Falhas que impedem o processamento
- `DEBUG`: InformaÃ§Ãµes detalhadas (apenas em modo debug)

### Debugging

Para ativar modo debug, modifique em `settings.py`:

```python
enable_debug: bool = True
debug_screenshot_on_error: bool = True
```

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

1. **InicializaÃ§Ã£o**
   - Carrega ou cria arquivo de progresso
   - Inicializa container e configuraÃ§Ãµes
   - Popula fila de datas nÃ£o processadas

2. **ExecuÃ§Ã£o**
   - Workers processam datas da fila
   - Cada data Ã© processada independentemente
   - Progresso Ã© salvo apÃ³s cada data

3. **FinalizaÃ§Ã£o**
   - Shutdown graceful salva progresso final
   - Cleanup de recursos
   - RelatÃ³rio final

## ğŸ†˜ SoluÃ§Ã£o de Problemas

### Scraper NÃ£o Inicia

- Verificar dependÃªncias: `pip install -r requirements.txt`
- Verificar permissÃµes do arquivo de progresso
- Verificar logs para erros de inicializaÃ§Ã£o

### Workers Param ou Falham

- Verificar memÃ³ria disponÃ­vel
- Verificar conexÃ£o com internet
- Verificar se o site DJE estÃ¡ acessÃ­vel

### Progresso NÃ£o Salva

- Verificar permissÃµes de escrita
- Verificar espaÃ§o em disco
- Verificar se o arquivo nÃ£o estÃ¡ bloqueado

### Performance Lenta

- Reduzir nÃºmero de workers
- Aumentar delays entre requests
- Verificar recursos do sistema

## ğŸ“ Suporte

Para problemas ou dÃºvidas:

1. Verificar logs em `src/logs/`
2. Executar monitor para verificar status
3. Verificar arquivo de progresso
4. Consultar esta documentaÃ§Ã£o

---

**Desenvolvido para o projeto JusCash - Sistema de Scraping DJE-SP**
