# üï∑Ô∏è DJE Scraper - Guia Completo de Implementa√ß√£o

## üìã Vis√£o Geral do Projeto

O **DJE Scraper** √© um sistema completo de automa√ß√£o para extrair publica√ß√µes do Di√°rio da Justi√ßa Eletr√¥nico (DJE) de S√£o Paulo, desenvolvido seguindo **Arquitetura Hexagonal** em Python 3.12+. O sistema integra-se perfeitamente com a API JusCash existente para processar publica√ß√µes relacionadas ao INSS.

### üéØ Objetivos Principais

- ‚úÖ **Automa√ß√£o Di√°ria**: Extra√ß√£o autom√°tica a partir de 17/03/2025
- ‚úÖ **Busca Inteligente**: Filtragem por termos obrigat√≥rios (aposentadoria, benef√≠cio, INSS)
- ‚úÖ **Integra√ß√£o API**: Envio via endpoint `/api/scraper/publications`
- ‚úÖ **Arquitetura Robusta**: Hexagonal com separa√ß√£o clara de responsabilidades
- ‚úÖ **Monitoramento Completo**: Logs, alertas, m√©tricas e health checks
- ‚úÖ **Produ√ß√£o Ready**: Docker, systemd, backup autom√°tico

---

## üèóÔ∏è Arquitetura do Sistema

```txt
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   DJE Website   ‚îÇ     ‚îÇ   Scraper    ‚îÇ     ‚îÇ   API JusCash   ‚îÇ
‚îÇ   (Target)      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (Python)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (Node.js)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ                       ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
                        ‚îÇ   Database   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ (PostgreSQL) ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üß© Componentes Principais

1. **Domain Layer** - Entidades e regras de neg√≥cio
2. **Application Layer** - Use cases e orquestradores
3. **Infrastructure Layer** - Adapters para tecnologias externas
4. **Shared Kernel** - Utilit√°rios e container de depend√™ncias

---

## üìÅ Estrutura Completa do Projeto

```txt
juscash-dje-system/
‚îú‚îÄ‚îÄ üìÑ README.md
‚îú‚îÄ‚îÄ üìÑ docker-compose.yml
‚îú‚îÄ‚îÄ üìÑ .env
‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îú‚îÄ‚îÄ üìÑ scraper_cli.py
‚îú‚îÄ‚îÄ üìÑ production-setup.sh
‚îú‚îÄ‚îÄ üìÑ deployment-check.sh
‚îú‚îÄ‚îÄ üìÑ final-integration-test.sh
‚îÇ
‚îú‚îÄ‚îÄ üìÇ src/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ domain/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ entities/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ publication.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ scraping_execution.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ ports/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ scraping_repository.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ web_scraper.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ services/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ publication_validator.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ application/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ usecases/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ extract_publications.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ save_publications.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ validate_extracted_data.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ services/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ scraping_orchestrator.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ infrastructure/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ web/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dje_scraper_adapter.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ content_parser.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ retry_handler.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ api_client_adapter.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ config/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ settings.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ dynamic_config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ logging/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ logger.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ scheduler/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ scheduler_adapter.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ monitoring/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ performance_monitor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ backup/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ backup_manager.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ alerts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ alert_system.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ health/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ health_checker.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ utils/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ debugging_tools.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ cli/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ commands.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ shared/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ container.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ main.py
‚îÇ
‚îú‚îÄ‚îÄ üìÇ tests/
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ conftest.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ unit/
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ integration/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ config/
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ scraping_config.json
‚îÇ
‚îú‚îÄ‚îÄ üìÇ logs/
‚îú‚îÄ‚îÄ üìÇ backups/
‚îú‚îÄ‚îÄ üìÇ debug/
‚îú‚îÄ‚îÄ üìÇ scripts/
‚îî‚îÄ‚îÄ üìÇ docs/
```

---

## üöÄ Guia de Instala√ß√£o

### 1. Pr√©-requisitos

```bash
# Verificar requisitos
python3 --version  # >= 3.12
docker --version
docker-compose --version
node --version     # Para API JusCash
```

### 2. Configura√ß√£o Inicial

```bash
# Clonar estrutura (ou criar manualmente)
mkdir juscash-dje-system
cd juscash-dje-system

# Criar estrutura de pastas
mkdir -p {src,tests,config,logs,backups,scripts,docs}
mkdir -p src/{domain,application,infrastructure,cli,shared}
mkdir -p src/domain/{entities,ports,services}
mkdir -p src/application/{usecases,services}
mkdir -p src/infrastructure/{web,api,config,logging,scheduler,monitoring,backup,alerts,health,utils}
```

### 3. Depend√™ncias Python

```bash
# Criar requirements.txt
cat > requirements.txt << 'EOF'
# Core dependencies
asyncio
httpx>=0.25.0
loguru>=0.7.0
pydantic[email]>=2.0.0
python-dotenv>=1.0.0

# Web scraping
playwright>=1.40.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# Scheduling
APScheduler>=3.10.0

# Data processing
python-dateutil>=2.8.0
tenacity>=8.2.0

# CLI
click>=8.0.0

# System monitoring
psutil>=5.9.0

# Development and testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
EOF

# Instalar depend√™ncias
pip install -r requirements.txt

# Instalar browsers
python -m playwright install chromium --with-deps
```

### 4. Configura√ß√£o de Ambiente

```bash
# Criar .env
cat > .env << 'EOF'
# Ambiente
ENVIRONMENT=development

# API Configuration
API_BASE_URL=http://localhost:8000
SCRAPER_API_KEY=scraper_2024_sua_chave_muito_longa_e_segura_aqui
API_TIMEOUT=30

# Browser Configuration
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=30000
BROWSER_USER_AGENT=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36

# Scraper Configuration
SCRAPER_TARGET_URL=https://dje.tjsp.jus.br/cdje/index.do
SCRAPER_MAX_RETRIES=3
SCRAPER_RETRY_DELAY=5
SCRAPER_MAX_PAGES=20
SCRAPER_SEARCH_TERMS=aposentadoria,benef√≠cio,INSS

# Logging
LOG_LEVEL=INFO
LOG_DIR=./logs
LOG_ROTATION_DAYS=7
LOG_MAX_SIZE_MB=10
EOF
```

---

## üîß Implementa√ß√£o dos Componentes

1. Implementar Entidades de Dom√≠nio

    **src/domain/entities/publication.py**

    ```python
    # [C√≥digo da entidade Publication implementado anteriormente]
    ```

2. Implementar Ports (Interfaces)

    **src/domain/ports/web_scraper.py**

    ```python
    # [C√≥digo das interfaces implementado anteriormente]
    ```

3. Implementar Use Cases

    **src/application/usecases/extract_publications.py**

    ```python
    # [C√≥digo dos use cases implementado anteriormente]
    ```

4. Implementar Adapters

    **src/infrastructure/web/dje_scraper_adapter.py**

    ```python
    # [C√≥digo do scraper implementado anteriormente]
    ```

5. Implementar CLI

    **src/cli/commands.py**

    ```python
    # [C√≥digo da CLI implementado anteriormente]
    ```

6. Implementar Entry Point

    **src/main.py**

    ```python
    # [C√≥digo do main implementado anteriormente]
    ```

---

## üê≥ Docker e Deploy

### 1. Dockerfile

```dockerfile
# [Dockerfile implementado anteriormente]
```

### 2. Docker Compose

```yaml
# [docker-compose.yml implementado anteriormente]
```

### 3. Deploy em Produ√ß√£o

```bash
# Executar script de setup
chmod +x production-setup.sh
./production-setup.sh

# Verificar instala√ß√£o
./deployment-check.sh

# Teste de integra√ß√£o completa
./final-integration-test.sh
```

---

## üéÆ Usando o Sistema

### 1. CLI Commands

```bash
# Executar scraping manual
python scraper_cli.py run --max-pages 5

# Verificar status
python scraper_cli.py status

# Configura√ß√£o
python scraper_cli.py config show
python scraper_cli.py config set --key max_pages --value 10

# Backup
python scraper_cli.py backup logs --days-back 1

# Alertas
python scraper_cli.py alerts list

# Testes
python scraper_cli.py test api
python scraper_cli.py test dje

# Monitoramento
python scraper_cli.py monitor --duration 60
```

### 2. Modo Agendado (Produ√ß√£o)

```bash
# Iniciar servi√ßo systemd
sudo systemctl start dje-scraper
sudo systemctl enable dje-scraper

# Monitorar
journalctl -fu dje-scraper
```

---

## üìä Monitoramento e Observabilidade

### 1. Logs Estruturados

```bash
# Logs principais
tail -f logs/scraper_$(date +%Y-%m-%d).log

# Logs de erro
tail -f logs/errors_$(date +%Y-%m-%d).log
```

### 2. M√©tricas de Performance

```bash
# Monitor em tempo real
python monitor_performance.py

# Health check
curl http://localhost:8000/health

# Estat√≠sticas via CLI
python scraper_cli.py status
```

### 3. Alertas

- **Email**: Configur√°vel para alertas cr√≠ticos
- **Logs**: Todos os eventos s√£o registrados
- **Sistema**: Integra√ß√£o com monitoramento externo

---

## üîí Seguran√ßa e Boas Pr√°ticas

### 1. Autentica√ß√£o

- **API Key**: Obrigat√≥ria para todas as requisi√ß√µes ao endpoint scraper
- **Rate Limiting**: 1000 requests/15min para scraper vs 100 para usu√°rios
- **Valida√ß√£o**: Rigorosa de todos os inputs

### 2. Configura√ß√µes Seguras

```bash
# Gerar API Key forte
openssl rand -hex 32

# Configurar firewall
sudo ufw allow ssh
sudo ufw allow 8000/tcp
sudo ufw enable

# Limites de sistema
echo "* soft nofile 65536" >> /etc/security/limits.conf
echo "* hard nofile 65536" >> /etc/security/limits.conf
```

### 3. Backup e Recupera√ß√£o

- **Backup Autom√°tico**: Di√°rio via cron
- **Reten√ß√£o**: 30 dias para logs, 90 dias para dados
- **Compress√£o**: Autom√°tica para economizar espa√ßo

---

## üß™ Testes

### 1. Testes Unit√°rios

```bash
# Executar testes
pytest tests/unit/ -v

# Com coverage
pytest tests/unit/ --cov=src --cov-report=html
```

### 2. Testes de Integra√ß√£o

```bash
# Teste de API
python test_api_connection.py

# Teste manual de scraping
python test_scraper_manual.py

# Teste completo
./final-integration-test.sh
```

### 3. Benchmark

```bash
# Performance
python benchmark.py

# Load test
for i in {1..100}; do
  curl -X POST http://localhost:8000/api/scraper/publications \
    -H "X-API-Key: sua-chave" \
    -H "Content-Type: application/json" \
    -d '{"test": true}' &
done
```

---

## üîß Troubleshooting

### Problemas Comuns

1. **Erro de API Key**

   ```bash
   # Verificar configura√ß√£o
   grep SCRAPER_API_KEY .env
   # Deve ser a mesma na API e no scraper
   ```

2. **Timeout do Browser**

   ```bash
   # Aumentar timeout
   export BROWSER_TIMEOUT=60000
   # Ou rodar sem headless para debug
   export BROWSER_HEADLESS=false
   ```

3. **Falha de Conex√£o com API**

   ```bash
   # Testar conectividade
   curl http://localhost:8000/health
   # Verificar se servi√ßos est√£o rodando
   docker-compose ps
   ```

4. **Erro de Permiss√£o Playwright**

   ```bash
   # Reinstalar browsers
   python -m playwright install chromium --with-deps
   ```

### Debug Avan√ßado

```bash
# Logs detalhados
LOG_LEVEL=DEBUG python scraper_cli.py run

# Captura de tela em erros
BROWSER_HEADLESS=false python scraper_cli.py test dje

# Network debugging
python -c "
import asyncio
from src.infrastructure.utils.debugging_tools import DebugTools
# Usar ferramentas de debug
"
```

---

## üìà Otimiza√ß√µes de Performance

### 1. Sistema

```bash
# Executar otimiza√ß√µes
./performance-tuning.sh

# Aplicar configura√ß√µes
source python_env.sh
```

### 2. Browser

```python
# Configura√ß√µes otimizadas j√° implementadas
OPTIMIZED_LAUNCH_OPTIONS = {
    'headless': True,
    'args': [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        # ... outras otimiza√ß√µes
    ]
}
```

### 3. Rede

```bash
# Otimiza√ß√µes TCP aplicadas automaticamente
net.core.somaxconn = 65535
net.ipv4.tcp_congestion_control = bbr
```

---

## üîÑ Atualiza√ß√µes e Manuten√ß√£o

### 1. Atualiza√ß√µes de C√≥digo

```bash
# Backup antes de atualizar
./cleanup_cache.sh
python scraper_cli.py backup logs

# Atualizar depend√™ncias
pip install -r requirements.txt --upgrade

# Reinstalar browsers se necess√°rio
python -m playwright install chromium
```

### 2. Manuten√ß√£o Rotineira

```bash
# Limpeza semanal
0 2 * * 0 /path/to/cleanup_cache.sh

# Backup di√°rio
0 1 * * * /path/to/scraper_cli.py backup logs

# Health check
*/15 * * * * /path/to/scripts/health_check.sh
```

### 3. Monitoramento de Capacidade

```bash
# Verificar recursos
df -h  # Espa√ßo em disco
free -h  # Mem√≥ria
top  # CPU

# Logs de performance
python monitor_performance.py
```

---

## üìö Documenta√ß√£o Adicional

### 1. API Integration

- **Endpoint**: `POST /api/scraper/publications`
- **Headers**: `X-API-Key`, `Content-Type: application/json`
- **Rate Limit**: 1000 req/15min
- **Docs**: Consultar `../api/SCRAPER-INTEGRATION.md`

### 2. Configura√ß√£o Din√¢mica

- **Arquivo**: `config/scraping_config.json`
- **Hot Reload**: Altera√ß√µes aplicadas automaticamente
- **CLI**: `python scraper_cli.py config`

### 3. Extensibilidade

Para adicionar novos scrapers ou funcionalidades:

1. **Criar Port** (interface)
2. **Implementar Adapter**
3. **Registrar no Container**
4. **Adicionar Use Case**
5. **Expor via CLI**

---

## üéØ Roadmap Futuro

### Pr√≥ximas Funcionalidades

- [ ] **Multi-tenancy**: Suporte a m√∫ltiplos clientes
- [ ] **Dashboard Web**: Interface de monitoramento
- [ ] **API de M√©tricas**: Exposi√ß√£o via REST
- [ ] **Machine Learning**: Classifica√ß√£o autom√°tica
- [ ] **Webhooks**: Notifica√ß√µes em tempo real
- [ ] **Kubernetes**: Deploy escal√°vel

### Melhorias Planejadas

- [ ] **Cache Inteligente**: Redis para publica√ß√µes
- [ ] **Processamento Paralelo**: M√∫ltiplos workers
- [ ] **Fallback Adapters**: M√∫ltiplas fontes de dados
- [ ] **Advanced Analytics**: Relat√≥rios detalhados

---

## ü§ù Contribui√ß√£o

### Padr√µes de Desenvolvimento

1. **Arquitetura Hexagonal**: Sempre seguir separa√ß√£o de camadas
2. **Clean Code**: C√≥digo limpo e bem documentado
3. **Type Hints**: Usar anota√ß√µes de tipo
4. **Testes**: Cobrir novos c√≥digos com testes
5. **Logs**: Adicionar logs estruturados

### Pull Request Process

1. Fork do projeto
2. Criar branch para feature
3. Implementar seguindo padr√µes
4. Adicionar testes
5. Atualizar documenta√ß√£o
6. Submeter PR

---

## üìÑ Licen√ßa

Este projeto est√° sob licen√ßa MIT. Consulte arquivo `LICENSE` para detalhes.

---

## üéâ Conclus√£o

O **DJE Scraper** √© um sistema robusto, escal√°vel e pronto para produ√ß√£o que automatiza completamente a extra√ß√£o de publica√ß√µes do DJE-SP. Com arquitetura hexagonal, monitoramento completo e integra√ß√£o perfeita com a API JusCash, o sistema garante:

- ‚úÖ **Reliability**: Retry autom√°tico e tratamento de erros
- ‚úÖ **Observability**: Logs, m√©tricas e alertas completos
- ‚úÖ **Scalability**: Preparado para crescimento
- ‚úÖ **Maintainability**: C√≥digo limpo e bem estruturado
- ‚úÖ **Security**: Autentica√ß√£o e valida√ß√£o rigorosa

**üöÄ Status**: ‚úÖ **PRONTO PARA PRODU√á√ÉO**

Para suporte ou d√∫vidas, consulte a documenta√ß√£o detalhada ou execute:

```bash
python scraper_cli.py --help
```
