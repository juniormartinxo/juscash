# ğŸ Scraper DJE - JusCash

Sistema de scraping automatizado para o DiÃ¡rio da JustiÃ§a EletrÃ´nico (DJE), desenvolvido em Python com arquitetura hexagonal.

## ğŸ“‹ Ãndice

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [Uso](#-uso)
- [Testes](#-testes)
- [Arquitetura](#-arquitetura)
- [Logs](#-logs)
- [Troubleshooting](#-troubleshooting)

## ğŸš€ CaracterÃ­sticas

- **Scraping Automatizado**: Coleta publicaÃ§Ãµes do DJE com base em critÃ©rios configurÃ¡veis
- **Agendamento**: ExecuÃ§Ã£o automÃ¡tica diÃ¡ria via APScheduler
- **Cache Redis**: Sistema de cache para otimizaÃ§Ã£o de performance
- **Banco PostgreSQL**: Armazenamento persistente com SQLAlchemy
- **Playwright**: NavegaÃ§Ã£o web robusta e confiÃ¡vel
- **Arquitetura Hexagonal**: CÃ³digo limpo e testÃ¡vel
- **Logs Estruturados**: Sistema de logging com Loguru
- **Docker Support**: ExecuÃ§Ã£o via containers

## ğŸ“¦ PrÃ©-requisitos

### Sistema

- **Python 3.8+** (recomendado 3.11)
- **PostgreSQL** (para banco de dados)
- **Redis** (para cache - opcional)
- **Git** (para versionamento)

### Linux/Ubuntu

```bash
sudo apt-get update
sudo apt-get install python3 python3-pip python3-venv python3-dev build-essential
```

### macOS

```bash
brew install python3 wget curl git
```

## ğŸ”§ InstalaÃ§Ã£o

### OpÃ§Ã£o 1: InstalaÃ§Ã£o AutomÃ¡tica (Recomendada)

Execute o script de instalaÃ§Ã£o a partir do diretÃ³rio raiz do projeto:

```bash
# Instalar apenas o scraper
./install.sh --scraper-only

# OU instalar todo o sistema (incluindo scraper)
./install.sh
```

### OpÃ§Ã£o 2: InstalaÃ§Ã£o Manual

1. **Navegar para o diretÃ³rio do scraper:**

    ```bash
    cd backend/scraper
    ```

2. **Criar ambiente virtual:**

    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3. **Instalar dependÃªncias:**

    ```bash
    pip install --upgrade pip
    pip install -r requirements.txt
    ```

4. **Configurar Playwright:**

    ```bash
    python -m playwright install chromium
    python -m playwright install-deps chromium
    ```

## âš¡ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Crie ou configure o arquivo `.env` no diretÃ³rio raiz do projeto:

```env
# Banco de Dados
DATABASE_URL=postgresql://usuario:senha@localhost:5432/juscash_db

# Redis (opcional)
REDIS_URL=redis://localhost:6379/0

# ConfiguraÃ§Ãµes do DJE
DJE_CADERNO=1
DJE_INSTANCIA=1
DJE_LOCAL=1
DJE_PARTE=1

# ConfiguraÃ§Ãµes de Scraping
SCRAPING_HEADLESS=true
SCRAPING_TIMEOUT=30000
SCRAPING_MAX_RETRIES=3
SCRAPING_REQUIRED_TERMS=termo1,termo2,termo3

# Agendamento
SCHEDULER_START_DATE=2024-01-01
SCHEDULER_EXECUTION_HOUR=8
SCHEDULER_EXECUTION_MINUTE=0
```

### ConfiguraÃ§Ãµes AvanÃ§adas

As configuraÃ§Ãµes podem ser ajustadas em `src/config/settings.py`:

- **Timeout de requisiÃ§Ãµes**
- **User-Agent personalizado**
- **ConfiguraÃ§Ãµes de retry**
- **ConfiguraÃ§Ãµes de logging**

## ğŸ¯ Uso

### ExecuÃ§Ã£o Local

```bash
# ExecuÃ§Ã£o Ãºnica (imediata)
./scripts/run-scraper-local.sh

# ExecuÃ§Ã£o com agendamento automÃ¡tico
./scripts/run-scraper-local.sh --schedule

# Teste de conexÃ£o com banco
cd backend/scraper && source venv/bin/activate
python -m src.main --test-db

# Teste de scraping
python -m src.main --test-scraping
```

### ExecuÃ§Ã£o via Docker

```bash
# Executar scraper via Docker
./scripts/run-scraper-docker.sh

# OU executar todo o sistema
docker-compose up --build
```

### Modos de ExecuÃ§Ã£o

1. **Modo Imediato**: Executa uma vez e para
2. **Modo Agendado**: Executa automaticamente todos os dias no horÃ¡rio configurado
3. **Modo Teste**: Executa testes de conectividade e funcionalidade

## ğŸ§ª Testes

### Executar Todos os Testes

```bash
./scripts/test-scraper.sh
```

### Testes EspecÃ­ficos

```bash
cd backend/scraper && source venv/bin/activate

# Teste de conexÃ£o com banco
python -m src.main --test-db

# Teste de scraping
python -m src.main --test-scraping

# Testes unitÃ¡rios (se disponÃ­veis)
python -m pytest tests/ -v
```

## ğŸ“‹ Arquitetura

O scraper segue a **Arquitetura Hexagonal** (Ports & Adapters):

```txt
src/
â”œâ”€â”€ main.py                 # Ponto de entrada da aplicaÃ§Ã£o
â”œâ”€â”€ config/                 # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ database.py
â”œâ”€â”€ core/                   # Regras de negÃ³cio
â”‚   â”œâ”€â”€ entities/
â”‚   â”œâ”€â”€ usecases/
â”‚   â””â”€â”€ ports/
â”œâ”€â”€ adapters/               # Adaptadores de infraestrutura
â”‚   â”œâ”€â”€ primary/           # Adaptadores primÃ¡rios (entrada)
â”‚   â””â”€â”€ secondary/         # Adaptadores secundÃ¡rios (saÃ­da)
â””â”€â”€ shared/                # UtilitÃ¡rios compartilhados
    â”œâ”€â”€ logger.py
    â””â”€â”€ value_objects.py
```

### Componentes Principais

- **Entities**: Modelos de domÃ­nio (Publication, ScrapingResult)
- **Use Cases**: Casos de uso (ScrapePublications, ScheduleScraping)
- **Adapters**: ImplementaÃ§Ãµes concretas (Playwright, SQLAlchemy, Redis)
- **Ports**: Interfaces/contratos entre camadas

## ğŸ“Š Logs

Os logs sÃ£o salvos em `backend/scraper/logs/`:

```txt
logs/
â”œâ”€â”€ scraper.log           # Log principal
â”œâ”€â”€ scraper_error.log     # Apenas erros
â””â”€â”€ scraper_debug.log     # Debug detalhado
```

### NÃ­veis de Log

- **INFO**: InformaÃ§Ãµes gerais de execuÃ§Ã£o
- **WARNING**: Avisos e situaÃ§Ãµes nÃ£o crÃ­ticas
- **ERROR**: Erros que impedem a execuÃ§Ã£o
- **DEBUG**: InformaÃ§Ãµes detalhadas para debugging

## ğŸ” Troubleshooting

### Problemas Comuns

#### 1. Erro de ImportaÃ§Ã£o do Playwright

```bash
# Reinstalar browsers
python -m playwright install chromium --with-deps
```

#### 2. Erro de ConexÃ£o com Banco

```bash
# Verificar se PostgreSQL estÃ¡ rodando
sudo systemctl status postgresql

# Testar conexÃ£o
python -m src.main --test-db
```

#### 3. Erro de ConexÃ£o com Redis

```bash
# Verificar se Redis estÃ¡ rodando
redis-cli ping

# Redis Ã© opcional - o scraper funciona sem ele
```

#### 4. PermissÃµes no Linux

```bash
# Dar permissÃµes aos scripts
chmod +x scripts/*.sh
```

#### 5. DependÃªncias do Sistema (Ubuntu)

```bash
# Instalar dependÃªncias do Playwright
sudo apt-get install libnss3-dev libatk-bridge2.0-dev libdrm-dev \
    libxcomposite-dev libxdamage-dev libxrandr-dev libgbm-dev \
    libxss-dev libasound2-dev
```

### VerificaÃ§Ã£o de SaÃºde

```bash
# Verificar instalaÃ§Ã£o completa
./scripts/test-scraper.sh

# Verificar apenas importaÃ§Ãµes Python
cd backend/scraper && source venv/bin/activate
python -c "
import playwright, sqlalchemy, redis, loguru, asyncio
print('âœ… Todas as dependÃªncias OK')
"
```

### Logs de Debug

Para debugging detalhado, configure no `.env`:

```env
LOG_LEVEL=DEBUG
```

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [ConfiguraÃ§Ã£o do Banco](../../database/README.md)
- [ConfiguraÃ§Ã£o do Redis](../../database/redis/README.md)
- [Scripts de InstalaÃ§Ã£o](../../scripts/README.md)
- [Docker Setup](../../docker-README.md)
