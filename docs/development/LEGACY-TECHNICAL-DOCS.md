# üîß Documenta√ß√£o T√©cnica - JusCash

> Sistema de Gerenciamento de Publica√ß√µes do Di√°rio da Justi√ßa Eletr√¥nico (DJE-SP)  
> Documenta√ß√£o T√©cnica Completa para Desenvolvedores e Administradores

---

## üìñ √çndice

1. [Vis√£o Geral da Arquitetura](#vis√£o-geral-da-arquitetura)
2. [Tecnologias Utilizadas](#tecnologias-utilizadas)
3. [Estrutura do Projeto](#estrutura-do-projeto)
4. [API REST](#api-rest)
5. [Banco de Dados](#banco-de-dados)
6. [Sistema de Scraping](#sistema-de-scraping)
7. [Frontend React](#frontend-react)
8. [Infraestrutura e DevOps](#infraestrutura-e-devops)
9. [Seguran√ßa](#seguran√ßa)
10. [Monitoramento e Logs](#monitoramento-e-logs)
11. [Instala√ß√£o e Configura√ß√£o](#instala√ß√£o-e-configura√ß√£o)
12. [Guias de Desenvolvimento](#guias-de-desenvolvimento)

---

## üèóÔ∏è Vis√£o Geral da Arquitetura

### Arquitetura do Sistema

O JusCash foi desenvolvido seguindo os princ√≠pios de **Clean Architecture** (Arquitetura Hexagonal), garantindo:

- **Separa√ß√£o clara de responsabilidades**
- **Independ√™ncia de frameworks**
- **Testabilidade**
- **Flexibilidade para mudan√ßas**
- **Escalabilidade**

```mermaid
graph TB
    subgraph "Frontend"
        UI[React + TypeScript]
        UI --> Components[Componentes UI]
        UI --> Hooks[Custom Hooks]
        UI --> Services[API Services]
    end

    subgraph "API Backend"
        Controllers[Controllers]
        UseCases[Use Cases]
        Domain[Domain Layer]
        Repositories[Repositories]
        Infrastructure[Infrastructure]
    end

    subgraph "Scraper Python"
        ScrapingOrchestrator[Scraping Orchestrator]
        WebScraper[Web Scraper]
        DataValidator[Data Validator]
        FileManager[File Manager]
    end

    subgraph "Banco de Dados"
        PostgreSQL[(PostgreSQL)]
        Redis[(Redis Cache)]
    end

    UI --> Controllers
    Controllers --> UseCases
    UseCases --> Domain
    UseCases --> Repositories
    Repositories --> Infrastructure
    Infrastructure --> PostgreSQL

    ScrapingOrchestrator --> WebScraper
    ScrapingOrchestrator --> DataValidator
    DataValidator --> FileManager
    ScrapingOrchestrator --> PostgreSQL
    
    Infrastructure --> Redis
    ScrapingOrchestrator --> Redis
```

### Princ√≠pios Arquiteturais

#### 1. Clean Architecture (Backend)

- **Domain Layer**: Entidades, regras de neg√≥cio e contratos
- **Application Layer**: Casos de uso e orquestra√ß√£o
- **Infrastructure Layer**: Implementa√ß√µes t√©cnicas
- **Web Layer**: Controllers, middlewares e rotas

#### 2. Component-Based Architecture (Frontend)

- **Componentes Reutiliz√°veis**: UI components isolados
- **Custom Hooks**: L√≥gica compartilhada
- **Context API**: Gerenciamento de estado global
- **Service Layer**: Comunica√ß√£o com API

#### 3. Hexagonal Architecture (Scraper)

- **Portas e Adaptadores**: Interfaces bem definidas
- **Dom√≠nio Isolado**: L√≥gica de neg√≥cio independente
- **Adaptadores**: Implementa√ß√µes espec√≠ficas

---

## üíª Tecnologias Utilizadas

### Backend (Node.js)

| Tecnologia | Vers√£o | Prop√≥sito |
|------------|--------|-----------|
| **Node.js** | 20+ | Runtime JavaScript |
| **Express.js** | 5.x | Framework web |
| **TypeScript** | 5.x | Tipagem est√°tica |
| **Prisma ORM** | 5.x | Object-Relational Mapping |
| **JWT** | 9.x | Autentica√ß√£o |
| **Zod** | 3.x | Valida√ß√£o de schemas |
| **Winston** | 3.x | Sistema de logs |
| **bcrypt** | 5.x | Hash de senhas |
| **Helmet** | 7.x | Seguran√ßa HTTP |
| **CORS** | 2.x | Cross-Origin Resource Sharing |

### Frontend (React)

| Tecnologia | Vers√£o | Prop√≥sito |
|------------|--------|-----------|
| **React** | 18.x | Framework UI |
| **TypeScript** | 5.x | Tipagem est√°tica |
| **Vite** | 5.x | Build tool |
| **Tailwind CSS** | 3.x | Framework CSS |
| **Lucide React** | 0.x | √çcones |
| **React Hook Form** | 7.x | Gerenciamento de formul√°rios |
| **React DnD** | 16.x | Drag and Drop |
| **Axios** | 1.x | Cliente HTTP |

### Scraper (Python)

| Tecnologia | Vers√£o | Prop√≥sito |
|------------|--------|-----------|
| **Python** | 3.11+ | Runtime |
| **Playwright** | 1.x | Automa√ß√£o web |
| **SQLAlchemy** | 2.x | ORM Python |
| **Pydantic** | 2.x | Valida√ß√£o de dados |
| **APScheduler** | 3.x | Agendamento de tarefas |
| **Loguru** | 0.x | Sistema de logs |
| **Redis** | 4.x | Cache e filas |
| **Requests** | 2.x | Cliente HTTP |

### Banco de Dados

| Tecnologia | Vers√£o | Prop√≥sito |
|------------|--------|-----------|
| **PostgreSQL** | 16+ | Banco relacional |
| **Redis** | 7+ | Cache e sess√µes |

### DevOps e Infraestrutura

| Tecnologia | Vers√£o | Prop√≥sito |
|------------|--------|-----------|
| **Docker** | 24+ | Containeriza√ß√£o |
| **Docker Compose** | 2.x | Orquestra√ß√£o local |
| **Nginx** | 1.x | Proxy reverso |
| **Git** | 2.x | Controle de vers√£o |

---

## üìÅ Estrutura do Projeto

### Estrutura Geral

```
juscash/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API REST Node.js
‚îÇ   ‚îî‚îÄ‚îÄ scraper/               # Sistema de scraping Python
‚îú‚îÄ‚îÄ frontend/                  # Interface React
‚îú‚îÄ‚îÄ database/                  # Configura√ß√µes de banco
‚îú‚îÄ‚îÄ docs/                      # Documenta√ß√£o
‚îú‚îÄ‚îÄ scripts/                   # Scripts utilit√°rios
‚îú‚îÄ‚îÄ docker-compose.yml         # Orquestra√ß√£o Docker
‚îî‚îÄ‚îÄ README.md                  # Documenta√ß√£o principal
```

### Backend API (Node.js)

```
backend/api/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ domain/               # Camada de dom√≠nio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/         # Entidades do neg√≥cio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories/     # Contratos dos reposit√≥rios
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/         # Servi√ßos de dom√≠nio
‚îÇ   ‚îú‚îÄ‚îÄ application/          # Camada de aplica√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ usecases/         # Casos de uso
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/       # Camada de infraestrutura
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/         # Adaptadores Prisma
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security/         # JWT e autentica√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ web/             # Controllers e rotas
‚îÇ   ‚îî‚îÄ‚îÄ shared/              # C√≥digo compartilhado
‚îÇ       ‚îú‚îÄ‚îÄ config/          # Configura√ß√µes
‚îÇ       ‚îú‚îÄ‚îÄ utils/           # Utilit√°rios
‚îÇ       ‚îî‚îÄ‚îÄ validation/      # Schemas Zod
‚îú‚îÄ‚îÄ prisma/                  # Schema e migra√ß√µes
‚îú‚îÄ‚îÄ tests/                   # Testes automatizados
‚îî‚îÄ‚îÄ package.json             # Depend√™ncias Node.js
```

### Scraper Python

```
backend/scraper/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ domain/              # Camada de dom√≠nio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entities/        # Entidades do scraping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ports/           # Interfaces/contratos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/        # Servi√ßos de dom√≠nio
‚îÇ   ‚îú‚îÄ‚îÄ application/         # Camada de aplica√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/        # Orquestra√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ usecases/        # Casos de uso
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/      # Camada de infraestrutura
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ web/            # Adaptadores web
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/       # Adaptadores banco
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ files/          # Gerenciamento arquivos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring/     # Monitoramento
‚îÇ   ‚îî‚îÄ‚îÄ main.py             # Ponto de entrada
‚îú‚îÄ‚îÄ requirements.txt         # Depend√™ncias Python
‚îî‚îÄ‚îÄ config/                 # Configura√ß√µes
```

### Frontend React

```
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/          # Componentes UI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/             # Componentes base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KanbanBoard.tsx # Board principal
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ pages/              # P√°ginas da aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ services/           # Servi√ßos API
‚îÇ   ‚îú‚îÄ‚îÄ hooks/              # Custom hooks
‚îÇ   ‚îú‚îÄ‚îÄ contexts/           # Contextos React
‚îÇ   ‚îú‚îÄ‚îÄ types/              # Defini√ß√µes TypeScript
‚îÇ   ‚îî‚îÄ‚îÄ lib/                # Utilit√°rios
‚îú‚îÄ‚îÄ public/                 # Arquivos est√°ticos
‚îî‚îÄ‚îÄ package.json            # Depend√™ncias React
```

---

## üåê API REST

### Documenta√ß√£o Swagger/OpenAPI

A API possui documenta√ß√£o completa Swagger dispon√≠vel em:

- **Local**: `${API_BASE_URL}-docs`
- **Produ√ß√£o**: `https://[seu-dominio]/api-docs`

### Endpoints Principais

#### Autentica√ß√£o

##### POST `/api/auth/register`

Registra um novo usu√°rio no sistema.

**Request Body:**

```json
{
  "name": "Jo√£o Silva",
  "email": "joao@exemplo.com",
  "password": "MinhaSenh@123"
}
```

**Response (201):**

```json
{
  "success": true,
  "data": {
    "user": {
      "id": "cm123456789",
      "name": "Jo√£o Silva",
      "email": "joao@exemplo.com",
      "isActive": true,
      "createdAt": "2024-01-15T10:30:00.000Z"
    },
    "tokens": {
      "accessToken": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
      "refreshToken": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
    }
  },
  "message": "Usu√°rio registrado com sucesso"
}
```

##### POST `/api/auth/login`

Autentica um usu√°rio existente.

**Request Body:**

```json
{
  "email": "joao@exemplo.com",
  "password": "MinhaSenh@123"
}
```

**Response (200):**

```json
{
  "success": true,
  "data": {
    "user": {
      "id": "cm123456789",
      "name": "Jo√£o Silva",
      "email": "joao@exemplo.com",
      "isActive": true
    },
    "tokens": {
      "accessToken": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
      "refreshToken": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
    }
  },
  "message": "Login realizado com sucesso"
}
```

##### POST `/api/auth/refresh`

Renova o token de acesso usando o refresh token.

**Request Body:**

```json
{
  "refreshToken": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
```

##### POST `/api/auth/logout`

Realiza logout invalidando o refresh token.

#### Publica√ß√µes

##### GET `/api/publications`

Lista publica√ß√µes com pagina√ß√£o e filtros.

**Query Parameters:**

- `page` (number): P√°gina (padr√£o: 1)
- `limit` (number): Itens por p√°gina (padr√£o: 20, m√°x: 100)
- `status` (enum): Filtro por status
- `search` (string): Busca por texto
- `processNumber` (string): Filtro por n√∫mero do processo
- `startDate` (date): Data inicial
- `endDate` (date): Data final
- `authors` (string): Filtro por autores
- `defendant` (string): Filtro por r√©u

**Response (200):**

```json
{
  "success": true,
  "data": [
    {
      "id": "cm123456789",
      "processNumber": "1234567-89.2023.4.03.6100",
      "publicationDate": "2023-12-15",
      "availabilityDate": "2023-12-16",
      "authors": ["Jo√£o Silva", "Maria Santos"],
      "defendant": "Instituto Nacional do Seguro Social - INSS",
      "grossValue": 50000,
      "netValue": 45000,
      "interestValue": 5000,
      "attorneyFees": 10000,
      "content": "Conte√∫do da publica√ß√£o...",
      "status": "NOVA",
      "createdAt": "2023-12-16T08:00:00.000Z",
      "updatedAt": "2023-12-16T08:00:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 20,
    "total": 150,
    "totalPages": 8,
    "hasNext": true,
    "hasPrev": false
  }
}
```

##### GET `/api/publications/:id`

Busca uma publica√ß√£o espec√≠fica por ID.

**Response (200):**

```json
{
  "success": true,
  "data": {
    "id": "cm123456789",
    "processNumber": "1234567-89.2023.4.03.6100",
    "publicationDate": "2023-12-15",
    "availabilityDate": "2023-12-16",
    "authors": ["Jo√£o Silva", "Maria Santos"],
    "defendant": "Instituto Nacional do Seguro Social - INSS",
    "lawyers": {
      "author": ["Dr. Jo√£o Advogado - OAB/SP 123456"],
      "defendant": ["Dr. Maria Procuradora - OAB/SP 654321"]
    },
    "grossValue": 50000,
    "netValue": 45000,
    "interestValue": 5000,
    "attorneyFees": 10000,
    "content": "Conte√∫do completo da publica√ß√£o...",
    "status": "NOVA",
    "scrapingSource": "DJE-SP",
    "caderno": "3",
    "instancia": "1",
    "local": "Capital",
    "parte": "1",
    "extractionMetadata": {
      "extractedAt": "2023-12-16T08:00:00.000Z",
      "source": "automated_scraping",
      "confidence": 0.95
    },
    "createdAt": "2023-12-16T08:00:00.000Z",
    "updatedAt": "2023-12-16T08:00:00.000Z"
  }
}
```

##### PUT `/api/publications/:id/status`

Atualiza o status de uma publica√ß√£o.

**Request Body:**

```json
{
  "status": "LIDA",
  "notes": "Publica√ß√£o analisada e aprovada para pr√≥xima etapa"
}
```

**Response (200):**

```json
{
  "success": true,
  "data": {
    "id": "cm123456789",
    "status": "LIDA",
    "updatedAt": "2023-12-16T14:30:00.000Z"
  },
  "message": "Status atualizado com sucesso"
}
```

#### M√©tricas e Sa√∫de

##### GET `/api/health`

Verifica a sa√∫de do sistema.

**Response (200):**

```json
{
  "success": true,
  "data": {
    "status": "healthy",
    "timestamp": "2023-12-16T14:30:00.000Z",
    "uptime": 86400,
    "version": "1.0.0",
    "services": {
      "database": "connected",
      "redis": "connected"
    },
    "metrics": {
      "totalPublications": 1500,
      "todayPublications": 25,
      "activeUsers": 5
    }
  }
}
```

##### GET `/api/metrics`

Retorna m√©tricas detalhadas do sistema.

### Middlewares

#### 1. Autentica√ß√£o (authMiddleware)

- Valida JWT tokens
- Verifica se usu√°rio est√° ativo
- Adiciona dados do usu√°rio ao request

#### 2. Rate Limiting (rateLimitMiddleware)

- Limita requisi√ß√µes por IP/usu√°rio
- Configur√°vel por endpoint
- Padr√£o: 100 req/15min

#### 3. Valida√ß√£o (validationMiddleware)

- Valida request body com schemas Zod
- Sanitiza dados de entrada
- Retorna erros formatados

#### 4. Logging (loggingMiddleware)

- Registra todas as requisi√ß√µes
- Inclui timing e status codes
- Logs estruturados com Winston

#### 5. Seguran√ßa (securityMiddleware)

- Headers de seguran√ßa (Helmet)
- Prote√ß√£o CORS
- Sanitiza√ß√£o XSS

### Tratamento de Erros

```typescript
// Estrutura padr√£o de erro
{
  "success": false,
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Dados inv√°lidos fornecidos",
    "details": [
      {
        "field": "email",
        "message": "Email deve ser v√°lido"
      }
    ]
  },
  "timestamp": "2023-12-16T14:30:00.000Z",
  "path": "/api/auth/register"
}
```

#### C√≥digos de Erro HTTP

| C√≥digo | Descri√ß√£o | Uso |
|--------|-----------|-----|
| 400 | Bad Request | Dados inv√°lidos ou malformados |
| 401 | Unauthorized | Token inv√°lido ou expirado |
| 403 | Forbidden | Usu√°rio sem permiss√£o |
| 404 | Not Found | Recurso n√£o encontrado |
| 409 | Conflict | Conflito de dados (ex: email j√° existe) |
| 422 | Unprocessable Entity | Dados v√°lidos mas regra de neg√≥cio violada |
| 429 | Too Many Requests | Rate limit excedido |
| 500 | Internal Server Error | Erro interno do servidor |

---

## üóÑÔ∏è Banco de Dados

### Schema do PostgreSQL

#### Diagrama de Relacionamentos

```mermaid
erDiagram
    User ||--o{ UserRefreshToken : has
    User ||--o{ UserSession : has
    User ||--o{ AuthLog : generates
    User ||--o{ PublicationLog : creates

    Publication ||--o{ PublicationLog : has
    Publication }o--|| ScrapingExecution : belongs_to

    ScrapingExecution ||--o{ Publication : contains
    ScrapingExecution ||--o{ ScrapingLog : generates
    ScrapingExecution ||--o{ ScrapingConfiguration : uses
```

#### Tabelas Principais

##### users

Tabela central de usu√°rios do sistema.

```sql
CREATE TABLE users (
    id VARCHAR PRIMARY KEY,              -- CUID2
    name VARCHAR NOT NULL,               -- Nome completo
    email VARCHAR UNIQUE NOT NULL,       -- Email √∫nico
    password_hash VARCHAR NOT NULL,      -- Hash bcrypt da senha
    last_password_change TIMESTAMP,     -- √öltima altera√ß√£o de senha
    is_password_temporary BOOLEAN DEFAULT true, -- Senha tempor√°ria
    is_active BOOLEAN DEFAULT true,     -- Usu√°rio ativo
    created_at TIMESTAMP DEFAULT NOW(), -- Data de cria√ß√£o
    updated_at TIMESTAMP DEFAULT NOW(), -- √öltima atualiza√ß√£o
    deactivated_at TIMESTAMP           -- Data de desativa√ß√£o
);

-- √çndices para performance
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_created_at ON users(created_at);
CREATE INDEX idx_users_active ON users(is_active, created_at);
```

##### publications

Tabela principal de publica√ß√µes do DJE.

```sql
CREATE TABLE publications (
    id VARCHAR PRIMARY KEY,              -- CUID2
    process_number VARCHAR UNIQUE NOT NULL, -- N√∫mero CNJ do processo
    publication_date DATE,               -- Data oficial da publica√ß√£o
    availability_date DATE,              -- Data de disponibiliza√ß√£o
    authors TEXT[] NOT NULL,             -- Array de autores
    defendant VARCHAR DEFAULT 'Instituto Nacional do Seguro Social - INSS',
    lawyers JSONB,                       -- Advogados estruturados
    gross_value BIGINT,                  -- Valor bruto em centavos
    net_value BIGINT,                    -- Valor l√≠quido em centavos
    interest_value BIGINT,               -- Valor de juros em centavos
    attorney_fees BIGINT,                -- Honor√°rios em centavos
    content TEXT NOT NULL,               -- Conte√∫do completo
    status publication_status DEFAULT 'NOVA', -- Status do workflow
    created_at TIMESTAMP DEFAULT NOW(), -- Data de cria√ß√£o
    updated_at TIMESTAMP DEFAULT NOW(), -- √öltima atualiza√ß√£o
    scraping_source VARCHAR DEFAULT 'DJE-SP', -- Fonte do scraping
    caderno VARCHAR DEFAULT '3',         -- Caderno do DJE
    instancia VARCHAR DEFAULT '1',       -- Inst√¢ncia judicial
    local VARCHAR DEFAULT 'Capital',     -- Local da publica√ß√£o
    parte VARCHAR DEFAULT '1',           -- Parte do processo
    extraction_metadata JSONB,          -- Metadados da extra√ß√£o
    scraping_execution_id VARCHAR       -- FK para execu√ß√£o do scraping
);

-- √çndices otimizados
CREATE INDEX idx_publications_process_number ON publications(process_number);
CREATE INDEX idx_publications_status ON publications(status);
CREATE INDEX idx_publications_availability_date ON publications(availability_date);
CREATE INDEX idx_publications_created_at ON publications(created_at);
CREATE INDEX idx_publications_status_date ON publications(status, availability_date);
CREATE INDEX idx_publications_source_caderno ON publications(scraping_source, caderno);
CREATE INDEX idx_publications_defendant ON publications(defendant);

-- √çndice para busca textual
CREATE INDEX idx_publications_content_search ON publications 
USING gin(to_tsvector('portuguese', content));

-- √çndice para busca em autores
CREATE INDEX idx_publications_authors ON publications USING gin(authors);
```

##### scraping_executions

Registra execu√ß√µes do sistema de scraping.

```sql
CREATE TABLE scraping_executions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    execution_type scraping_execution_type NOT NULL, -- SCHEDULED, MANUAL, TEST
    status scraping_execution_status DEFAULT 'RUNNING', -- RUNNING, SUCCESS, FAILED
    started_at TIMESTAMP DEFAULT NOW(),
    finished_at TIMESTAMP,
    execution_time_seconds INTEGER,
    publications_found INTEGER DEFAULT 0,
    publications_new INTEGER DEFAULT 0,
    publications_duplicated INTEGER DEFAULT 0,
    publications_failed INTEGER DEFAULT 0,
    publications_saved INTEGER DEFAULT 0,
    criteria_used JSONB,                 -- Crit√©rios utilizados
    max_publications_limit INTEGER,     -- Limite m√°ximo configurado
    scraper_version VARCHAR,             -- Vers√£o do scraper
    browser_user_agent VARCHAR,         -- User agent utilizado
    error_details JSONB                  -- Detalhes de erros
);

-- √çndices para relat√≥rios
CREATE INDEX idx_scraping_executions_type_date ON scraping_executions(execution_type, started_at);
CREATE INDEX idx_scraping_executions_status_date ON scraping_executions(status, started_at);
```

##### publication_logs

Auditoria de a√ß√µes nas publica√ß√µes.

```sql
CREATE TABLE publication_logs (
    id VARCHAR PRIMARY KEY,
    publication_id VARCHAR NOT NULL,
    user_id VARCHAR NOT NULL,
    action publication_log_action NOT NULL, -- VIEW, STATUS_CHANGE, EDIT, etc.
    old_data JSONB,                      -- Estado anterior
    new_data JSONB,                      -- Estado novo
    notes TEXT,                          -- Observa√ß√µes
    created_at TIMESTAMP DEFAULT NOW(),
    
    FOREIGN KEY (publication_id) REFERENCES publications(id),
    FOREIGN KEY (user_id) REFERENCES users(id)
);

-- √çndices para auditoria
CREATE INDEX idx_publication_logs_publication_date ON publication_logs(publication_id, created_at);
CREATE INDEX idx_publication_logs_user_date ON publication_logs(user_id, created_at);
CREATE INDEX idx_publication_logs_action_date ON publication_logs(action, created_at);
```

#### Enums Principais

```sql
-- Status das publica√ß√µes
CREATE TYPE publication_status AS ENUM (
    'NOVA',
    'LIDA', 
    'ENVIADA_PARA_ADV',
    'CONCLUIDA'
);

-- Tipos de execu√ß√£o do scraping
CREATE TYPE scraping_execution_type AS ENUM (
    'SCHEDULED',
    'MANUAL',
    'TEST'
);

-- Status da execu√ß√£o do scraping
CREATE TYPE scraping_execution_status AS ENUM (
    'RUNNING',
    'SUCCESS',
    'FAILED',
    'CANCELLED'
);

-- A√ß√µes de log das publica√ß√µes
CREATE TYPE publication_log_action AS ENUM (
    'VIEW',
    'STATUS_CHANGE',
    'EDIT',
    'CREATE',
    'DELETE'
);
```

### Estrat√©gias de Performance

#### 1. Indexa√ß√£o Inteligente

- √çndices compostos para consultas frequentes
- √çndices GIN para arrays e busca textual
- √çndices parciais para dados ativos

#### 2. Particionamento

```sql
-- Particionamento por data para publica√ß√µes antigas
CREATE TABLE publications_2023 PARTITION OF publications
FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');

CREATE TABLE publications_2024 PARTITION OF publications
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

#### 3. Otimiza√ß√µes de Query

- LIMIT com OFFSET otimizado para pagina√ß√£o
- Cursor-based pagination para grandes datasets
- Query planning para consultas complexas

#### 4. Connection Pooling

```typescript
// Configura√ß√£o Prisma
{
  datasource: {
    provider: "postgresql",
    url: env("DATABASE_URL")
  },
  generator: {
    provider: "prisma-client-js",
    output: "../src/generated/prisma"
  }
}

// Pool de conex√µes otimizado
const prisma = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL
    }
  },
  log: ['query', 'info', 'warn', 'error']
})
```

### Backup e Recupera√ß√£o

#### 1. Backup Autom√°tico

```bash
#!/bin/bash
# Script de backup di√°rio

DB_NAME="juscash_db"
BACKUP_DIR="/backups/postgresql"
DATE=$(date +%Y%m%d_%H%M%S)

# Backup completo
pg_dump -h localhost -U postgres $DB_NAME > $BACKUP_DIR/full_backup_$DATE.sql

# Compacta√ß√£o
gzip $BACKUP_DIR/full_backup_$DATE.sql

# Manter apenas √∫ltimos 30 dias
find $BACKUP_DIR -name "full_backup_*.sql.gz" -mtime +30 -delete
```

#### 2. Point-in-Time Recovery

- WAL archiving habilitado
- Backup incremental cont√≠nuo
- Restore para momento espec√≠fico

#### 3. Replica√ß√£o

- Streaming replication para read replicas
- Failover autom√°tico configurado
- Monitoramento de lag de replica√ß√£o

---

# üîß Documenta√ß√£o T√©cnica - JusCash (Parte 2)

## üï∑Ô∏è Sistema de Scraping

### Arquitetura do Scraper

O sistema de scraping √© desenvolvido em Python seguindo os princ√≠pios de **Arquitetura Hexagonal**, garantindo separa√ß√£o clara entre l√≥gica de neg√≥cio e implementa√ß√µes t√©cnicas.

```mermaid
graph TB
    subgraph "Application Layer"
        Orchestrator[Scraping Orchestrator]
        UseCases[Use Cases]
    end
    
    subgraph "Domain Layer"
        Entities[Entities]
        Services[Domain Services]
        Ports[Ports/Interfaces]
    end
    
    subgraph "Infrastructure Layer"
        WebAdapter[Web Scraper Adapter]
        DBAdapter[Database Adapter]
        FileAdapter[File Adapter]
        MonitoringAdapter[Monitoring Adapter]
    end
    
    subgraph "External Systems"
        DJE[DJE Website]
        PostgreSQL[(PostgreSQL)]
        Files[JSON Files]
        API[JusCash API]
    end
    
    Orchestrator --> UseCases
    UseCases --> Services
    Services --> Entities
    UseCases --> Ports
    
    Ports --> WebAdapter
    Ports --> DBAdapter
    Ports --> FileAdapter
    Ports --> MonitoringAdapter
    
    WebAdapter --> DJE
    DBAdapter --> PostgreSQL
    FileAdapter --> Files
    MonitoringAdapter --> API
```

### Componentes Principais

#### 1. Scraping Orchestrator

**Localiza√ß√£o**: `backend/scraper/src/application/services/scraping_orchestrator.py`

Respons√°vel por orquestrar todo o processo de scraping:

```python
class ScrapingOrchestrator:
    def __init__(self, web_scraper, data_validator, file_saver, db_repository):
        self.web_scraper = web_scraper
        self.data_validator = data_validator
        self.file_saver = file_saver
        self.db_repository = db_repository
    
    async def execute_scraping(self, criteria: ScrapingCriteria) -> ScrapingResult:
        """
        Executa o processo completo de scraping
        """
        execution = await self.start_execution(criteria)
        
        try:
            # 1. Extrair publica√ß√µes do DJE
            raw_publications = await self.web_scraper.extract_publications(criteria)
            
            # 2. Validar e processar dados
            validated_publications = []
            for raw_pub in raw_publications:
                if self.data_validator.is_valid(raw_pub):
                    publication = self.data_validator.process(raw_pub)
                    validated_publications.append(publication)
            
            # 3. Salvar em arquivos de backup
            await self.file_saver.save_publications(validated_publications)
            
            # 4. Persistir no banco de dados
            saved_count = await self.db_repository.save_publications(validated_publications)
            
            # 5. Finalizar execu√ß√£o
            await self.complete_execution(execution, saved_count)
            
            return ScrapingResult(
                success=True,
                publications_found=len(raw_publications),
                publications_saved=saved_count
            )
            
        except Exception as e:
            await self.fail_execution(execution, str(e))
            raise
```

#### 2. Web Scraper Adapter

**Localiza√ß√£o**: `backend/scraper/src/infrastructure/web/dje_scraper_adapter.py`

Implementa a extra√ß√£o de dados do DJE usando Playwright:

```python
class DJEScraperAdapter:
    def __init__(self):
        self.browser = None
        self.page = None
        
    async def extract_publications(self, criteria: ScrapingCriteria) -> List[RawPublication]:
        """
        Extrai publica√ß√µes do DJE baseado nos crit√©rios
        """
        await self.initialize_browser()
        
        try:
            # 1. Navegar para o DJE
            await self.navigate_to_dje()
            
            # 2. Preencher formul√°rio de busca
            await self.fill_search_form(criteria)
            
            # 3. Submeter busca
            await self.submit_search()
            
            # 4. Extrair resultados
            publications = await self.extract_results()
            
            return publications
            
        finally:
            await self.cleanup_browser()
    
    async def navigate_to_dje(self):
        """Navega para a p√°gina do DJE"""
        url = "https://dje.tjsp.jus.br/cdje/consultaSimples.do"
        await self.page.goto(url, wait_until="networkidle")
        
    async def fill_search_form(self, criteria: ScrapingCriteria):
        """Preenche o formul√°rio de busca"""
        # Selecionar caderno
        await self.page.select_option("#caderno", criteria.caderno)
        
        # Selecionar inst√¢ncia
        await self.page.select_option("#instancia", criteria.instancia)
        
        # Data de publica√ß√£o
        await self.page.fill("#dtPublicacao", criteria.publication_date.strftime("%d/%m/%Y"))
        
        # Termos de busca (se especificados)
        if criteria.search_terms:
            await self.page.fill("#palavrasChave", " ".join(criteria.search_terms))
    
    async def extract_results(self) -> List[RawPublication]:
        """Extrai as publica√ß√µes dos resultados"""
        publications = []
        
        # Aguardar carregamento dos resultados
        await self.page.wait_for_selector(".resultadoLista")
        
        # Extrair cada publica√ß√£o
        publication_elements = await self.page.query_selector_all(".publicacao")
        
        for element in publication_elements:
            try:
                publication_data = await self.extract_publication_data(element)
                publications.append(RawPublication(**publication_data))
            except Exception as e:
                logger.warning(f"Erro ao extrair publica√ß√£o: {e}")
                continue
        
        return publications
```

#### 3. Content Parser

**Localiza√ß√£o**: `backend/scraper/src/infrastructure/web/enhanced_content_parser.py`

Processa o conte√∫do das publica√ß√µes extraindo informa√ß√µes estruturadas:

```python
class EnhancedContentParser:
    def __init__(self):
        self.value_patterns = {
            'gross_value': r'valor\s+bruto[:\s]+r\$\s*([\d.,]+)',
            'net_value': r'valor\s+l√≠quido[:\s]+r\$\s*([\d.,]+)',
            'attorney_fees': r'honor√°rios[:\s]+r\$\s*([\d.,]+)',
            'interest_value': r'juros[:\s]+r\$\s*([\d.,]+)'
        }
        
    def parse_publication(self, raw_content: str) -> ParsedPublication:
        """
        Processa o conte√∫do bruto da publica√ß√£o
        """
        return ParsedPublication(
            process_number=self.extract_process_number(raw_content),
            authors=self.extract_authors(raw_content),
            defendant=self.extract_defendant(raw_content),
            lawyers=self.extract_lawyers(raw_content),
            values=self.extract_values(raw_content),
            content=self.clean_content(raw_content)
        )
    
    def extract_process_number(self, content: str) -> Optional[str]:
        """Extrai o n√∫mero do processo"""
        pattern = r'(\d{7}-\d{2}\.\d{4}\.\d{1}\.\d{2}\.\d{4})'
        match = re.search(pattern, content)
        return match.group(1) if match else None
    
    def extract_values(self, content: str) -> Dict[str, int]:
        """Extrai valores monet√°rios convertendo para centavos"""
        values = {}
        
        for value_type, pattern in self.value_patterns.items():
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                value_str = match.group(1).replace('.', '').replace(',', '.')
                try:
                    # Converter para centavos
                    value_cents = int(float(value_str) * 100)
                    values[value_type] = value_cents
                except ValueError:
                    continue
        
        return values
```

### Configura√ß√£o e Agendamento

#### 1. Configura√ß√µes Din√¢micas

**Localiza√ß√£o**: `backend/scraper/src/infrastructure/config/settings.py`

```python
class ScrapingSettings:
    # Configura√ß√µes do DJE
    DJE_BASE_URL = "https://dje.tjsp.jus.br"
    DJE_TIMEOUT = 30000  # 30 segundos
    
    # Configura√ß√µes do navegador
    BROWSER_HEADLESS = True
    BROWSER_USER_AGENT = "Mozilla/5.0 (compatible; JusCashBot/1.0)"
    
    # Configura√ß√µes de retry
    MAX_RETRIES = 3
    RETRY_DELAY = 5  # segundos
    
    # Filtros autom√°ticos
    REQUIRED_TERMS = ["Instituto Nacional", "INSS", "Seguro Social"]
    EXCLUDED_TERMS = ["arquivado", "extinto"]
    
    # Limites de seguran√ßa
    MAX_PUBLICATIONS_PER_RUN = 1000
    MAX_EXECUTION_TIME = 3600  # 1 hora
    
    @classmethod
    def load_from_env(cls):
        """Carrega configura√ß√µes das vari√°veis de ambiente"""
        return cls(
            dje_caderno=os.getenv('DJE_CADERNO', '3'),
            dje_instancia=os.getenv('DJE_INSTANCIA', '1'),
            browser_headless=os.getenv('SCRAPING_HEADLESS', 'true').lower() == 'true',
            required_terms=os.getenv('SCRAPING_REQUIRED_TERMS', '').split(',')
        )
```

#### 2. Agendamento Autom√°tico

**Localiza√ß√£o**: `backend/scraper/src/infrastructure/scheduler/scheduler_adapter.py`

```python
class SchedulerAdapter:
    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        
    def start_daily_scraping(self):
        """Inicia agendamento di√°rio de scraping"""
        self.scheduler.add_job(
            func=self.execute_daily_scraping,
            trigger="cron",
            hour=8,  # 8h da manh√£
            minute=0,
            timezone="America/Sao_Paulo",
            id="daily_scraping",
            replace_existing=True
        )
        
        self.scheduler.start()
        logger.info("Agendamento di√°rio de scraping iniciado")
    
    async def execute_daily_scraping(self):
        """Executa scraping di√°rio automaticamente"""
        try:
            criteria = ScrapingCriteria(
                publication_date=datetime.now().date() - timedelta(days=1),  # Dia anterior
                caderno="3",
                instancia="1",
                search_terms=settings.REQUIRED_TERMS
            )
            
            orchestrator = get_scraping_orchestrator()
            result = await orchestrator.execute_scraping(criteria)
            
            logger.info(f"Scraping di√°rio conclu√≠do: {result.publications_saved} publica√ß√µes salvas")
            
        except Exception as e:
            logger.error(f"Erro no scraping di√°rio: {e}")
            await self.send_error_alert(e)
```

### Monitoramento e Alertas

#### 1. Sistema de Monitoramento

**Localiza√ß√£o**: `backend/scraper/src/infrastructure/monitoring/monitoring_service.py`

```python
class MonitoringService:
    def __init__(self, api_client):
        self.api_client = api_client
        
    async def track_execution(self, execution_id: str, metrics: Dict):
        """Registra m√©tricas de execu√ß√£o"""
        await self.api_client.post("/api/scraping/executions/{execution_id}/metrics", metrics)
        
    async def send_health_check(self):
        """Envia status de sa√∫de do scraper"""
        health_data = {
            "status": "healthy",
            "last_execution": self.get_last_execution_time(),
            "system_metrics": self.get_system_metrics(),
            "pending_tasks": self.get_pending_tasks_count()
        }
        
        await self.api_client.post("/api/scraping/health", health_data)
    
    def get_system_metrics(self) -> Dict:
        """Coleta m√©tricas do sistema"""
        import psutil
        
        return {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "uptime": time.time() - psutil.boot_time()
        }
```

---

## ‚öõÔ∏è Frontend React

### Arquitetura do Frontend

O frontend √© desenvolvido em React com TypeScript, seguindo padr√µes modernos de desenvolvimento:

```mermaid
graph TB
    subgraph "Components Layer"
        Pages[Pages]
        Components[UI Components]
        Layout[Layout Components]
    end
    
    subgraph "State Management"
        Context[React Context]
        Hooks[Custom Hooks]
        LocalState[Local State]
    end
    
    subgraph "Services Layer"
        API[API Service]
        Auth[Auth Service]
        Storage[Storage Service]
    end
    
    subgraph "Utils Layer"
        Validation[Validation]
        Formatting[Formatting]
        Constants[Constants]
    end
    
    Pages --> Components
    Pages --> Context
    Components --> Hooks
    Context --> API
    Hooks --> API
    API --> Auth
    Components --> Validation
    Components --> Formatting
```

### Componentes Principais

#### 1. KanbanBoard Component

**Localiza√ß√£o**: `frontend/src/components/KanbanBoard.tsx`

Componente central da aplica√ß√£o que implementa o board Kanban:

```typescript
interface KanbanBoardProps {
  filters: SearchFilters
}

export function KanbanBoard({ filters }: KanbanBoardProps) {
  const [columns, setColumns] = useState<Map<PublicationStatus, KanbanColumn>>(new Map())
  const [selectedPublication, setSelectedPublication] = useState<Publication | null>(null)
  const [loading, setLoading] = useState(true)
  const [loadingMore, setLoadingMore] = useState<Set<PublicationStatus>>(new Set())
  
  // Configura√ß√£o das colunas
  const COLUMN_CONFIG: Record<PublicationStatus, { title: React.ReactNode; color: string }> = {
    NOVA: { title: 'Nova Publica√ß√£o', color: 'bg-blue-50 border-blue-200' },
    LIDA: { title: 'Publica√ß√£o Lida', color: 'bg-yellow-50 border-yellow-200' },
    ENVIADA_PARA_ADV: { title: 'Enviar para Advogado', color: 'bg-orange-50 border-orange-200' },
    CONCLUIDA: { title: 'Conclu√≠do', color: 'bg-green-50 border-green-200' }
  }
  
  // Carregamento de dados com pagina√ß√£o
  const loadPublications = useCallback(async (
    status: PublicationStatus,
    page: number = 1,
    reset: boolean = false
  ) => {
    try {
      const response = await apiService.getPublications(page, 30, {
        ...filters,
        status
      })
      
      setColumns(prev => {
        const newColumns = new Map(prev)
        const existingColumn = newColumns.get(status)
        const existingPublications = reset ? [] : existingColumn?.publications || []
        
        newColumns.set(status, {
          id: status,
          title: COLUMN_CONFIG[status].title,
          publications: [...existingPublications, ...response.data],
          count: response.total || 0
        })
        
        return newColumns
      })
      
    } catch (error) {
      toast({
        title: "Erro ao carregar publica√ß√µes",
        description: `Erro ao carregar ${status}`,
        variant: "destructive"
      })
    }
  }, [filters, toast])
  
  // Drag and Drop
  const onDragEnd = useCallback(async (result: DropResult) => {
    if (!result.destination) return
    
    const sourceStatus = result.source.droppableId as PublicationStatus
    const destStatus = result.destination.droppableId as PublicationStatus
    
    if (sourceStatus === destStatus) return
    
    // Validar movimento
    if (!isValidMove(sourceStatus, destStatus)) {
      toast({
        title: "Movimento n√£o permitido",
        description: "Este movimento n√£o √© permitido pelas regras do sistema",
        variant: "destructive"
      })
      return
    }
    
    try {
      // Atualizar no servidor
      await apiService.updatePublicationStatus(publicationId, destStatus)
      
      // Atualizar estado local
      updateColumnState(publicationId, sourceStatus, destStatus)
      
    } catch (error) {
      toast({
        title: "Erro ao mover publica√ß√£o",
        description: "N√£o foi poss√≠vel alterar o status da publica√ß√£o",
        variant: "destructive"
      })
    }
  }, [])
  
  return (
    <DragDropContext onDragEnd={onDragEnd}>
      <div className="flex gap-6 h-full overflow-x-auto">
        {columnOrder.map(status => (
          <KanbanColumn
            key={status}
            status={status}
            column={columns.get(status)}
            loading={loading || loadingMore.has(status)}
            onLoadMore={() => loadMoreItems(status)}
            onPublicationClick={setSelectedPublication}
          />
        ))}
      </div>
      
      {selectedPublication && (
        <PublicationModal
          publication={selectedPublication}
          isOpen={isModalOpen}
          onClose={() => setIsModalOpen(false)}
        />
      )}
    </DragDropContext>
  )
}
```

#### 2. SearchFilters Component

**Localiza√ß√£o**: `frontend/src/components/SearchFilters.tsx`

Implementa o sistema de filtros avan√ßados:

```typescript
interface SearchFiltersProps {
  onFiltersChange: (filters: SearchFilters) => void
  initialFilters?: SearchFilters
}

export function SearchFilters({ onFiltersChange, initialFilters }: SearchFiltersProps) {
  const [filters, setFilters] = useState<SearchFilters>(initialFilters || {})
  const [showAdvanced, setShowAdvanced] = useState(false)
  
  const handleFilterChange = (key: keyof SearchFilters, value: any) => {
    const newFilters = { ...filters, [key]: value }
    setFilters(newFilters)
    onFiltersChange(newFilters)
  }
  
  return (
    <div className="bg-white p-4 rounded-lg border shadow-sm">
      {/* Filtros B√°sicos */}
      <div className="grid grid-cols-1 md:grid-cols-4 gap-4">
        <Input
          placeholder="Buscar por texto..."
          value={filters.search || ''}
          onChange={(e) => handleFilterChange('search', e.target.value)}
        />
        
        <Input
          placeholder="N√∫mero do processo"
          value={filters.processNumber || ''}
          onChange={(e) => handleFilterChange('processNumber', e.target.value)}
        />
        
        <Input
          type="date"
          placeholder="Data inicial"
          value={filters.startDate || ''}
          onChange={(e) => handleFilterChange('startDate', e.target.value)}
        />
        
        <Input
          type="date"
          placeholder="Data final"
          value={filters.endDate || ''}
          onChange={(e) => handleFilterChange('endDate', e.target.value)}
        />
      </div>
      
      {/* Filtros Avan√ßados */}
      {showAdvanced && (
        <div className="mt-4 grid grid-cols-1 md:grid-cols-3 gap-4">
          <Input
            placeholder="Autores"
            value={filters.authors || ''}
            onChange={(e) => handleFilterChange('authors', e.target.value)}
          />
          
          <Input
            placeholder="R√©u"
            value={filters.defendant || ''}
            onChange={(e) => handleFilterChange('defendant', e.target.value)}
          />
          
          <Select
            value={filters.status}
            onValueChange={(value) => handleFilterChange('status', value)}
          >
            <SelectTrigger>
              <SelectValue placeholder="Status" />
            </SelectTrigger>
            <SelectContent>
              <SelectItem value="NOVA">Nova</SelectItem>
              <SelectItem value="LIDA">Lida</SelectItem>
              <SelectItem value="ENVIADA_PARA_ADV">Enviada</SelectItem>
              <SelectItem value="CONCLUIDA">Conclu√≠da</SelectItem>
            </SelectContent>
          </Select>
        </div>
      )}
      
      {/* Controles */}
      <div className="mt-4 flex justify-between items-center">
        <Button
          variant="outline"
          onClick={() => setShowAdvanced(!showAdvanced)}
        >
          {showAdvanced ? 'Ocultar' : 'Mostrar'} Filtros Avan√ßados
        </Button>
        
        <div className="flex gap-2">
          <Button
            variant="outline"
            onClick={() => {
              setFilters({})
              onFiltersChange({})
            }}
          >
            Limpar Filtros
          </Button>
          
          <Button onClick={() => onFiltersChange(filters)}>
            Aplicar Filtros
          </Button>
        </div>
      </div>
    </div>
  )
}
```

### Servi√ßos de API

#### 1. API Service

**Localiza√ß√£o**: `frontend/src/services/api.ts`

Centraliza todas as chamadas para a API:

```typescript
class ApiService {
  private baseURL: string
  private axios: AxiosInstance
  
  constructor() {
    this.baseURL = import.meta.env.VITE_API_URL || 'http://localhost:3001'
    
    this.axios = axios.create({
      baseURL: this.baseURL,
      timeout: 30000,
      headers: {
        'Content-Type': 'application/json'
      }
    })
    
    this.setupInterceptors()
  }
  
  private setupInterceptors() {
    // Request interceptor - adiciona token de autentica√ß√£o
    this.axios.interceptors.request.use(
      (config) => {
        const token = localStorage.getItem('accessToken')
        if (token) {
          config.headers.Authorization = `Bearer ${token}`
        }
        return config
      },
      (error) => Promise.reject(error)
    )
    
    // Response interceptor - trata refresh de token
    this.axios.interceptors.response.use(
      (response) => response,
      async (error) => {
        if (error.response?.status === 401) {
          await this.refreshToken()
          // Retry request original
          return this.axios.request(error.config)
        }
        return Promise.reject(error)
      }
    )
  }
  
  // Autentica√ß√£o
  async login(email: string, password: string): Promise<AuthResponse> {
    const response = await this.axios.post('/api/auth/login', { email, password })
    this.setTokens(response.data.data.tokens)
    return response.data
  }
  
  async register(userData: RegisterData): Promise<AuthResponse> {
    const response = await this.axios.post('/api/auth/register', userData)
    this.setTokens(response.data.data.tokens)
    return response.data
  }
  
  async refreshToken(): Promise<void> {
    const refreshToken = localStorage.getItem('refreshToken')
    if (!refreshToken) {
      this.logout()
      return
    }
    
    try {
      const response = await this.axios.post('/api/auth/refresh', { refreshToken })
      this.setTokens(response.data.data.tokens)
    } catch (error) {
      this.logout()
      throw error
    }
  }
  
  // Publica√ß√µes
  async getPublications(
    page: number = 1,
    limit: number = 20,
    filters: SearchFilters = {}
  ): Promise<PaginatedResponse<Publication>> {
    const params = new URLSearchParams({
      page: page.toString(),
      limit: limit.toString(),
      ...Object.fromEntries(
        Object.entries(filters).filter(([_, v]) => v != null && v !== '')
      )
    })
    
    const response = await this.axios.get(`/api/publications?${params}`)
    return response.data
  }
  
  async getPublicationById(id: string): Promise<Publication> {
    const response = await this.axios.get(`/api/publications/${id}`)
    return response.data.data
  }
  
  async updatePublicationStatus(
    id: string,
    status: PublicationStatus,
    notes?: string
  ): Promise<Publication> {
    const response = await this.axios.put(`/api/publications/${id}/status`, {
      status,
      notes
    })
    return response.data.data
  }
  
  // Utilit√°rios
  private setTokens(tokens: { accessToken: string; refreshToken: string }) {
    localStorage.setItem('accessToken', tokens.accessToken)
    localStorage.setItem('refreshToken', tokens.refreshToken)
  }
  
  logout() {
    localStorage.removeItem('accessToken')
    localStorage.removeItem('refreshToken')
    window.location.href = '/login'
  }
  
  isAuthenticated(): boolean {
    return !!localStorage.getItem('accessToken')
  }
}

export const apiService = new ApiService()
```

### Hooks Customizados

#### 1. usePublications Hook

**Localiza√ß√£o**: `frontend/src/hooks/usePublications.ts`

```typescript
export function usePublications(filters: SearchFilters) {
  const [publications, setPublications] = useState<Publication[]>([])
  const [loading, setLoading] = useState(true)
  const [error, setError] = useState<string | null>(null)
  const [pagination, setPagination] = useState({
    page: 1,
    totalPages: 1,
    total: 0
  })
  
  const loadPublications = useCallback(async (page: number = 1, reset: boolean = false) => {
    try {
      setLoading(true)
      setError(null)
      
      const response = await apiService.getPublications(page, 20, filters)
      
      setPublications(prev => reset ? response.data : [...prev, ...response.data])
      setPagination({
        page: response.pagination.page,
        totalPages: response.pagination.totalPages,
        total: response.pagination.total
      })
      
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Erro desconhecido')
    } finally {
      setLoading(false)
    }
  }, [filters])
  
  const loadMore = useCallback(() => {
    if (pagination.page < pagination.totalPages && !loading) {
      loadPublications(pagination.page + 1, false)
    }
  }, [pagination, loading, loadPublications])
  
  const refresh = useCallback(() => {
    loadPublications(1, true)
  }, [loadPublications])
  
  useEffect(() => {
    loadPublications(1, true)
  }, [loadPublications])
  
  return {
    publications,
    loading,
    error,
    pagination,
    loadMore,
    refresh,
    hasMore: pagination.page < pagination.totalPages
  }
}
```

# üîß Documenta√ß√£o T√©cnica - JusCash (Parte 3)

## üê≥ Infraestrutura e DevOps

### Containeriza√ß√£o com Docker

#### 1. Docker Compose Configuration

**Localiza√ß√£o**: `docker-compose.yml`

```yaml
version: '3.8'

services:
  # Banco de Dados PostgreSQL
  postgres:
    image: postgres:16-alpine
    container_name: juscash_postgres
    environment:
      POSTGRES_DB: juscash_db
      POSTGRES_USER: juscash_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - juscash_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U juscash_user -d juscash_db"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Cache Redis
  redis:
    image: redis:7-alpine
    container_name: juscash_redis
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - redis_data:/data
      - ./database/redis/redis.conf:/usr/local/etc/redis/redis.conf
    ports:
      - "6379:6379"
    networks:
      - juscash_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # API Backend
  api:
    build:
      context: ./backend/api
      dockerfile: Dockerfile
      target: production
    container_name: juscash_api
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://juscash_user:${POSTGRES_PASSWORD}@postgres:5432/juscash_db
      REDIS_URL: redis://redis:6379
      JWT_ACCESS_SECRET: ${JWT_ACCESS_SECRET}
      JWT_REFRESH_SECRET: ${JWT_REFRESH_SECRET}
      API_PORT: 3001
    ports:
      - "3001:3001"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - juscash_network
    healthcheck:
      test: ["CMD", "curl", "-f", "${API_BASE_URL}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Scraper Python
  scraper:
    build:
      context: ./backend/scraper
      dockerfile: Dockerfile
    container_name: juscash_scraper
    environment:
      DATABASE_URL: postgresql://juscash_user:${POSTGRES_PASSWORD}@postgres:5432/juscash_db
      REDIS_URL: redis://redis:6379
      API_BASE_URL: http://api:3001
      SCRAPING_HEADLESS: "true"
      SCRAPING_TIMEOUT: 30000
    volumes:
      - scraper_data:/app/data
      - scraper_logs:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    networks:
      - juscash_network
    restart: unless-stopped

  # Frontend Web
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        VITE_API_URL: http://localhost:3001
    container_name: juscash_frontend
    ports:
      - "3000:80"
    depends_on:
      - api
    networks:
      - juscash_network
    restart: unless-stopped

  # Proxy Reverso Nginx
  nginx:
    image: nginx:alpine
    container_name: juscash_nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - frontend
      - api
    networks:
      - juscash_network
    restart: unless-stopped

networks:
  juscash_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  scraper_data:
  scraper_logs:
```

#### 2. Dockerfile - API Backend

**Localiza√ß√£o**: `backend/api/Dockerfile`

```dockerfile
# Multi-stage build para otimiza√ß√£o
FROM node:20-alpine AS base

# Instalar depend√™ncias do sistema
RUN apk add --no-cache \
    libc6-compat \
    ca-certificates \
    tzdata

# Configurar timezone
ENV TZ=America/Sao_Paulo

WORKDIR /app

# Copiar arquivos de depend√™ncias
COPY package.json pnpm-lock.yaml ./
COPY prisma ./prisma/

# Instalar pnpm
RUN npm install -g pnpm

# === STAGE: Dependencies ===
FROM base AS deps
RUN pnpm install --frozen-lockfile

# === STAGE: Development ===
FROM base AS development
COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Gerar cliente Prisma
RUN pnpm prisma generate

EXPOSE 3001
CMD ["pnpm", "dev"]

# === STAGE: Builder ===
FROM base AS builder
COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Gerar cliente Prisma e build
RUN pnpm prisma generate
RUN pnpm build

# === STAGE: Production ===
FROM node:20-alpine AS production

RUN apk add --no-cache \
    libc6-compat \
    ca-certificates \
    dumb-init

# Criar usu√°rio n√£o-root
RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 --gid 1001 --shell /bin/sh nodejs

WORKDIR /app

# Copiar arquivos necess√°rios
COPY --from=builder --chown=nodejs:nodejs /app/dist ./dist
COPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --from=builder --chown=nodejs:nodejs /app/package.json ./package.json
COPY --from=builder --chown=nodejs:nodejs /app/prisma ./prisma

USER nodejs

EXPOSE 3001

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node dist/health-check.js

# Usar dumb-init para handling de sinais
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "dist/main.js"]
```

#### 3. Dockerfile - Scraper Python

**Localiza√ß√£o**: `backend/scraper/Dockerfile`

```dockerfile
FROM python:3.11-slim

# Instalar depend√™ncias do sistema
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    ca-certificates \
    procps \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Instalar Playwright browsers
RUN pip install playwright==1.40.0
RUN playwright install --with-deps chromium

# Criar usu√°rio n√£o-root
RUN groupadd --gid 1001 scraper && \
    useradd --uid 1001 --gid scraper --shell /bin/bash --create-home scraper

WORKDIR /app

# Copiar requirements e instalar depend√™ncias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo da aplica√ß√£o
COPY --chown=scraper:scraper . .

# Criar diret√≥rios necess√°rios
RUN mkdir -p /app/data /app/logs && \
    chown -R scraper:scraper /app

USER scraper

# Health check
HEALTHCHECK --interval=60s --timeout=10s --start-period=30s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8080/health')" || exit 1

EXPOSE 8080

CMD ["python", "-m", "src.main"]
```

### Configura√ß√£o de Produ√ß√£o

#### 1. Nginx Configuration

**Localiza√ß√£o**: `nginx/nginx.conf`

```nginx
events {
    worker_connections 1024;
}

http {
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=auth:10m rate=5r/s;
    
    # Upstream servers
    upstream api_backend {
        server api:3001;
        keepalive 32;
    }
    
    upstream frontend_backend {
        server frontend:80;
        keepalive 32;
    }
    
    # Main server block
    server {
        listen 80;
        listen 443 ssl http2;
        server_name juscash.com.br www.juscash.com.br;
        
        # SSL Configuration
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
        ssl_prefer_server_ciphers off;
        
        # Security headers
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Strict-Transport-Security "max-age=31536000; includeSubDomains";
        
        # Gzip compression
        gzip on;
        gzip_comp_level 6;
        gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;
        
        # API routes
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://api_backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_cache_bypass $http_upgrade;
            
            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # Auth routes with stricter rate limiting
        location /api/auth/ {
            limit_req zone=auth burst=10 nodelay;
            
            proxy_pass http://api_backend;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
        
        # Frontend application
        location / {
            proxy_pass http://frontend_backend;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Cache static assets
            location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
                expires 1y;
                add_header Cache-Control "public, immutable";
            }
        }
        
        # Health check
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
```

---

## üîí Seguran√ßa

### Estrat√©gias de Seguran√ßa

#### 1. Autentica√ß√£o JWT

O sistema utiliza JWT com estrat√©gia de refresh token:

```typescript
// Configura√ß√£o JWT
const JWT_CONFIG = {
  accessToken: {
    expiresIn: '15m',
    algorithm: 'HS256'
  },
  refreshToken: {
    expiresIn: '7d',
    algorithm: 'HS256'
  }
}

// Middleware de autentica√ß√£o
export const authMiddleware = async (req: Request, res: Response, next: NextFunction) => {
  try {
    const token = extractTokenFromHeader(req.headers.authorization)
    
    if (!token) {
      return res.status(401).json({
        success: false,
        error: { code: 'NO_TOKEN', message: 'Token de acesso n√£o fornecido' }
      })
    }
    
    const decoded = jwt.verify(token, process.env.JWT_ACCESS_SECRET!) as JWTPayload
    
    // Verificar se usu√°rio ainda est√° ativo
    const user = await userRepository.findById(decoded.userId)
    if (!user || !user.isActive) {
      return res.status(401).json({
        success: false,
        error: { code: 'USER_INACTIVE', message: 'Usu√°rio inativo ou n√£o encontrado' }
      })
    }
    
    req.user = user
    next()
    
  } catch (error) {
    if (error instanceof jwt.TokenExpiredError) {
      return res.status(401).json({
        success: false,
        error: { code: 'TOKEN_EXPIRED', message: 'Token expirado' }
      })
    }
    
    return res.status(401).json({
      success: false,
      error: { code: 'INVALID_TOKEN', message: 'Token inv√°lido' }
    })
  }
}
```

#### 2. Rate Limiting

Implementa√ß√£o de rate limiting por IP e usu√°rio:

```typescript
import rateLimit from 'express-rate-limit'
import RedisStore from 'rate-limit-redis'

// Store Redis para rate limiting
const redisStore = new RedisStore({
  sendCommand: (...args: string[]) => redisClient.call(...args)
})

// Rate limiting geral
export const generalRateLimit = rateLimit({
  store: redisStore,
  windowMs: 15 * 60 * 1000, // 15 minutos
  max: 100, // m√°ximo 100 requests por IP
  message: {
    success: false,
    error: {
      code: 'RATE_LIMIT_EXCEEDED',
      message: 'Muitas requisi√ß√µes. Tente novamente em alguns minutos.'
    }
  },
  standardHeaders: true,
  legacyHeaders: false
})

// Rate limiting para autentica√ß√£o
export const authRateLimit = rateLimit({
  store: redisStore,
  windowMs: 15 * 60 * 1000,
  max: 5, // m√°ximo 5 tentativas de login por IP
  skipSuccessfulRequests: true,
  message: {
    success: false,
    error: {
      code: 'AUTH_RATE_LIMIT_EXCEEDED',
      message: 'Muitas tentativas de login. Tente novamente em 15 minutos.'
    }
  }
})
```

#### 3. Valida√ß√£o e Sanitiza√ß√£o

Usando Zod para valida√ß√£o robusta:

```typescript
import { z } from 'zod'

// Schema de valida√ß√£o para registro
export const registerSchema = z.object({
  name: z.string()
    .min(2, 'Nome deve ter pelo menos 2 caracteres')
    .max(100, 'Nome deve ter no m√°ximo 100 caracteres')
    .regex(/^[a-zA-Z√Ä-√ø\s]+$/, 'Nome deve conter apenas letras e espa√ßos'),
    
  email: z.string()
    .email('Email deve ser v√°lido')
    .max(255, 'Email deve ter no m√°ximo 255 caracteres')
    .toLowerCase(),
    
  password: z.string()
    .min(8, 'Senha deve ter pelo menos 8 caracteres')
    .max(128, 'Senha deve ter no m√°ximo 128 caracteres')
    .regex(/^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*?&])[A-Za-z\d@$!%*?&]/, 
           'Senha deve conter ao menos: 1 min√∫scula, 1 mai√∫scula, 1 n√∫mero e 1 s√≠mbolo')
})

// Middleware de valida√ß√£o
export const validateSchema = (schema: z.ZodSchema) => {
  return (req: Request, res: Response, next: NextFunction) => {
    try {
      const validated = schema.parse(req.body)
      req.body = validated
      next()
    } catch (error) {
      if (error instanceof z.ZodError) {
        return res.status(400).json({
          success: false,
          error: {
            code: 'VALIDATION_ERROR',
            message: 'Dados inv√°lidos fornecidos',
            details: error.errors.map(err => ({
              field: err.path.join('.'),
              message: err.message
            }))
          }
        })
      }
      next(error)
    }
  }
}
```

#### 4. Prote√ß√£o contra Ataques

```typescript
import helmet from 'helmet'
import cors from 'cors'

// Configura√ß√£o de seguran√ßa
app.use(helmet({
  contentSecurityPolicy: {
    directives: {
      defaultSrc: ["'self'"],
      styleSrc: ["'self'", "'unsafe-inline'"],
      scriptSrc: ["'self'"],
      imgSrc: ["'self'", "data:", "https:"],
      connectSrc: ["'self'"],
      fontSrc: ["'self'"],
      objectSrc: ["'none'"],
      mediaSrc: ["'self'"],
      frameSrc: ["'none'"]
    }
  },
  crossOriginEmbedderPolicy: false
}))

// CORS configurado
app.use(cors({
  origin: (origin, callback) => {
    const allowedOrigins = process.env.CORS_ORIGIN?.split(',') || []
    if (!origin || allowedOrigins.includes(origin)) {
      callback(null, true)
    } else {
      callback(new Error('N√£o permitido pelo CORS'))
    }
  },
  credentials: true,
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
  allowedHeaders: ['Content-Type', 'Authorization']
}))
```

### Auditoria e Logs de Seguran√ßa

#### 1. Log de Autentica√ß√£o

```typescript
// AuthLog model para auditoria
interface AuthLogData {
  ipAddress?: string
  userAgent?: string
  email: string
  userId?: string
  action: 'LOGIN' | 'LOGOUT' | 'REGISTER' | 'PASSWORD_CHANGE'
  status: 'SUCCESS' | 'FAILED'
  failureReason?: string
  deviceInfo?: object
  locationInfo?: object
}

// Service para logs de autentica√ß√£o
export class AuthLogService {
  async logAuthEvent(data: AuthLogData): Promise<void> {
    await prisma.authLog.create({
      data: {
        ...data,
        createdAt: new Date()
      }
    })
    
    // Log cr√≠tico para arquivo
    if (data.status === 'FAILED') {
      logger.warn('Authentication failure', {
        email: data.email,
        ipAddress: data.ipAddress,
        action: data.action,
        failureReason: data.failureReason
      })
    }
  }
  
  async detectSuspiciousActivity(email: string): Promise<boolean> {
    const recentFailures = await prisma.authLog.count({
      where: {
        email,
        action: 'LOGIN',
        status: 'FAILED',
        createdAt: {
          gte: new Date(Date.now() - 15 * 60 * 1000) // √∫ltimos 15 minutos
        }
      }
    })
    
    return recentFailures >= 5
  }
}
```

---

## üìä Monitoramento e Logs

### Sistema de Logs

#### 1. Configura√ß√£o Winston

```typescript
import winston from 'winston'

// Configura√ß√£o do logger
export const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  defaultMeta: {
    service: 'juscash-api',
    version: process.env.npm_package_version
  },
  transports: [
    // Console para desenvolvimento
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    }),
    
    // Arquivo para produ√ß√£o
    new winston.transports.File({
      filename: 'logs/error.log',
      level: 'error',
      maxsize: 5242880, // 5MB
      maxFiles: 5
    }),
    
    new winston.transports.File({
      filename: 'logs/combined.log',
      maxsize: 5242880,
      maxFiles: 5
    })
  ]
})
```

#### 2. M√©tricas de Performance

```typescript
// Middleware de m√©tricas
export const metricsMiddleware = (req: Request, res: Response, next: NextFunction) => {
  const startTime = Date.now()
  
  res.on('finish', () => {
    const duration = Date.now() - startTime
    const route = req.route?.path || req.path
    
    logger.info('HTTP Request', {
      method: req.method,
      url: req.url,
      route,
      statusCode: res.statusCode,
      duration,
      userAgent: req.get('User-Agent'),
      ip: req.ip,
      userId: req.user?.id
    })
    
    // M√©tricas para Prometheus (se configurado)
    if (process.env.ENABLE_METRICS === 'true') {
      httpRequestDuration.observe(
        { method: req.method, route, status_code: res.statusCode },
        duration / 1000
      )
      
      httpRequestsTotal.inc({
        method: req.method,
        route,
        status_code: res.statusCode
      })
    }
  })
  
  next()
}
```

### Health Checks

#### 1. Health Check Endpoint

```typescript
// Health check completo
export const healthCheck = async (req: Request, res: Response) => {
  const startTime = Date.now()
  
  try {
    // Verificar conex√£o com banco
    const dbCheck = await checkDatabase()
    
    // Verificar conex√£o com Redis
    const redisCheck = await checkRedis()
    
    // Verificar m√©tricas do sistema
    const systemMetrics = await getSystemMetrics()
    
    const healthData = {
      status: 'healthy',
      timestamp: new Date().toISOString(),
      uptime: process.uptime(),
      version: process.env.npm_package_version,
      environment: process.env.NODE_ENV,
      responseTime: Date.now() - startTime,
      services: {
        database: dbCheck.status,
        redis: redisCheck.status
      },
      metrics: systemMetrics
    }
    
    const overallStatus = [dbCheck.status, redisCheck.status].every(status => status === 'healthy')
      ? 'healthy'
      : 'unhealthy'
    
    res.status(overallStatus === 'healthy' ? 200 : 503).json({
      success: true,
      data: { ...healthData, status: overallStatus }
    })
    
  } catch (error) {
    logger.error('Health check failed', error)
    
    res.status(503).json({
      success: false,
      data: {
        status: 'unhealthy',
        timestamp: new Date().toISOString(),
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    })
  }
}

async function checkDatabase(): Promise<{ status: string; responseTime: number }> {
  const start = Date.now()
  
  try {
    await prisma.$queryRaw`SELECT 1`
    return {
      status: 'healthy',
      responseTime: Date.now() - start
    }
  } catch (error) {
    return {
      status: 'unhealthy',
      responseTime: Date.now() - start
    }
  }
}
```

---

## ‚öôÔ∏è Instala√ß√£o e Configura√ß√£o

### Instala√ß√£o Completa

#### 1. Pr√©-requisitos do Sistema

**Para Desenvolvimento:**

```bash
# Node.js 20+
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs

# pnpm
npm install -g pnpm

# Python 3.11+
sudo apt-get install python3.11 python3.11-pip python3.11-venv

# Docker e Docker Compose
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo apt-get install docker-compose-plugin

# PostgreSQL (se n√£o usar Docker)
sudo apt-get install postgresql-16 postgresql-client-16

# Git
sudo apt-get install git
```

#### 2. Setup Automatizado

**Script de Instala√ß√£o:**

```bash
#!/bin/bash
# install.sh - Script de instala√ß√£o autom√°tica

set -e

echo "üöÄ Iniciando instala√ß√£o do JusCash..."

# Verificar pr√©-requisitos
command -v node >/dev/null 2>&1 || { echo "Node.js n√£o encontrado. Instale Node.js 20+"; exit 1; }
command -v python3 >/dev/null 2>&1 || { echo "Python3 n√£o encontrado. Instale Python 3.11+"; exit 1; }
command -v docker >/dev/null 2>&1 || { echo "Docker n√£o encontrado. Instale Docker"; exit 1; }

# Clonar reposit√≥rio (se necess√°rio)
if [ ! -d "juscash" ]; then
  echo "üì• Clonando reposit√≥rio..."
  git clone https://github.com/seu-usuario/juscash.git
  cd juscash
fi

# Configurar vari√°veis de ambiente
echo "üîß Configurando vari√°veis de ambiente..."
if [ ! -f .env ]; then
  cp .env.example .env
  
  # Gerar secrets JWT aleat√≥rios
  JWT_ACCESS_SECRET=$(openssl rand -base64 32)
  JWT_REFRESH_SECRET=$(openssl rand -base64 32)
  POSTGRES_PASSWORD=$(openssl rand -base64 16)
  
  sed -i "s/your-super-secret-access-key-min-32-chars/$JWT_ACCESS_SECRET/g" .env
  sed -i "s/your-super-secret-refresh-key-min-32-chars/$JWT_REFRESH_SECRET/g" .env
  sed -i "s/your-postgres-password/$POSTGRES_PASSWORD/g" .env
  
  echo "‚úÖ Arquivo .env criado com secrets aleat√≥rios"
fi

# Instalar depend√™ncias do backend
echo "üì¶ Instalando depend√™ncias do backend..."
cd backend/api
pnpm install
cd ../..

# Instalar depend√™ncias do frontend
echo "üì¶ Instalando depend√™ncias do frontend..."
cd frontend
pnpm install
cd ..

# Configurar Python virtual environment
echo "üêç Configurando ambiente Python..."
cd backend/scraper
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
python -m playwright install chromium
cd ../..

# Subir servi√ßos com Docker
echo "üê≥ Iniciando servi√ßos com Docker..."
docker-compose up -d postgres redis

# Aguardar servi√ßos ficarem prontos
echo "‚è≥ Aguardando servi√ßos ficarem prontos..."
sleep 10

# Executar migra√ß√µes do banco
echo "üóÑÔ∏è Executando migra√ß√µes do banco..."
cd backend/api
pnpm prisma migrate deploy
pnpm prisma generate
cd ../..

# Executar seeds (dados iniciais)
echo "üå± Populando dados iniciais..."
cd backend/api
pnpm db:seed
cd ../..

echo "‚úÖ Instala√ß√£o conclu√≠da com sucesso!"
echo ""
echo "üéØ Pr√≥ximos passos:"
echo "1. Inicie a API: cd backend/api && pnpm dev"
echo "2. Inicie o frontend: cd frontend && pnpm dev"
echo "3. Inicie o scraper: cd backend/scraper && source venv/bin/activate && python -m src.main"
echo ""
echo "üåê URLs de acesso:"
echo "   Frontend: http://localhost:3000"
echo "   API: http://localhost:3001"
echo "   API Docs: ${API_BASE_URL}-docs"
```

#### 3. Configura√ß√£o Manual

**Passo a Passo Detalhado:**

1. **Clonar Reposit√≥rio:**

```bash
git clone https://github.com/seu-usuario/juscash.git
cd juscash
```

2. **Configurar Vari√°veis de Ambiente:**

```bash
cp .env.example .env
# Editar .env com suas configura√ß√µes
```

3. **Backend API:**

```bash
cd backend/api
pnpm install
pnpm prisma generate
pnpm prisma migrate dev
pnpm db:seed
pnpm dev
```

4. **Frontend:**

```bash
cd frontend
pnpm install
pnpm dev
```

5. **Scraper Python:**

```bash
cd backend/scraper
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python -m playwright install chromium
python -m src.main --test
```

### Configura√ß√£o de Produ√ß√£o

#### 1. Vari√°veis de Ambiente

```bash
# .env.production
NODE_ENV=production
API_PORT=3001

# Database
DATABASE_URL=postgresql://user:password@host:5432/juscash_db

# JWT Secrets (ALTERAR EM PRODU√á√ÉO!)
JWT_ACCESS_SECRET=seu-secret-super-seguro-de-no-minimo-32-caracteres
JWT_REFRESH_SECRET=seu-refresh-secret-super-seguro-de-no-minimo-32-caracteres

# CORS
CORS_ORIGIN=https://seu-dominio.com,https://www.seu-dominio.com

# Rate Limiting
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100

# Logging
LOG_LEVEL=info

# Redis
REDIS_URL=redis://localhost:6379

# Scraper
SCRAPING_HEADLESS=true
SCRAPING_TIMEOUT=30000
SCRAPING_MAX_RETRIES=3
```

#### 2. Deploy com Docker

```bash
# Build e deploy
docker-compose -f docker-compose.prod.yml up -d

# Verificar sa√∫de dos servi√ßos
docker-compose ps
docker-compose logs -f

# Executar migra√ß√µes
docker-compose exec api pnpm prisma migrate deploy
```

#### 3. Monitoramento

```bash
# Logs em tempo real
docker-compose logs -f api
docker-compose logs -f scraper

# M√©tricas de sistema
curl ${API_BASE_URL}/health
curl ${API_BASE_URL}/metrics
```

---

## üìö Guias de Desenvolvimento

### Contribuindo para o Projeto

#### 1. Estrutura de Commits

```
tipo(escopo): descri√ß√£o

Tipos permitidos:
- feat: nova funcionalidade
- fix: corre√ß√£o de bug
- docs: documenta√ß√£o
- style: formata√ß√£o
- refactor: refatora√ß√£o
- test: testes
- chore: manuten√ß√£o

Exemplo:
feat(api): adicionar endpoint de busca avan√ßada
fix(scraper): corrigir extra√ß√£o de valores monet√°rios
docs(readme): atualizar guia de instala√ß√£o
```

#### 2. Padr√µes de C√≥digo

**TypeScript/JavaScript:**

```typescript
// Usar interfaces para tipos
interface Publication {
  id: string
  processNumber: string
  status: PublicationStatus
}

// Usar enums para constantes
enum PublicationStatus {
  NOVA = 'NOVA',
  LIDA = 'LIDA',
  ENVIADA_PARA_ADV = 'ENVIADA_PARA_ADV',
  CONCLUIDA = 'CONCLUIDA'
}

// Fun√ß√µes async/await
async function getPublications(filters: SearchFilters): Promise<Publication[]> {
  try {
    const result = await prisma.publication.findMany({
      where: buildWhereClause(filters)
    })
    return result
  } catch (error) {
    logger.error('Error fetching publications', error)
    throw error
  }
}
```

**Python:**

```python
# Type hints obrigat√≥rios
from typing import List, Optional, Dict, Any

# Classes com dataclasses
@dataclass
class Publication:
    id: str
    process_number: str
    content: str
    status: PublicationStatus
    created_at: datetime

# Async/await para I/O
async def extract_publications(criteria: ScrapingCriteria) -> List[Publication]:
    """Extrai publica√ß√µes baseado nos crit√©rios fornecidos."""
    try:
        publications = await scraper.extract(criteria)
        return [Publication.from_raw(pub) for pub in publications]
    except Exception as e:
        logger.error(f"Error extracting publications: {e}")
        raise
```

#### 3. Testing Strategy

**Backend Tests:**

```typescript
// Unit tests
describe('PublicationService', () => {
  it('should create publication with valid data', async () => {
    const publicationData = createValidPublicationData()
    const result = await publicationService.create(publicationData)
    
    expect(result).toBeDefined()
    expect(result.status).toBe(PublicationStatus.NOVA)
  })
})

// Integration tests
describe('Publication API', () => {
  it('should return paginated publications', async () => {
    const response = await request(app)
      .get('/api/publications')
      .set('Authorization', `Bearer ${validToken}`)
      .expect(200)
    
    expect(response.body.success).toBe(true)
    expect(response.body.data).toBeArray()
  })
})
```

### Troubleshooting Comum

#### 1. Problemas de Database

```bash
# Resetar database
pnpm prisma migrate reset

# Regenerar cliente
pnpm prisma generate

# Verificar conex√£o
pnpm prisma db pull
```

#### 2. Problemas do Scraper

```bash
# Testar scraper
python -m src.main --test-scraping

# Verificar browsers
python -m playwright install chromium

# Debug mode
SCRAPING_HEADLESS=false python -m src.main
```

#### 3. Problemas de Performance

```bash
# Analisar queries lentas
EXPLAIN ANALYZE SELECT * FROM publications WHERE ...

# Verificar √≠ndices
\d+ publications

# Monitorar conex√µes
SELECT count(*) FROM pg_stat_activity;
```

---

**Fim da Documenta√ß√£o T√©cnica Completa**

---

### üìÑ Informa√ß√µes da Documenta√ß√£o

**Vers√£o**: 1.0  
**Data**: Junho 2025
**Autore**: Junior Martins

**Contato T√©cnico**:

- Email: <amjr.box@gmail.com>
- GitHub: <https://github.com/juniromartinxo>

---

*Esta documenta√ß√£o t√©cnica fornece todos os detalhes necess√°rios para desenvolvimento, manuten√ß√£o e deploy do sistema JusCash.*
