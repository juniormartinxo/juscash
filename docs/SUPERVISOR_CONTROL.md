# Controle do Supervisor - Multi-Date Scraper

Este documento explica como controlar os processos do scraper usando o `supervisorctl`.

## üéõÔ∏è Processos Dispon√≠veis

### Processos Principais

1. **`multi_date_scraper`** - Scraper principal multi-data multi-worker
2. **`scraper_api`** - API do scraper 
3. **`file_monitor`** - Monitor de arquivos JSON
4. **`progress_monitor`** - Monitor de progresso (opcional)

### Processos Auxiliares

5. **`main_app`** - Scraper original (DESABILITADO para evitar conflitos)
6. **`scraper_cli`** - Interface CLI (manual)

## üöÄ Comandos de Controle

### Iniciar/Parar Processos

```bash
# Entrar no supervisorctl
supervisorctl

# Ver status de todos os processos
status

# Iniciar o multi-date scraper
start multi_date_scraper

# Parar o multi-date scraper (shutdown graceful)
stop multi_date_scraper

# Reiniciar o multi-date scraper
restart multi_date_scraper

# Iniciar monitor de progresso
start progress_monitor

# Parar monitor de progresso
stop progress_monitor
```

### Controle de M√∫ltiplos Processos

```bash
# Iniciar todos os processos essenciais
start scraper_api file_monitor multi_date_scraper

# Parar todos os processos
stop all

# Reiniciar todos os processos
restart all

# Ver logs em tempo real
tail -f multi_date_scraper
```

## üìä Configura√ß√µes Espec√≠ficas

### Multi-Date Scraper
```ini
[program:multi_date_scraper]
autostart=true                     # Inicia automaticamente
autorestart=unexpected             # S√≥ reinicia em falhas inesperadas
stopwaitsecs=30                     # 30s para shutdown graceful
stopsignal=SIGINT                   # Permite cleanup adequado
```

### Progress Monitor
```ini
[program:progress_monitor]
autostart=false                    # N√£o inicia automaticamente
autorestart=true                   # Reinicia se falhar
command=monitor_progress.py --interval 60  # Atualiza a cada 60s
```

## üîÑ Cen√°rios de Uso

### 1. Inicializa√ß√£o Completa do Sistema

```bash
supervisorctl start scraper_api
supervisorctl start file_monitor
supervisorctl start multi_date_scraper
supervisorctl start progress_monitor  # Opcional
```

### 2. Parar Tudo para Manuten√ß√£o

```bash
supervisorctl stop all
```

### 3. Reiniciar Apenas o Scraper

```bash
supervisorctl restart multi_date_scraper
```

### 4. Monitoramento

```bash
# Ver status
supervisorctl status

# Ver logs do scraper
supervisorctl tail -f multi_date_scraper

# Ver logs do monitor
supervisorctl tail -f progress_monitor
```

## üõ°Ô∏è Configura√ß√µes de Seguran√ßa

### Preven√ß√£o de Conflitos

- **`main_app`** est√° `autostart=false` para evitar conflito com `multi_date_scraper`
- Apenas um processo de scraping ativo por vez
- Prioridades configuradas para ordem correta de inicializa√ß√£o

### Shutdown Graceful

- `stopwaitsecs=30` - Tempo suficiente para salvar progresso
- `stopsignal=SIGINT` - Permite cleanup adequado
- `autorestart=unexpected` - N√£o reinicia quando termina normalmente

## üìù Status dos Processos

### Estados Poss√≠veis

- **RUNNING** - Processo executando normalmente
- **STOPPED** - Processo parado
- **STARTING** - Processo iniciando
- **STOPPING** - Processo parando
- **EXITED** - Processo terminou (c√≥digo de sa√≠da)
- **FATAL** - Processo falhou ao iniciar

### C√≥digos de Sa√≠da

- **0** - Termina√ß√£o normal (todas as datas processadas)
- **1** - Erro geral
- **2** - Interrup√ß√£o por usu√°rio (Ctrl+C)
- **130** - SIGINT recebido

## üîß Troubleshooting

### Scraper N√£o Inicia

```bash
# Ver logs detalhados
supervisorctl tail multi_date_scraper

# Verificar configura√ß√£o
supervisorctl status multi_date_scraper

# Tentar restart
supervisorctl restart multi_date_scraper
```

### Conflitos de Processo

```bash
# Verificar se main_app est√° rodando
supervisorctl status main_app

# Parar main_app se necess√°rio
supervisorctl stop main_app

# Iniciar multi_date_scraper
supervisorctl start multi_date_scraper
```

### Monitor N√£o Funciona

```bash
# Verificar se arquivo de progresso existe
ls -la src/scrap_workrs.json

# Iniciar manualmente para debug
python monitor_progress.py --once

# Iniciar via supervisor
supervisorctl start progress_monitor
```

## üìã Checklist de Inicializa√ß√£o

1. ‚úÖ Verificar se Docker est√° rodando
2. ‚úÖ Verificar se containers est√£o up
3. ‚úÖ Entrar no container do scraper
4. ‚úÖ Executar `supervisorctl status`
5. ‚úÖ Iniciar processos necess√°rios
6. ‚úÖ Verificar logs com `tail -f`

## üö® Alertas e Monitoramento

### Logs Importantes

- Processo iniciado: "üöÄ Iniciando scraping multi-data"
- Processo conclu√≠do: "‚úÖ Todas as datas foram processadas!"
- Erro cr√≠tico: "‚ùå Erro durante execu√ß√£o"
- Shutdown: "‚úÖ Shutdown graceful conclu√≠do"

### M√©tricas de Performance

- Datas processadas por hora
- Workers ativos
- Publica√ß√µes encontradas
- Taxa de erro por data

---

**Use este guia para controlar efetivamente o sistema de scraping multi-data.** 