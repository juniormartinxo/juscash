# CorreÃ§Ãµes Aplicadas ao Scraper Multi-Date

Este documento detalha as correÃ§Ãµes aplicadas baseadas nos logs de erro do sistema.

## ğŸ› Problemas Identificados nos Logs

### 1. Falha na SubmissÃ£o do FormulÃ¡rio
```
ERROR | âŒ NÃ£o foi possÃ­vel submeter o formulÃ¡rio
ERROR | âŒ Erro ao preencher formulÃ¡rio: Falha ao submeter formulÃ¡rio
```

### 2. Browser Fechado Prematuramente
```
ERROR | Page.query_selector: Target page, context or browser has been closed
INFO reaped unknown pid 1403 (terminated by SIGKILL)
```

### 3. Sobrecarga de Recursos
- 3 workers simultÃ¢neos causando problemas de memÃ³ria
- MÃºltiplos browsers consumindo recursos excessivos

## âœ… CorreÃ§Ãµes Aplicadas

### 1. Melhorias na SubmissÃ£o do FormulÃ¡rio

**Arquivo**: `src/infrastructure/web/dje_scraper_adapter.py`

- âœ… **Novos seletores** para botÃ£o de submissÃ£o
- âœ… **VerificaÃ§Ã£o de visibilidade** dos elementos
- âœ… **Fallback JavaScript** quando seletores falham
- âœ… **VerificaÃ§Ã£o de mudanÃ§a de URL** para confirmar submissÃ£o

```python
submit_selectors = [
    'input[value="Pesquisar"]',
    'input[name="pbConsultar"]',     # NOVO
    'button:text("Pesquisar")',
    'input[type="submit"]',
    'button[type="submit"]',
    '.botaoPesquisar',               # NOVO
    '#pbConsultar',                  # NOVO
    '[onclick*="consultar"]'         # NOVO
]
```

### 2. Melhorias no Cleanup do Browser

- âœ… **VerificaÃ§Ãµes de pÃ¡gina vÃ¡lida** antes do uso
- âœ… **Cleanup robusto** com tratamento de erros
- âœ… **VerificaÃ§Ã£o de pÃ¡gina fechada** antes de operaÃ§Ãµes

```python
if not self.page or self.page.is_closed():
    raise Exception("PÃ¡gina do browser foi fechada")
```

### 3. ReduÃ§Ã£o de ConcorrÃªncia

**Arquivo**: `multi_date_scraper.py`

- âœ… **Reduzido de 3 para 1 worker** para evitar sobrecarga
- âœ… **Timeout por data** (5 minutos mÃ¡ximo)
- âœ… **Delay entre processamentos** (2 segundos)
- âœ… **Cleanup individual** por worker

```python
NUM_WORKERS = 1  # Reduzido para evitar sobrecarga
timeout=300      # 5 minutos por data
await asyncio.sleep(2)  # Delay entre processamentos
```

### 4. Tratamento de Erros Melhorado

- âœ… **Timeout individual** por data processada
- âœ… **Isolamento de erros** entre workers
- âœ… **Retry inteligente** para falhas temporÃ¡rias
- âœ… **Cleanup automÃ¡tico** de recursos

## ğŸš€ Como Testar as CorreÃ§Ãµes

### 1. Teste Local (Desenvolvimento)

```bash
cd backend/scraper

# Resetar progresso (opcional)
rm -f src/scrap_workrs.json

# Executar scraper
python run_multi_date_scraper.py
```

### 2. Teste via Supervisor (ProduÃ§Ã£o)

```bash
# Entrar no supervisorctl
supervisorctl

# Ver status
status

# Parar processo atual (se rodando)
stop multi_date_scraper

# Iniciar com correÃ§Ãµes
start multi_date_scraper

# Monitorar logs
tail -f multi_date_scraper
```

### 3. Monitoramento de Progresso

```bash
# Monitor contÃ­nuo
python monitor_progress.py

# VerificaÃ§Ã£o Ãºnica
python monitor_progress.py --once
```

## ğŸ“Š Comportamento Esperado ApÃ³s CorreÃ§Ãµes

### Estados Normais

1. **InicializaÃ§Ã£o**:
   ```
   ğŸ”§ Inicializando datas de 17/03/2025 atÃ© 17/06/2025
   âœ… Inicializadas 93 datas para processamento
   ğŸ‘· Worker worker_1 iniciado
   ```

2. **Processamento**:
   ```
   ğŸ“… Configurando data especÃ­fica: 17/03/2025...
   âœ… Datas configuradas: 17/03/2025 atÃ© 17/03/2025
   ğŸ¯ Tentando submeter com selector: input[value="Pesquisar"]
   âœ… FormulÃ¡rio submetido com sucesso
   ```

3. **ConclusÃ£o**:
   ```
   âœ… Data 17/03/2025 processada pelo worker worker_1: X publicaÃ§Ãµes encontradas
   ğŸ§¹ Cleanup worker worker_1 concluÃ­do
   ```

### Tratamento de Erros

1. **Falha na submissÃ£o**:
   ```
   ğŸ”„ Tentando submissÃ£o via JavaScript...
   âœ… FormulÃ¡rio submetido via JavaScript
   ```

2. **Timeout**:
   ```
   â° Timeout processando data 17/03/2025 no worker worker_1
   ğŸ”„ Data 17/03/2025 recolocada na fila (tentativa 1/3)
   ```

3. **Browser fechado**:
   ```
   âŒ PÃ¡gina do browser foi fechada
   ğŸ§¹ Cleanup worker worker_1 concluÃ­do
   ```

## ğŸ› ï¸ ConfiguraÃ§Ãµes Ajustadas

### Timeouts e Delays

```python
# Timeout por data
timeout=300  # 5 minutos

# Delay entre processamentos  
await asyncio.sleep(2)

# Timeout de navegaÃ§Ã£o
await self.page.wait_for_load_state("networkidle", timeout=15000)
```

### Recursos

```python
# Workers reduzidos
NUM_WORKERS = 1

# Retry por data
max_retries = 3

# VerificaÃ§Ãµes de saÃºde
if not self.page or self.page.is_closed():
    raise Exception("PÃ¡gina do browser foi fechada")
```

## ğŸ“ˆ MÃ©tricas de Sucesso

- âœ… **0 browsers mortos** com SIGKILL
- âœ… **FormulÃ¡rios submetidos** sem falha
- âœ… **Workers estÃ¡veis** sem crashes
- âœ… **Progresso salvo** apÃ³s cada data
- âœ… **Cleanup adequado** de recursos

## ğŸ”§ Troubleshooting

### Se ainda houver falhas na submissÃ£o:

1. **Verificar conectividade**:
   ```bash
   curl -I https://esaj.tjsp.jus.br/cdje/
   ```

2. **Screenshot de debug**:
   - Procurar por arquivos `debug_*.png` em `/app/debug/`

3. **Tentar submissÃ£o manual**:
   - Acessar o site e verificar mudanÃ§as na interface

### Se workers continuarem falhando:

1. **Reduzir timeout**:
   ```python
   timeout=180  # 3 minutos
   ```

2. **Aumentar delay**:
   ```python
   await asyncio.sleep(5)  # 5 segundos
   ```

3. **Verificar recursos do sistema**:
   ```bash
   htop  # CPU e memÃ³ria
   df -h  # EspaÃ§o em disco
   ```

---

**As correÃ§Ãµes foram aplicadas para tornar o scraper mais robusto e estÃ¡vel.** ğŸ¯ 