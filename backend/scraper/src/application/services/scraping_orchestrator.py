"""
Orquestrador principal do processo de scraping
"""

import subprocess
import asyncio
import sys
import os
from pathlib import Path
from typing import List
from datetime import datetime, timedelta
from uuid import uuid4

from application.usecases.extract_publications import ExtractPublicationsUseCase
from application.usecases.save_publications_to_files import (
    SavePublicationsToFilesUseCase,
)
from domain.entities.scraping_execution import ScrapingExecution, ExecutionType
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


class ScrapingOrchestrator:
    """
    Orquestra todo o processo de scraping di√°rio
    Agora usa o scraping.py que funciona completamente
    """

    def __init__(self, container):
        self.container = container
        # Manter os use cases para compatibilidade, mas usar scraping.py como m√©todo principal
        self.extract_usecase = ExtractPublicationsUseCase(container.web_scraper)
        self.save_usecase = SavePublicationsToFilesUseCase()

    async def execute_daily_scraping(self) -> ScrapingExecution:
        """
        Executa o processo completo de scraping di√°rio usando o scraping.py
        """
        execution = ScrapingExecution(
            execution_id=str(uuid4()),
            execution_type=ExecutionType.SCHEDULED,
            criteria_used={
                "search_terms": ["RPV", "pagamento pelo INSS"],
                "caderno": "3",
                "instancia": "1",
                "local": "Capital",
                "parte": "1",
                "method": "scraping_py_integration",
            },
        )

        try:
            logger.info(f"üöÄ Iniciando execu√ß√£o di√°ria {execution.execution_id}")

            # Usar scraping.py para fazer o scraping real
            result = await self._execute_scraping_py()

            if result["success"]:
                execution.publications_found = result.get("publications_found", 0)
                execution.publications_saved = result.get("publications_saved", 0)
                execution.publications_failed = result.get("publications_failed", 0)
                execution.publications_new = result.get("publications_saved", 0)
                execution.publications_duplicated = 0

                execution.mark_as_completed()
                logger.info(
                    f"‚úÖ Execu√ß√£o {execution.execution_id} conclu√≠da com sucesso"
                )
                logger.info(
                    f"üì§ {execution.publications_found} publica√ß√µes encontradas"
                )
                logger.info(f"üíæ {execution.publications_saved} publica√ß√µes salvas")
            else:
                raise Exception(
                    f"Falha no scraping.py: {result.get('error', 'Erro desconhecido')}"
                )

        except Exception as error:
            logger.error(f"‚ùå Erro na execu√ß√£o {execution.execution_id}: {error}")
            execution.mark_as_failed(
                {"error": str(error), "type": type(error).__name__}
            )

        # Salvar informa√ß√µes da execu√ß√£o (logs locais)
        if hasattr(self.container, "scraping_repository"):
            await self.container.scraping_repository.save_scraping_execution(execution)

        return execution

    async def _execute_scraping_py(self) -> dict:
        """
        Executa o scraping.py para fazer o scraping real
        """
        try:
            # Data atual (para execu√ß√£o no mesmo dia)
            today = datetime.now()
            date_str = today.strftime("%Y-%m-%d")

            # Caminho para o scraping.py
            scraper_dir = Path(__file__).parent.parent.parent.parent
            scraping_py_path = scraper_dir / "scraping.py"

            if not scraping_py_path.exists():
                raise FileNotFoundError(
                    f"scraping.py n√£o encontrado em: {scraping_py_path}"
                )

            logger.info(f"üìÖ Executando scraping para data: {date_str}")
            logger.info(f"üìÑ Usando script: {scraping_py_path}")

            # Comando para executar o scraping.py
            cmd = [
                sys.executable,  # python
                str(scraping_py_path),
                "run",
                "--start-date",
                date_str,
                "--end-date",
                date_str,
                "--headless",  # Sempre headless no agendamento
            ]

            logger.info(f"üîÑ Executando comando: {' '.join(cmd)}")

            # Executar o comando
            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(scraper_dir),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env={**dict(os.environ), "PYTHONPATH": str(scraper_dir)},
            )

            stdout, stderr = await process.communicate()

            # Logs do processo
            if stdout:
                stdout_text = stdout.decode("utf-8", errors="ignore")
                logger.info(f"üìä Output do scraping.py:\n{stdout_text}")

                # Extrair estat√≠sticas do output
                stats = self._parse_scraping_output(stdout_text)
            else:
                stats = {
                    "publications_found": 0,
                    "publications_saved": 0,
                    "publications_failed": 0,
                }

            if stderr:
                stderr_text = stderr.decode("utf-8", errors="ignore")
                logger.warning(f"‚ö†Ô∏è Stderr do scraping.py:\n{stderr_text}")

            # Verificar c√≥digo de sa√≠da
            if process.returncode == 0:
                logger.info("‚úÖ scraping.py executado com sucesso")
                return {"success": True, **stats}
            else:
                error_msg = f"scraping.py falhou com c√≥digo {process.returncode}"
                logger.error(f"‚ùå {error_msg}")
                return {"success": False, "error": error_msg}

        except Exception as error:
            logger.error(f"‚ùå Erro ao executar scraping.py: {error}")
            return {"success": False, "error": str(error)}

    def _parse_scraping_output(self, output: str) -> dict:
        """
        Extrai estat√≠sticas do output do scraping.py
        """
        import re

        stats = {
            "publications_found": 0,
            "publications_saved": 0,
            "publications_failed": 0,
        }

        try:
            # Buscar padr√µes no output
            # Exemplo: "üìÑ Publica√ß√µes encontradas: 25"
            found_match = re.search(r"Publica√ß√µes encontradas:\s*(\d+)", output)
            if found_match:
                stats["publications_found"] = int(found_match.group(1))

            # Exemplo: "‚úÖ Publica√ß√µes salvas: 20"
            saved_match = re.search(r"Publica√ß√µes salvas:\s*(\d+)", output)
            if saved_match:
                stats["publications_saved"] = int(saved_match.group(1))

            # Exemplo: "‚ùå Falhas: 5"
            failed_match = re.search(r"Falhas:\s*(\d+)", output)
            if failed_match:
                stats["publications_failed"] = int(failed_match.group(1))

            logger.info(f"üìä Estat√≠sticas extra√≠das: {stats}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair estat√≠sticas: {e}")

        return stats

    # Manter m√©todo original como fallback
    async def execute_daily_scraping_original(self) -> ScrapingExecution:
        """
        M√©todo original usando DJEScraperAdapter (manter como fallback)
        """
        execution = ScrapingExecution(
            execution_id=str(uuid4()),
            execution_type=ExecutionType.SCHEDULED,
            criteria_used={
                "search_terms": ["RPV", "pagamento pelo INSS"],
                "caderno": "3",
                "instancia": "1",
                "local": "Capital",
                "parte": "1",
                "method": "original_adapter",
            },
        )

        try:
            logger.info(f"üöÄ Iniciando execu√ß√£o original {execution.execution_id}")

            # Termos de busca obrigat√≥rios
            search_terms = ["RPV", "pagamento pelo INSS"]

            # Extrair publica√ß√µes
            publications = []
            async for publication in self.extract_usecase.execute(
                search_terms, max_pages=20
            ):
                publications.append(publication)
                execution.publications_found += 1

            # Salvar publica√ß√µes em JSON
            if publications:
                save_stats = await self.save_usecase.execute(publications)
                execution.publications_new = save_stats["saved"]
                execution.publications_failed = save_stats["failed"]
                execution.publications_saved = save_stats["saved"]
                execution.publications_duplicated = 0

                # Log das estat√≠sticas
                file_stats = self.save_usecase.get_file_stats()
                logger.info(f"üìä Estat√≠sticas dos arquivos: {file_stats}")

            execution.mark_as_completed()
            logger.info(f"‚úÖ Execu√ß√£o original {execution.execution_id} conclu√≠da")

        except Exception as error:
            logger.error(
                f"‚ùå Erro na execu√ß√£o original {execution.execution_id}: {error}"
            )
            execution.mark_as_failed(
                {"error": str(error), "type": type(error).__name__}
            )

        return execution
