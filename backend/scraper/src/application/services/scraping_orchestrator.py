"""
Orquestrador principal do processo de scraping
"""

from typing import List
from datetime import datetime
from uuid import uuid4

from application.usecases.extract_publications import ExtractPublicationsUseCase
from application.usecases.save_publications import SavePublicationsUseCase
from domain.entities.scraping_execution import ScrapingExecution, ExecutionType
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


class ScrapingOrchestrator:
    """
    Orquestra todo o processo de scraping di√°rio
    """

    def __init__(self, container):
        self.container = container
        self.extract_usecase = ExtractPublicationsUseCase(container.web_scraper)
        self.save_usecase = SavePublicationsUseCase()  # Agora usa fila Redis

    async def execute_daily_scraping(self) -> ScrapingExecution:
        """
        Executa o processo completo de scraping di√°rio
        """
        execution = ScrapingExecution(
            execution_id=str(uuid4()),
            execution_type=ExecutionType.SCHEDULED,
            criteria_used={
                "search_terms": [
                    "RPV",
                    "pagamento pelo INSS",
                ],  # Termos corretos para RPV
                "caderno": "3",
                "instancia": "1",
                "local": "Capital",
                "parte": "1",
            },
        )

        try:
            logger.info(f"üöÄ Iniciando execu√ß√£o {execution.execution_id}")

            # Termos de busca obrigat√≥rios (podem vir de configura√ß√£o)
            search_terms = ["RPV", "pagamento pelo INSS"]  # Configur√°vel

            # Extrair publica√ß√µes
            publications = []
            async for publication in self.extract_usecase.execute(
                search_terms, max_pages=20
            ):
                publications.append(publication)
                execution.publications_found += 1

            # Enfileirar publica√ß√µes para processamento ass√≠ncrono
            if publications:
                save_stats = await self.save_usecase.execute(publications)
                execution.publications_new = save_stats[
                    "enqueued"
                ]  # Mudan√ßa: agora s√£o enfileiradas
                execution.publications_failed = save_stats["failed"]
                execution.publications_saved = save_stats[
                    "enqueued"
                ]  # Ser√£o processadas pelo worker
                execution.publications_duplicated = (
                    0  # Duplica√ß√£o ser√° verificada pelo worker
                )

                # Log das estat√≠sticas da fila
                queue_stats = self.save_usecase.get_queue_stats()
                logger.info(f"üìä Estat√≠sticas da fila Redis: {queue_stats}")

            execution.mark_as_completed()
            logger.info(f"‚úÖ Execu√ß√£o {execution.execution_id} conclu√≠da com sucesso")
            logger.info(f"üì§ {execution.publications_found} publica√ß√µes extra√≠das")
            logger.info(f"üìù {execution.publications_new} publica√ß√µes enfileiradas")

        except Exception as error:
            logger.error(f"‚ùå Erro na execu√ß√£o {execution.execution_id}: {error}")
            execution.mark_as_failed(
                {"error": str(error), "type": type(error).__name__}
            )

        # Salvar informa√ß√µes da execu√ß√£o (logs locais)
        if hasattr(self.container, "scraping_repository"):
            await self.container.scraping_repository.save_scraping_execution(execution)

        return execution
