"""
Orquestrador principal do processo de scraping
"""

from typing import List
from datetime import datetime
from uuid import uuid4

from application.usecases.extract_publications import ExtractPublicationsUseCase
from application.usecases.save_publications_to_files import SavePublicationsToFilesUseCase
from domain.entities.scraping_execution import ScrapingExecution, ExecutionType
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


class ScrapingOrchestrator:
    """
    Orquestra todo o processo de scraping di√°rio
    """

    def __init__(self, container):
        self.container = container
        self.extract_usecase = ExtractPublicationsUseCase(container.web_scraper)
        self.save_usecase = SavePublicationsToFilesUseCase()  # Agora salva em arquivos locais

    async def execute_daily_scraping(self) -> ScrapingExecution:
        """
        Executa o processo completo de scraping di√°rio
        """
        execution = ScrapingExecution(
            execution_id=str(uuid4()),
            execution_type=ExecutionType.SCHEDULED,
            criteria_used={
                "search_terms": [
                    "RPV",
                    "pagamento pelo INSS",
                ],  # Termos corretos para RPV
                "caderno": "3",
                "instancia": "1",
                "local": "Capital",
                "parte": "1",
            },
        )

        try:
            logger.info(f"üöÄ Iniciando execu√ß√£o {execution.execution_id}")

            # Termos de busca obrigat√≥rios (podem vir de configura√ß√£o)
            search_terms = ["RPV", "pagamento pelo INSS"]  # Configur√°vel

            # Extrair publica√ß√µes
            publications = []
            async for publication in self.extract_usecase.execute(
                search_terms, max_pages=20
            ):
                publications.append(publication)
                execution.publications_found += 1

            # Salvar publica√ß√µes em arquivos locais (TXT e JSON)
            if publications:
                save_stats = await self.save_usecase.execute(publications)
                execution.publications_new = save_stats["saved"]
                execution.publications_failed = save_stats["failed"]
                execution.publications_saved = save_stats["saved"]
                execution.publications_duplicated = 0  # N√£o h√° verifica√ß√£o de duplica√ß√£o local

                # Log das estat√≠sticas dos arquivos
                file_stats = self.save_usecase.get_file_stats()
                logger.info(f"üìä Estat√≠sticas dos arquivos: {file_stats}")

            execution.mark_as_completed()
            logger.info(f"‚úÖ Execu√ß√£o {execution.execution_id} conclu√≠da com sucesso")
            logger.info(f"üì§ {execution.publications_found} publica√ß√µes extra√≠das")
            logger.info(f"üíæ {execution.publications_saved} publica√ß√µes salvas em arquivos")

        except Exception as error:
            logger.error(f"‚ùå Erro na execu√ß√£o {execution.execution_id}: {error}")
            execution.mark_as_failed(
                {"error": str(error), "type": type(error).__name__}
            )

        # Salvar informa√ß√µes da execu√ß√£o (logs locais)
        if hasattr(self.container, "scraping_repository"):
            await self.container.scraping_repository.save_scraping_execution(execution)

        return execution
