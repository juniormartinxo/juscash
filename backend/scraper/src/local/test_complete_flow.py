#!/usr/bin/env python3
"""
Teste do fluxo completo: encontrar links, baixar PDFs, processar conteÃºdo
"""

import sys
import asyncio
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from infrastructure.web.dje_scraper_adapter import DJEScraperAdapter
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


async def test_complete_pdf_flow():
    """Testa o fluxo completo com download real de PDFs"""

    print("ğŸ§ª Teste do Fluxo Completo com PDFs")
    print("=" * 50)
    print("ğŸ’¡ Vai baixar e processar PDFs reais!")
    print()

    scraper = DJEScraperAdapter()
    publications_found = 0

    try:
        # Inicializar
        await scraper.initialize()

        print("ğŸ” Executando scraping completo...")
        search_terms = ["INSS"]
        max_pages = 1  # Limitar a 1 pÃ¡gina para teste

        # Usar o mÃ©todo principal do scraper
        async for publication in scraper.scrape_publications(search_terms, max_pages):
            publications_found += 1

            print(f"\nğŸ“„ PUBLICAÃ‡ÃƒO {publications_found}:")
            print(f"   ğŸ”¢ Processo: {publication.process_number}")
            print(f"   ğŸ“… Data: {publication.publication_date}")
            print(
                f"   ğŸ‘¥ Autores: {len(publication.authors)} ({', '.join(publication.authors[:2])}...)"
            )
            print(f"   âš–ï¸ Advogados: {len(publication.lawyers)}")
            print(
                f"   ğŸ’° Valores: {len([v for v in [publication.gross_value, publication.net_value, publication.interest_value, publication.attorney_fees] if v])}"
            )
            print(f"   ğŸ“ ConteÃºdo: {len(publication.content)} chars")

            # Mostrar alguns valores se existirem
            if publication.interest_value:
                print(f"   ğŸ’µ Juros: R$ {publication.interest_value.amount}")
            if publication.attorney_fees:
                print(f"   ğŸ’¼ HonorÃ¡rios: R$ {publication.attorney_fees.amount}")

            # Limitar teste a 3 publicaÃ§Ãµes
            if publications_found >= 3:
                print(f"\nâš ï¸ Limitando teste a {publications_found} publicaÃ§Ãµes")
                break

        print(f"\nğŸ¯ RESULTADO FINAL:")
        print(f"   âœ… {publications_found} publicaÃ§Ãµes processadas com sucesso")

        return publications_found > 0

    except Exception as error:
        print(f"âŒ Erro durante teste: {error}")
        import traceback

        traceback.print_exc()
        return False

    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    print("ğŸš€ Teste Completo do Fluxo PDF")
    print("ğŸ’¡ Este teste vai:")
    print("   1. Acessar a consulta avanÃ§ada")
    print("   2. Encontrar elementos tr.ementaClass")
    print("   3. Extrair URLs dinÃ¢micas dos PDFs")
    print("   4. Baixar PDFs reais")
    print("   5. Processar conteÃºdo com o parser aprimorado")
    print("   6. Extrair publicaÃ§Ãµes estruturadas")
    print()

    success = asyncio.run(test_complete_pdf_flow())

    if success:
        print("\nğŸ‰ TESTE COMPLETO PASSOU!")
        print("âœ… O fluxo de PDFs dinÃ¢micos estÃ¡ funcionando perfeitamente")
        print("ğŸ’¡ Agora vocÃª pode executar: py scraper_cli.py run")
    else:
        print("\nâŒ TESTE FALHOU")
        print("âš ï¸ Verifique os logs para mais detalhes")

    sys.exit(0 if success else 1)
