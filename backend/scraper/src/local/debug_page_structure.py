#!/usr/bin/env python3
"""
Debug da estrutura da pÃ¡gina para encontrar os elementos corretos
"""

import sys
import asyncio
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from infrastructure.web.dje_scraper_adapter import DJEScraperAdapter
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


async def debug_page_structure():
    """Debug completo da estrutura da pÃ¡gina"""

    print("ğŸ” Debug da Estrutura da PÃ¡gina DJE")
    print("=" * 50)

    scraper = DJEScraperAdapter()

    try:
        # Inicializar
        await scraper.initialize()

        # Navegar
        print("ğŸ“ Navegando para consulta avanÃ§ada...")
        await scraper._navigate_to_advanced_search()

        # Preencher formulÃ¡rio com termos mais amplos
        print("ğŸ“ Preenchendo formulÃ¡rio com termos amplos...")
        search_terms = ["INSS"]  # Termo mais amplo
        await scraper._fill_advanced_search_form(search_terms)

        # Aguardar resultados
        await asyncio.sleep(3)

        # Debug: tÃ­tulo da pÃ¡gina
        title = await scraper.page.title()
        print(f"ğŸ“„ TÃ­tulo da pÃ¡gina: {title}")

        # Debug: URL atual
        url = scraper.page.url
        print(f"ğŸŒ URL atual: {url}")

        # Verificar se hÃ¡ resultados
        print("\nğŸ” ANÃLISE DE ELEMENTOS:")
        print("-" * 30)

        # 1. Buscar tr.ementaClass
        ementa_elements = await scraper.page.query_selector_all("tr.ementaClass")
        print(f"ğŸ“‹ tr.ementaClass: {len(ementa_elements)} encontrados")

        # 2. Buscar variaÃ§Ãµes de class
        class_variations = [
            'tr[class*="ementa"]',
            'tr[class*="Ementa"]',
            'tr[class*="resultado"]',
            'tr[class*="publicacao"]',
            'tr[class*="linha"]',
        ]

        for selector in class_variations:
            elements = await scraper.page.query_selector_all(selector)
            print(f"ğŸ“‹ {selector}: {len(elements)} encontrados")

        # 3. Buscar todos os tr com class
        all_tr_with_class = await scraper.page.query_selector_all("tr[class]")
        print(f"ğŸ“‹ tr[class] (todos): {len(all_tr_with_class)} encontrados")

        # Mostrar as primeiras 10 classes
        if all_tr_with_class:
            print("   ğŸ“ Primeiras classes encontradas:")
            for i, tr in enumerate(all_tr_with_class[:10]):
                class_name = await tr.get_attribute("class")
                print(f"      {i+1}. class='{class_name}'")

        # 4. Buscar elementos com onclick
        onclick_elements = await scraper.page.query_selector_all('[onclick*="popup"]')
        print(f"ğŸ”— Elementos com onclick popup: {len(onclick_elements)} encontrados")

        if onclick_elements:
            print("   ğŸ“ Primeiros onclick encontrados:")
            for i, element in enumerate(onclick_elements[:5]):
                onclick_attr = await element.get_attribute("onclick")
                tag_name = await element.evaluate("el => el.tagName")
                print(
                    f"      {i+1}. <{tag_name.lower()}> onclick='{onclick_attr[:100]}...'"
                )

        # 5. Buscar por consultaSimples.do
        consulta_elements = await scraper.page.query_selector_all(
            '[onclick*="consultaSimples.do"]'
        )
        print(
            f"ğŸ“„ Elementos com consultaSimples.do: {len(consulta_elements)} encontrados"
        )

        # 6. Verificar se hÃ¡ mensagem de "nenhum resultado"
        no_results_selectors = [
            ':text("Nenhum resultado")',
            ':text("NÃ£o foram encontrados")',
            ':text("sem resultados")',
            ".mensagem",
            ".aviso",
            ".erro",
        ]

        for selector in no_results_selectors:
            elements = await scraper.page.query_selector_all(selector)
            if elements:
                for element in elements:
                    text = await element.inner_text()
                    print(f"âš ï¸ Mensagem encontrada: '{text}'")

        # 7. Capturar screenshot para anÃ¡lise
        screenshot_path = "debug_page_structure.png"
        await scraper.page.screenshot(path=screenshot_path, full_page=True)
        print(f"ğŸ“¸ Screenshot salvo: {screenshot_path}")

        # 8. Salvar HTML da pÃ¡gina
        html_content = await scraper.page.content()
        html_path = "debug_page_content.html"
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"ğŸ’¾ HTML salvo: {html_path}")

        # 9. Buscar tabelas
        tables = await scraper.page.query_selector_all("table")
        print(f"ğŸ“Š Tabelas encontradas: {len(tables)}")

        if tables:
            for i, table in enumerate(tables[:3]):
                rows = await table.query_selector_all("tr")
                print(f"   Tabela {i+1}: {len(rows)} linhas")

        return True

    except Exception as error:
        print(f"âŒ Erro durante debug: {error}")
        import traceback

        traceback.print_exc()
        return False

    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    print("ğŸš€ Debug da Estrutura da PÃ¡gina DJE")
    print("ğŸ’¡ Vamos descobrir a estrutura real da pÃ¡gina")
    print()

    success = asyncio.run(debug_page_structure())

    if success:
        print("\nâœ… DEBUG CONCLUÃDO")
        print("ğŸ“‹ Verifique os arquivos gerados:")
        print("   â€¢ debug_page_structure.png - Screenshot da pÃ¡gina")
        print("   â€¢ debug_page_content.html - HTML completo")
        print("ğŸ’¡ Use essas informaÃ§Ãµes para ajustar os seletores")
    else:
        print("\nâŒ DEBUG FALHOU")

    sys.exit(0 if success else 1)
