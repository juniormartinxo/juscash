#!/usr/bin/env python3
"""
Teste do scraper principal com critÃ©rios corretos
"""

import sys
import asyncio
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from infrastructure.web.dje_scraper_adapter import DJEScraperAdapter
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


async def test_main_scraper():
    """Testa o scraper principal com critÃ©rios especÃ­ficos"""

    print("ğŸš€ Teste do Scraper Principal")
    print("=" * 40)
    print("ğŸ“‹ CritÃ©rios:")
    print("   ğŸ“… Data: 13/11/2024")
    print("   ğŸ“‹ Caderno: 3 - Judicial - 1Âª InstÃ¢ncia - Capital - Parte I")
    print('   ğŸ” Palavras: "RPV" e "pagamento pelo INSS"')
    print()

    scraper = DJEScraperAdapter()
    publications_found = 0

    try:
        # Inicializar
        await scraper.initialize()

        # Executar scraping
        print("ğŸ” Iniciando scraping...")

        async for publication in scraper.scrape_publications(
            search_terms=["RPV", "pagamento pelo INSS"], max_pages=2
        ):
            publications_found += 1
            print(f"\nğŸ“„ PUBLICAÃ‡ÃƒO {publications_found}:")
            print(f"   ğŸ“… Data PublicaÃ§Ã£o: {publication.publication_date}")
            print(f"   ğŸ“… Data DisponibilizaÃ§Ã£o: {publication.availability_date}")
            print(f"   ğŸ“‹ Processo: {publication.process_number}")
            print(f"   ğŸ‘¤ Autores: {', '.join(publication.authors)}")
            print(f"   âš–ï¸ Advogados: {len(publication.lawyers)} advogado(s)")
            print(
                f"   ğŸ’° Valor Bruto: {publication.gross_value.to_real() if publication.gross_value else 'N/A'}"
            )
            print(
                f"   ğŸ’° Valor LÃ­quido: {publication.net_value.to_real() if publication.net_value else 'N/A'}"
            )
            print(f"   ğŸ“ ConteÃºdo: {publication.content[:200]}...")

            # Limitar para nÃ£o sobrecarregar
            if publications_found >= 5:
                print("\nâš ï¸ Limitando a 5 publicaÃ§Ãµes para teste")
                break

        return publications_found > 0

    except Exception as error:
        print(f"âŒ Erro durante scraping: {error}")
        import traceback

        traceback.print_exc()
        return False

    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    print("ğŸ¯ Teste do Scraper Principal com CritÃ©rios EspecÃ­ficos")
    print("ğŸ’¡ Usando data 13/11/2024 e termos especÃ­ficos")
    print()

    success = asyncio.run(test_main_scraper())

    if success:
        print("\nğŸ‰ TESTE PASSOU!")
        print("âœ… PublicaÃ§Ãµes encontradas e processadas")
    else:
        print("\nâš ï¸ TESTE FALHOU")
        print("âŒ Nenhuma publicaÃ§Ã£o encontrada ou erro no processamento")

    sys.exit(0 if success else 1)
