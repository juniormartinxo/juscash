#!/usr/bin/env python3
"""
Teste do novo fluxo de scraping com download de PDFs
"""

import sys
import asyncio
from pathlib import Path

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from infrastructure.web.dje_scraper_adapter import DJEScraperAdapter
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


async def test_pdf_scraper_flow():
    """Testa o novo fluxo com download de PDFs"""

    print("ğŸ§ª Teste do Novo Fluxo com Download de PDFs")
    print("=" * 60)

    scraper = DJEScraperAdapter()

    try:
        # Inicializar scraper
        print("ğŸŒ Inicializando scraper...")
        await scraper.initialize()

        # Testar navegaÃ§Ã£o para consulta avanÃ§ada
        print("ğŸ“ Navegando para consulta avanÃ§ada...")
        await scraper._navigate_to_advanced_search()

        # Testar preenchimento do formulÃ¡rio
        print("ğŸ“ Preenchendo formulÃ¡rio de pesquisa...")
        search_terms = ["aposentadoria", "benefÃ­cio", "INSS"]
        await scraper._fill_advanced_search_form(search_terms)

        # Buscar por elementos tr.ementaClass
        print("ğŸ” Buscando elementos tr.ementaClass...")
        ementa_elements = await scraper.page.query_selector_all("tr.ementaClass")

        if ementa_elements:
            print(f"âœ… Encontrados {len(ementa_elements)} elementos tr.ementaClass")

            # Verificar primeiro elemento
            for i, element in enumerate(
                ementa_elements[:3]
            ):  # Testar apenas os primeiros 3
                print(f"\nğŸ“‹ Analisando elemento {i+1}:")

                # Buscar elementos com onclick
                onclick_elements = await element.query_selector_all(
                    '[onclick*="popup"]'
                )
                print(f"   ğŸ”— Elementos com onclick: {len(onclick_elements)}")

                for j, onclick_element in enumerate(onclick_elements):
                    onclick_attr = await onclick_element.get_attribute("onclick")
                    if onclick_attr and "consultaSimples.do" in onclick_attr:
                        print(f"   ğŸ“„ Link {j+1}: {onclick_attr[:100]}...")

                        # Extrair URL
                        pdf_url = await scraper._extract_pdf_url_from_onclick(
                            onclick_attr
                        )
                        if pdf_url:
                            print(f"   âœ… URL extraÃ­da: {pdf_url}")

                            # Para teste, nÃ£o vamos baixar todos os PDFs
                            print(f"   âš ï¸ Download de PDF desabilitado para teste")
                            break

                if i >= 2:  # Limitar teste a 3 elementos
                    break
        else:
            print("âŒ Nenhum elemento tr.ementaClass encontrado")

            # Debug: verificar se a pÃ¡gina carregou corretamente
            title = await scraper.page.title()
            print(f"ğŸ“„ TÃ­tulo da pÃ¡gina: {title}")

            # Verificar se hÃ¡ outros elementos de resultado
            other_elements = await scraper.page.query_selector_all("tr")
            print(f"ğŸ” Total de elementos tr encontrados: {len(other_elements)}")

            # Verificar elementos com class
            class_elements = await scraper.page.query_selector_all("tr[class]")
            print(f"ğŸ” Elementos tr com class: {len(class_elements)}")

            if class_elements:
                for i, el in enumerate(class_elements[:5]):
                    class_name = await el.get_attribute("class")
                    print(f"   TR {i+1}: class='{class_name}'")

        print("\nğŸ¯ Teste concluÃ­do!")
        return True

    except Exception as error:
        print(f"âŒ Erro durante teste: {error}")
        logger.error(f"Erro no teste: {error}")
        import traceback

        traceback.print_exc()
        return False

    finally:
        await scraper.cleanup()


async def test_pdf_url_extraction():
    """Testa apenas a extraÃ§Ã£o de URL do onclick"""
    print("\nğŸ§ª Teste de ExtraÃ§Ã£o de URL do PDF")
    print("=" * 40)

    scraper = DJEScraperAdapter()

    # Exemplos de onclick reais
    test_onclicks = [
        "return popup('/cdje/consultaSimples.do?cdVolume=19&nuDiario=4092&cdCaderno=12&nuSeqpagina=3710');",
        "return popup('/cdje/consultaSimples.do?cdVolume=20&nuDiario=4093&cdCaderno=12&nuSeqpagina=3711');",
        "popup('/cdje/consultaSimples.do?cdVolume=21&nuDiario=4094&cdCaderno=12&nuSeqpagina=3712');",
    ]

    for i, onclick in enumerate(test_onclicks):
        print(f"\nğŸ”— Teste {i+1}: {onclick}")
        url = await scraper._extract_pdf_url_from_onclick(onclick)
        if url:
            print(f"   âœ… URL extraÃ­da: {url}")
        else:
            print(f"   âŒ Falha na extraÃ§Ã£o")


if __name__ == "__main__":
    print("ğŸš€ Iniciando teste do novo fluxo PDF scraper...")
    print("ğŸ’¡ Este teste verifica se conseguimos acessar e extrair links dos PDFs")
    print()

    # Primeiro teste: extraÃ§Ã£o de URL
    asyncio.run(test_pdf_url_extraction())

    print("\n" + "=" * 60)

    # Segundo teste: fluxo completo
    success = asyncio.run(test_pdf_scraper_flow())

    if success:
        print(f"\nâœ… TESTE PASSOU - Novo fluxo implementado com sucesso!")
        print(f"ğŸ’¡ Agora Ã© necessÃ¡rio instalar as dependÃªncias de PDF")
        print(f"ğŸ“‹ Execute: pip install PyPDF2 pdfplumber")
    else:
        print(f"\nâŒ TESTE FALHOU - Ainda hÃ¡ problemas a resolver")

    sys.exit(0 if success else 1)
