import asyncio
import sys
import signal
from datetime import datetime, time
from typing import Optional

from src.config.settings import settings
from src.config.database import create_tables, close_db_connections
from src.shared.logger import get_logger, setup_logging
from src.shared.value_objects import ScrapingCriteria

# Imports dos adaptadores
from src.adapters.secondary.playwright_scraper import PlaywrightScraperAdapter
from src.adapters.secondary.sqlalchemy_repository import EnhancedSQLAlchemyRepository as SQLAlchemyRepository
from src.adapters.secondary.redis_cache import RedisCacheAdapter
from src.adapters.primary.scheduler_adapter import APSchedulerAdapter

# Imports dos casos de uso
from src.core.usecases.scrape_publications import ScrapePublicationsUseCase
from src.core.usecases.schedule_scraping import ScheduleScrapingUseCase

# Setup inicial do logging
setup_logging()
logger = get_logger(__name__)


class DJEScrapingApplication:
    """
    AplicaÃ§Ã£o principal do sistema de scraping do DJE.
    
    Coordena a inicializaÃ§Ã£o de todos os componentes e gerencia
    o ciclo de vida da aplicaÃ§Ã£o seguindo a arquitetura hexagonal.
    """
    
    def __init__(self):
        """Inicializa a aplicaÃ§Ã£o com configuraÃ§Ãµes padrÃ£o."""
        self.settings = settings
        
        # Adaptadores (infraestrutura)
        self.scraper_adapter: Optional[PlaywrightScraperAdapter] = None
        self.database_adapter: Optional[SQLAlchemyRepository] = None
        self.cache_adapter: Optional[RedisCacheAdapter] = None
        self.scheduler_adapter: Optional[APSchedulerAdapter] = None
        
        # Casos de uso (aplicaÃ§Ã£o)
        self.scrape_use_case: Optional[ScrapePublicationsUseCase] = None
        self.schedule_use_case: Optional[ScheduleScrapingUseCase] = None
        
        # Estado da aplicaÃ§Ã£o
        self.is_running = False
        self.shutdown_event = asyncio.Event()
        
        # Configurar handlers para sinais do sistema
        self._setup_signal_handlers()
    
    def _setup_signal_handlers(self):
        """Configura handlers para sinais do sistema (SIGINT, SIGTERM)."""
        if sys.platform != "win32":
            # Unix/Linux
            for sig in (signal.SIGTERM, signal.SIGINT):
                signal.signal(sig, self._signal_handler)
        else:
            # Windows
            signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Handler para sinais de shutdown."""
        logger.info(f"Sinal recebido: {signum}. Iniciando shutdown graceful...")
        self.shutdown_event.set()
    
    async def initialize(self) -> None:
        """
        Inicializa todos os componentes da aplicaÃ§Ã£o.
        
        Raises:
            Exception: Se houver erro na inicializaÃ§Ã£o de qualquer componente
        """
        logger.info("ðŸš€ Iniciando aplicaÃ§Ã£o DJE Scraping...")
        
        try:
            # 1. Inicializar banco de dados
            logger.info("ðŸ“Š Inicializando banco de dados...")
            #await create_tables()
            
            self.database_adapter = SQLAlchemyRepository(
                database_url=self.settings.database.url
            )
            logger.info("âœ… Banco de dados inicializado")
            
            # 2. Inicializar cache Redis (opcional)
            try:
                logger.info("ðŸ”„ Inicializando cache Redis...")
                self.cache_adapter = RedisCacheAdapter(
                    redis_url=self.settings.redis.url
                )
                await self.cache_adapter.initialize()
                logger.info("âœ… Cache Redis inicializado")
            except Exception as e:
                logger.warning(f"âš ï¸ Cache Redis nÃ£o disponÃ­vel: {str(e)}")
                self.cache_adapter = None
            
            # 3. Inicializar scraper Playwright
            logger.info("ðŸŒ Inicializando scraper Playwright...")
            self.scraper_adapter = PlaywrightScraperAdapter(
                headless=self.settings.scraping.headless,
                timeout=self.settings.scraping.timeout,
                user_agent=self.settings.scraping.user_agent,
                max_retries=self.settings.scraping.max_retries
            )
            await self.scraper_adapter.initialize()
            logger.info("âœ… Scraper Playwright inicializado")
            
            # 4. Inicializar agendador
            logger.info("â° Inicializando agendador...")
            self.scheduler_adapter = APSchedulerAdapter()
            await self.scheduler_adapter.initialize()
            logger.info("âœ… Agendador inicializado")
            
            # 5. Inicializar casos de uso
            logger.info("ðŸŽ¯ Inicializando casos de uso...")
            self.scrape_use_case = ScrapePublicationsUseCase(
                scraper=self.scraper_adapter,
                database=self.database_adapter,
                cache=self.cache_adapter
            )
            
            self.schedule_use_case = ScheduleScrapingUseCase(
                scheduler=self.scheduler_adapter,
                scrape_use_case=self.scrape_use_case
            )
            logger.info("âœ… Casos de uso inicializados")
            
            self.is_running = True
            logger.info("ðŸŽ‰ AplicaÃ§Ã£o inicializada com sucesso!")
            
        except Exception as e:
            logger.error(f"âŒ Erro na inicializaÃ§Ã£o da aplicaÃ§Ã£o: {str(e)}")
            await self.cleanup()
            raise
    
    async def run_immediate_scraping(self) -> dict:
        """
        Executa scraping imediato (uma vez).
        
        Returns:
            DicionÃ¡rio com estatÃ­sticas da execuÃ§Ã£o
            
        Raises:
            Exception: Se aplicaÃ§Ã£o nÃ£o estiver inicializada ou houver erro
        """
        if not self.is_running or not self.scrape_use_case:
            raise RuntimeError("AplicaÃ§Ã£o nÃ£o foi inicializada")
        
        logger.info("ðŸ”„ Executando scraping imediato...")
        
        # Criar critÃ©rios padrÃ£o
        criteria = ScrapingCriteria(
            required_terms=tuple(self.settings.scraping.required_terms),
            caderno=self.settings.dje_caderno,
            instancia=self.settings.dje_instancia,
            local=self.settings.dje_local,
            parte=self.settings.dje_parte
        )
        
        # Executar scraping
        stats = await self.scrape_use_case.execute(
            criteria=criteria,
            skip_duplicates=True
        )
        
        logger.info(f"âœ… Scraping imediato concluÃ­do: {stats}")
        return stats
    
    async def setup_scheduled_scraping(self) -> str:
        """
        Configura execuÃ§Ã£o automÃ¡tica diÃ¡ria do scraping.
        
        Returns:
            ID do job agendado
            
        Raises:
            Exception: Se aplicaÃ§Ã£o nÃ£o estiver inicializada ou houver erro
        """
        if not self.is_running or not self.schedule_use_case:
            raise RuntimeError("AplicaÃ§Ã£o nÃ£o foi inicializada")
        
        logger.info("ðŸ“… Configurando scraping automÃ¡tico...")
        
        # Parsear data de inÃ­cio
        start_date = datetime.strptime(
            self.settings.scheduler.start_date, 
            "%Y-%m-%d"
        )
        
        # Configurar horÃ¡rio de execuÃ§Ã£o
        execution_time = time(
            hour=self.settings.scheduler.execution_hour,
            minute=self.settings.scheduler.execution_minute
        )
        
        # Criar critÃ©rios padrÃ£o
        criteria = ScrapingCriteria(
            required_terms=tuple(self.settings.scraping.required_terms),
            caderno=self.settings.dje_caderno,
            instancia=self.settings.dje_instancia,
            local=self.settings.dje_local,
            parte=self.settings.dje_parte
        )
        
        # Configurar agendamento
        job_id = await self.schedule_use_case.setup_daily_execution(
            start_date=start_date,
            execution_time=execution_time,
            criteria=criteria
        )
        
        # Iniciar agendador
        await self.schedule_use_case.start_scheduler()
        
        logger.info(f"âœ… Scraping automÃ¡tico configurado. Job ID: {job_id}")
        logger.info(f"ðŸ“… ExecuÃ§Ãµes diÃ¡rias a partir de {start_date.date()} Ã s {execution_time}")
        
        return job_id
    
    async def run_with_scheduler(self) -> None:
        """
        Executa a aplicaÃ§Ã£o com agendamento automÃ¡tico.
        
        MantÃ©m a aplicaÃ§Ã£o rodando e aguarda sinal de shutdown.
        """
        if not self.is_running:
            raise RuntimeError("AplicaÃ§Ã£o nÃ£o foi inicializada")
        
        logger.info("ðŸ”„ Iniciando modo agendado...")
        
        try:
            # Configurar scraping automÃ¡tico
            job_id = await self.setup_scheduled_scraping()
            
            logger.info("â° AplicaÃ§Ã£o rodando em modo agendado. Pressione Ctrl+C para parar.")
            
            # Aguardar sinal de shutdown
            while not self.shutdown_event.is_set():
                await asyncio.sleep(1)
            
            logger.info("ðŸ›‘ Sinal de shutdown recebido")
            
        except KeyboardInterrupt:
            logger.info("ðŸ›‘ InterrupÃ§Ã£o via teclado recebida")
        except Exception as e:
            logger.error(f"âŒ Erro durante execuÃ§Ã£o agendada: {str(e)}")
            raise
        finally:
            await self.cleanup()
    
    async def cleanup(self) -> None:
        """
        Limpa recursos e fecha conexÃµes.
        """
        logger.info("ðŸ§¹ Iniciando limpeza de recursos...")
        
        self.is_running = False
        
        # Parar agendador
        if self.scheduler_adapter:
            try:
                await self.scheduler_adapter.stop()
                logger.info("âœ… Agendador parado")
            except Exception as e:
                logger.error(f"âŒ Erro ao parar agendador: {str(e)}")
        
        # Fechar scraper
        if self.scraper_adapter:
            try:
                await self.scraper_adapter.close()
                logger.info("âœ… Scraper fechado")
            except Exception as e:
                logger.error(f"âŒ Erro ao fechar scraper: {str(e)}")
        
        # Fechar cache
        if self.cache_adapter:
            try:
                await self.cache_adapter.close()
                logger.info("âœ… Cache fechado")
            except Exception as e:
                logger.error(f"âŒ Erro ao fechar cache: {str(e)}")
        
        # Fechar conexÃµes de banco
        try:
            await close_db_connections()
            logger.info("âœ… ConexÃµes de banco fechadas")
        except Exception as e:
            logger.error(f"âŒ Erro ao fechar banco: {str(e)}")
        
        logger.info("ðŸ Limpeza concluÃ­da")


async def main():
    """
    FunÃ§Ã£o principal da aplicaÃ§Ã£o.
    
    Ponto de entrada que pode ser usado de diferentes formas:
    - ExecuÃ§Ã£o imediata: python -m src.main
    - ExecuÃ§Ã£o agendada: python -m src.main --schedule
    """
    app = DJEScrapingApplication()
    
    try:
        # Inicializar aplicaÃ§Ã£o
        await app.initialize()
        
        # Verificar argumentos da linha de comando
        if len(sys.argv) > 1 and "--schedule" in sys.argv:
            # Modo agendado
            await app.run_with_scheduler()
        else:
            # Modo imediato (execuÃ§Ã£o Ãºnica)
            logger.info("ðŸ”„ Modo execuÃ§Ã£o imediata")
            stats = await app.run_immediate_scraping()
            
            # Exibir resumo
            logger.info("ðŸ“Š RESUMO DA EXECUÃ‡ÃƒO:")
            logger.info(f"   ðŸ“‹ PublicaÃ§Ãµes encontradas: {stats.get('publications_found', 0)}")
            logger.info(f"   âœ… PublicaÃ§Ãµes novas: {stats.get('publications_new', 0)}")
            logger.info(f"   ðŸ”„ PublicaÃ§Ãµes duplicadas: {stats.get('publications_duplicated', 0)}")
            logger.info(f"   âŒ PublicaÃ§Ãµes com erro: {stats.get('publications_failed', 0)}")
            logger.info(f"   â±ï¸ Tempo de execuÃ§Ã£o: {stats.get('execution_time_seconds', 0):.2f}s")
    
    except KeyboardInterrupt:
        logger.info("ðŸ›‘ AplicaÃ§Ã£o interrompida pelo usuÃ¡rio")
    except Exception as e:
        logger.error(f"âŒ Erro fatal na aplicaÃ§Ã£o: {str(e)}")
        sys.exit(1)
    finally:
        await app.cleanup()


async def test_scraper():
    scraper = PlaywrightScraperAdapter(headless=False)
    
    try:
        print("ðŸš€ Inicializando scraper...")
        await scraper.initialize()
        print("âœ… Scraper inicializado!")
        
        print("ðŸŒ Navegando para teste...")
        await scraper.page.goto('https://example.com')
        print("âœ… NavegaÃ§Ã£o concluÃ­da!")
        
        print("â³ Aguardando 5 segundos...")
        await scraper.page.wait_for_timeout(5000)
        
        print("ðŸ“¸ Tirando screenshot...")
        await scraper.page.screenshot(path='test_scraper.png')
        print("âœ… Screenshot salvo!")
        
    except Exception as e:
        print(f"âŒ Erro: {e}")
    
    finally:
        print("ðŸ”’ Fechando scraper...")
        await scraper.close()
        print("âœ… Teste concluÃ­do!")

# CLI adicional para operaÃ§Ãµes especÃ­ficas
async def test_scraping():
    """FunÃ§Ã£o para testar o scraping sem agendamento."""
    logger.info("ðŸ§ª Modo teste de scraping")
    app = DJEScrapingApplication()
    
    try:
        await app.initialize()
        stats = await app.run_immediate_scraping()
        logger.info(f"ðŸ§ª Teste concluÃ­do: {stats}")
        return stats
    finally:
        await app.cleanup()


async def test_database():
    """FunÃ§Ã£o para testar conexÃ£o com banco de dados."""
    logger.info("ðŸ§ª Testando conexÃ£o com banco de dados")
    
    try:
        from src.config.database import get_db_session
        
        async with await get_db_session() as session:
            logger.info("âœ… ConexÃ£o com banco de dados OK")
            return True
    except Exception as e:
        logger.error(f"âŒ Erro na conexÃ£o: {str(e)}")
        return False


if __name__ == "__main__":
    # Verificar argumentos especiais
    if len(sys.argv) > 1:
        if "--test-scraping" in sys.argv:
            asyncio.run(test_scraper())
        elif "--test-db" in sys.argv:
            asyncio.run(test_database())
        else:
            asyncio.run(main())
    else:
        asyncio.run(main())