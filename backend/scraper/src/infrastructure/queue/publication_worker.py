"""
Worker - Processador de publica√ß√µes da fila Redis
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any
from domain.entities.publication import Publication, MonetaryValue
from infrastructure.queue.redis_queue_adapter import RedisQueueAdapter
from infrastructure.api.api_client_adapter import ApiClientAdapter

# from infrastructure.files.report_txt_saver import ReportTxtSaver  # Temporariamente desabilitado
from infrastructure.logging.logger import setup_logger
from infrastructure.config.settings import get_settings

logger = setup_logger(__name__)


class PublicationWorker:
    """
    Worker para processar publica√ß√µes da fila Redis
    """

    def __init__(self):
        self.settings = get_settings()
        self.queue = RedisQueueAdapter()
        self.api_client = ApiClientAdapter()
        # self.report_saver = ReportTxtSaver()  # Temporariamente desabilitado
        self.is_running = False
        self._stop_event = asyncio.Event()

        # Configurar diret√≥rio de relat√≥rios JSON
        self.json_dir = Path(self.settings.reports.json_dir)
        if not self.json_dir.exists():
            self.json_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"üìÅ Diret√≥rio JSON criado: {self.json_dir}")

    async def start(self):
        """Inicia o worker"""
        if self.is_running:
            logger.warning("‚ö†Ô∏è  Worker j√° est√° executando")
            return

        self.is_running = True
        self._stop_event.clear()

        logger.info("üöÄ Iniciando Publication Worker")
        logger.info("üìä Configura√ß√µes:")
        logger.info(f"   üìù Fila: {self.settings.redis.queue_name}")
        logger.info(f"   üîÑ Max tentativas: {self.settings.redis.max_retries}")
        logger.info(f"   ‚è∞ Delay retry: {self.settings.redis.retry_delay}s")
        logger.info(f"   üì¶ Batch size: {self.settings.redis.batch_size}")

        try:
            # Loop principal do worker
            while not self._stop_event.is_set():
                await self._process_batch()
                await self._process_delayed_publications()

                # Pequena pausa para n√£o sobrecarregar
                await asyncio.sleep(1)

        except Exception as error:
            logger.error(f"‚ùå Erro no worker: {error}")
            raise
        finally:
            self.is_running = False
            logger.info("‚èπÔ∏è  Publication Worker finalizado")

    async def stop(self):
        """Para o worker gracefully"""
        logger.info("üõë Parando Publication Worker...")
        self._stop_event.set()

        # Aguardar at√© 10 segundos para finalizar processamento atual
        for _ in range(10):
            if not self.is_running:
                break
            await asyncio.sleep(1)

        if self.is_running:
            logger.warning("‚ö†Ô∏è  Worker ainda est√° executando ap√≥s timeout")

    async def _process_batch(self):
        """Processa um lote de publica√ß√µes"""
        batch_size = self.settings.redis.batch_size
        processed_count = 0

        for _ in range(batch_size):
            if self._stop_event.is_set():
                break

            # Tentar processar uma publica√ß√£o
            publication_data = await self.queue.dequeue_publication()

            if not publication_data:
                # Fila vazia, parar o lote
                break

            success = await self._process_publication(publication_data)

            if success:
                processed_count += 1
                # Excluir arquivo JSON ap√≥s sucesso
                await self._delete_json_file(publication_data)
            else:
                # Reenfileirar com retry ou mover para DLQ
                await self._handle_failed_publication(publication_data)

        if processed_count > 0:
            logger.info(
                f"üìä Lote processado: {processed_count} publica√ß√µes enviadas para API"
            )

    async def _process_publication(self, publication_data: Dict[str, Any]) -> bool:
        """
        Processa uma publica√ß√£o individual

        Args:
            publication_data: Dados da publica√ß√£o

        Returns:
            True se processamento foi bem-sucedido
        """
        try:
            # Reconstituir entidade Publication
            publication = self._reconstruct_publication(publication_data)

            if not publication:
                logger.error(
                    f"‚ùå Erro ao reconstituir publica√ß√£o: {publication_data.get('process_number')}"
                )
                return False

            # Enviar para API
            logger.debug(f"üì§ Processando: {publication.process_number}")

            success = await self.api_client.save_publication(publication)

            if success:
                logger.debug(f"‚úÖ Publica√ß√£o processada: {publication.process_number}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è  Falha ao processar: {publication.process_number}")
                return False

        except Exception as error:
            logger.error(f"‚ùå Erro ao processar publica√ß√£o: {error}")
            logger.error(
                f"üîß Dados: {publication_data.get('process_number', 'UNKNOWN')}"
            )
            return False

    async def _delete_json_file(self, publication_data: Dict[str, Any]) -> None:
        """
        Exclui o arquivo JSON ap√≥s processamento bem-sucedido

        Args:
            publication_data: Dados da publica√ß√£o
        """
        try:
            # Construir o caminho do arquivo
            process_number = publication_data.get("process_number", "")
            if not process_number:
                return

            safe_process_number = process_number.replace("/", "_").replace(".", "_")
            filename = f"{safe_process_number}.json"
            file_path = self.json_dir / filename

            if file_path.exists():
                file_path.unlink()
                logger.info(f"üóëÔ∏è Arquivo JSON exclu√≠do: {filename}")
            else:
                logger.debug(f"‚ö†Ô∏è Arquivo JSON n√£o encontrado: {filename}")

        except Exception as error:
            logger.error(f"‚ùå Erro ao excluir arquivo JSON: {error}")

    def _reconstruct_publication(self, publication_data: Dict[str, Any]) -> Publication:
        """
        Reconstitui entidade Publication a partir dos dados da fila

        Args:
            publication_data: Dados serializados da publica√ß√£o

        Returns:
            Inst√¢ncia de Publication
        """
        try:
            from domain.entities.publication import Lawyer

            # Reconstituir publication_date
            publication_date = None
            if publication_data.get("publication_date"):
                publication_date = datetime.fromisoformat(
                    publication_data["publication_date"]
                )

            # Reconstituir availability_date (obrigat√≥rio)
            availability_date = datetime.now()
            if publication_data.get("availability_date"):
                availability_date = datetime.fromisoformat(
                    publication_data["availability_date"]
                )

            # Reconstituir lawyers
            lawyers = []
            for lawyer_data in publication_data.get("lawyers", []):
                if isinstance(lawyer_data, dict):
                    lawyers.append(
                        Lawyer(
                            name=lawyer_data.get("name", ""),
                            oab=lawyer_data.get("oab", ""),
                        )
                    )
                elif isinstance(lawyer_data, str):
                    # Formato string simples
                    lawyers.append(Lawyer(name=lawyer_data, oab=""))

            # Reconstituir valores monet√°rios
            gross_value = None
            net_value = None
            interest_value = None
            attorney_fees = None

            monetary_values = publication_data.get("monetary_values", [])
            for mv_data in monetary_values:
                if isinstance(mv_data, dict):
                    amount_cents = mv_data.get("value", 0)
                    mv_type = mv_data.get("type", "").lower()

                    if "honorario" in mv_type or "attorney" in mv_type:
                        attorney_fees = MonetaryValue(amount_cents=amount_cents)
                    elif "juros" in mv_type or "interest" in mv_type:
                        interest_value = MonetaryValue(amount_cents=amount_cents)
                    elif "bruto" in mv_type or "gross" in mv_type:
                        gross_value = MonetaryValue(amount_cents=amount_cents)
                    elif "liquido" in mv_type or "net" in mv_type:
                        net_value = MonetaryValue(amount_cents=amount_cents)

            publication = Publication(
                process_number=publication_data["process_number"],
                publication_date=publication_date,
                availability_date=availability_date,
                authors=publication_data.get("authors", []),
                lawyers=lawyers,
                gross_value=gross_value,
                net_value=net_value,
                interest_value=interest_value,
                attorney_fees=attorney_fees,
                content=publication_data.get("content", ""),
                extraction_metadata=publication_data.get("metadata", {}),
            )

            return publication

        except Exception as error:
            logger.error(f"‚ùå Erro ao reconstituir publica√ß√£o: {error}")
            logger.error(f"üîß Dados: {publication_data}")
            return None

    async def _handle_failed_publication(self, publication_data: Dict[str, Any]):
        """
        Trata publica√ß√£o que falhou no processamento

        Args:
            publication_data: Dados da publica√ß√£o que falhou
        """
        retry_count = publication_data.get("retry_count", 0)
        max_retries = self.settings.redis.max_retries
        process_number = publication_data.get("process_number", "UNKNOWN")

        if retry_count < max_retries:
            # Reenfileirar com delay
            delay_seconds = self.settings.redis.retry_delay * (
                2**retry_count
            )  # Backoff exponencial

            logger.warning(
                f"üîÑ Reenfileirando {process_number} (tentativa {retry_count + 1}/{max_retries}, delay: {delay_seconds}s)"
            )

            await self.queue.requeue_publication(publication_data, delay_seconds)
        else:
            # M√°ximo de tentativas atingido - mover para Dead Letter Queue e excluir arquivo
            logger.error(
                f"üíÄ Publica√ß√£o {process_number} falhou ap√≥s {max_retries} tentativas - movendo para DLQ"
            )
            await self._move_to_dead_letter_queue(publication_data)
            await self._delete_json_file(publication_data)

    async def _move_to_dead_letter_queue(self, publication_data: Dict[str, Any]):
        """
        Move publica√ß√£o para Dead Letter Queue

        Args:
            publication_data: Dados da publica√ß√£o
        """
        try:
            dlq_key = f"{self.settings.redis.queue_name}:dlq"

            # Adicionar timestamp de envio para DLQ
            publication_data["dlq_timestamp"] = asyncio.get_event_loop().time()
            publication_data["dlq_reason"] = "max_retries_exceeded"

            self.queue.redis_client.lpush(dlq_key, json.dumps(publication_data))

            logger.warning(
                f"üíÄ Publica√ß√£o movida para DLQ: {publication_data.get('process_number')}"
            )

        except Exception as error:
            logger.error(f"‚ùå Erro ao mover para DLQ: {error}")

    async def _process_delayed_publications(self):
        """Processa fila com delay para mover publica√ß√µes prontas"""
        try:
            await self.queue.process_delayed_queue()
        except Exception as error:
            logger.error(f"‚ùå Erro ao processar fila com delay: {error}")

    def get_stats(self) -> Dict[str, Any]:
        """
        Obt√©m estat√≠sticas do worker

        Returns:
            Estat√≠sticas do worker e da fila
        """
        queue_stats = self.queue.get_queue_stats()

        return {
            "worker_running": self.is_running,
            "queue_stats": queue_stats,
            "settings": {
                "queue_name": self.settings.redis.queue_name,
                "max_retries": self.settings.redis.max_retries,
                "retry_delay": self.settings.redis.retry_delay,
                "batch_size": self.settings.redis.batch_size,
            },
        }
