# Fila Redis para PublicaÃ§Ãµes

## VisÃ£o Geral

Este mÃ³dulo implementa uma fila Redis para processar o cadastramento de publicaÃ§Ãµes de forma assÃ­ncrona, desacoplando o processo de scraping do envio para a API.

## Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SCRAPER   â”‚â”€â”€â”€â–¶â”‚    REDIS    â”‚â”€â”€â”€â–¶â”‚   WORKER    â”‚â”€â”€â”€â–¶â”‚     API     â”‚
â”‚             â”‚    â”‚    QUEUE    â”‚    â”‚             â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Componentes

### 1. RedisQueueAdapter

- **LocalizaÃ§Ã£o**: `infrastructure/queue/redis_queue_adapter.py`
- **FunÃ§Ã£o**: Gerencia operaÃ§Ãµes da fila Redis
- **Funcionalidades**:
  - Enfileiramento de publicaÃ§Ãµes
  - Desenfileiramento com timeout
  - Reenfileiramento com delay (retry)
  - Processamento de fila com delay
  - EstatÃ­sticas da fila
  - Dead Letter Queue (DLQ)

### 2. PublicationWorker

- **LocalizaÃ§Ã£o**: `infrastructure/queue/publication_worker.py`
- **FunÃ§Ã£o**: Processa publicaÃ§Ãµes da fila e envia para API
- **Funcionalidades**:
  - Processamento em lote
  - Retry com backoff exponencial
  - Dead Letter Queue para falhas permanentes
  - Shutdown graceful
  - Monitoramento e estatÃ­sticas

### 3. SavePublicationsUseCase (Atualizado)

- **LocalizaÃ§Ã£o**: `application/usecases/save_publications.py`
- **FunÃ§Ã£o**: Enfileira publicaÃ§Ãµes ao invÃ©s de enviar diretamente
- **MudanÃ§a**: Usa Redis Queue ao invÃ©s de API diretamente

## ConfiguraÃ§Ãµes

### VariÃ¡veis de Ambiente

```env
# Redis
REDIS_HOST=juscash-redis
REDIS_PORT=6379
REDIS_PASSWORD=your_password
REDIS_DB=0

# Fila
REDIS_QUEUE_NAME=publications_queue
REDIS_MAX_RETRIES=3
REDIS_RETRY_DELAY=60
REDIS_BATCH_SIZE=10
REDIS_WORKER_TIMEOUT=300
```

## Fluxo de Processamento

### 1. Scraping (Produtor)

```python
# O scraper extrai publicaÃ§Ãµes e as enfileira
publications = [...]  # Lista de publicaÃ§Ãµes extraÃ­das
save_usecase = SavePublicationsUseCase()
result = await save_usecase.execute(publications)
# PublicaÃ§Ãµes sÃ£o enfileiradas no Redis
```

### 2. Worker (Consumidor)

```python
# Worker processa publicaÃ§Ãµes da fila
worker = PublicationWorker()
await worker.start()  # Processa continuamente
```

### 3. Retry e DLQ

- **Retry**: PublicaÃ§Ãµes com falha sÃ£o reenfileiradas com delay
- **Backoff**: Delay aumenta exponencialmente (60s, 120s, 240s...)
- **DLQ**: ApÃ³s 3 tentativas, move para Dead Letter Queue

## Estrutura das Filas Redis

### Fila Principal

- **Chave**: `publications_queue`
- **Tipo**: Lista (FIFO)
- **ConteÃºdo**: JSON das publicaÃ§Ãµes

### Fila com Delay

- **Chave**: `publications_queue:delayed`
- **Tipo**: Sorted Set
- **Score**: Timestamp quando deve ser processada

### Dead Letter Queue

- **Chave**: `publications_queue:dlq`
- **Tipo**: Lista
- **ConteÃºdo**: PublicaÃ§Ãµes que falharam permanentemente

## Como Usar

### 1. Executar com Worker Integrado

```bash
# O scraper jÃ¡ inicia o worker automaticamente
python src/main.py
```

### 2. Worker Standalone

```bash
# Executar apenas o worker
python src/cli/redis_cli.py worker
```

### 3. Monitorar Fila

```bash
# Ver estatÃ­sticas
python src/cli/redis_cli.py stats
```

## Vantagens da ImplementaÃ§Ã£o

### 1. **Desacoplamento**

- Scraper nÃ£o depende da disponibilidade da API
- Processamento independente e paralelo

### 2. **ResilÃªncia**

- Retry automÃ¡tico com backoff exponencial
- Dead Letter Queue para investigaÃ§Ã£o de falhas
- PersistÃªncia das publicaÃ§Ãµes no Redis

### 3. **Performance**

- Processamento em lote
- Scraping mais rÃ¡pido (nÃ£o aguarda API)
- Rate limiting controlado no worker

### 4. **Monitoramento**

- EstatÃ­sticas em tempo real
- Visibilidade das filas
- CLI para operaÃ§Ãµes manuais

### 5. **Escalabilidade**

- MÃºltiplos workers podem processar a mesma fila
- ConfiguraÃ§Ã£o flexÃ­vel de batch size

## Troubleshooting

### 1. Fila Crescendo Muito

```bash
# Verificar estatÃ­sticas
python src/cli/redis_cli.py stats

# PossÃ­veis causas:
# - API fora do ar
# - Rate limiting muito agressivo
# - Muitas publicaÃ§Ãµes duplicadas
```

### 2. PublicaÃ§Ãµes na DLQ

```bash
# Investigar DLQ
python src/cli/redis_cli.py stats

# Ver logs do worker para detalhes dos erros
```

### 3. Worker NÃ£o Processando

```bash
# Verificar se worker estÃ¡ rodando
python src/cli/redis_cli.py stats

# Restart do worker
docker restart juscash-scraper
```

## Logs Importantes

### Enfileiramento

```
ğŸ“¤ Enfileirando 15 publicaÃ§Ãµes
ğŸ“Š Enfileiramento concluÃ­do: {'total': 15, 'enqueued': 15, 'failed': 0}
ğŸ“ˆ Estado da fila: 23 publicaÃ§Ãµes pendentes
```

### Worker Processing

```
ğŸš€ Iniciando Publication Worker
ğŸ“Š Lote processado: 10 publicaÃ§Ãµes enviadas para API
âœ… PublicaÃ§Ã£o processada: 0017752-90.2019.8.26.0053
```

### Retry e DLQ

```
ğŸ”„ Reenfileirando 0031736-13.2018.8.26.0053 (tentativa 2/3, delay: 120s)
ğŸ’€ PublicaÃ§Ã£o 0031738-83.2018.8.26.0053 falhou apÃ³s 3 tentativas - movendo para DLQ
```

## Desenvolvimento

### Limpar Fila (apenas desenvolvimento)

```python
from infrastructure.queue.redis_queue_adapter import RedisQueueAdapter

queue = RedisQueueAdapter()
queue.clear_queue()  # Apenas em WORK_MODE=development
```

### Testar Localmente

```bash
# Executar scraper uma vez
WORK_MODE=development python src/main.py
```
