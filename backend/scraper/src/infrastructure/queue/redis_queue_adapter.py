"""
Adapter - Fila Redis para publica√ß√µes
"""

import json
import redis
import asyncio
from typing import List, Optional, Dict, Any
from domain.entities.publication import Publication
from infrastructure.logging.logger import setup_logger
from infrastructure.config.settings import get_settings

logger = setup_logger(__name__)


class RedisQueueAdapter:
    """
    Adaptador para fila Redis de publica√ß√µes
    """

    def __init__(self):
        self.settings = get_settings().redis
        self.redis_client = None
        self._connect()

    def _connect(self):
        """Conecta ao Redis"""
        try:
            self.redis_client = redis.Redis(
                host=self.settings.host,
                port=self.settings.port,
                password=self.settings.password or None,
                db=self.settings.db,
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
                retry_on_timeout=True,
                health_check_interval=30,
            )

            # Testar conex√£o
            self.redis_client.ping()
            logger.info(
                f"‚úÖ Conectado ao Redis: {self.settings.host}:{self.settings.port}"
            )

        except Exception as error:
            logger.error(f"‚ùå Erro ao conectar no Redis: {error}")
            logger.error(f"üîß Host: {self.settings.host}:{self.settings.port}")
            logger.error(f"üîß DB: {self.settings.db}")
            raise

    async def enqueue_publications(self, publications: List[Publication]) -> int:
        """
        Enfileira publica√ß√µes para processamento

        Args:
            publications: Lista de publica√ß√µes para enfileirar

        Returns:
            N√∫mero de publica√ß√µes enfileiradas
        """
        if not publications:
            logger.warning("üìù Nenhuma publica√ß√£o para enfileirar")
            return 0

        logger.info(f"üì§ Enfileirando {len(publications)} publica√ß√µes")

        enqueued_count = 0
        failed_count = 0

        for publication in publications:
            try:
                # Converter publica√ß√£o para JSON
                publication_data = {
                    "process_number": publication.process_number,
                    "publication_date": (
                        publication.publication_date.isoformat()
                        if publication.publication_date
                        else None
                    ),
                    "availability_date": publication.availability_date.isoformat(),
                    "authors": publication.authors,
                    "lawyers": [
                        {"name": lawyer.name, "oab": lawyer.oab}
                        for lawyer in publication.lawyers
                    ],
                    "monetary_values": [],
                    "content": publication.content,
                    "metadata": publication.extraction_metadata or {},
                    "retry_count": 0,
                    "enqueued_at": asyncio.get_event_loop().time(),
                }

                # Adicionar valores monet√°rios se existirem
                if publication.gross_value:
                    publication_data["monetary_values"].append(
                        {
                            "value": publication.gross_value.amount_cents,
                            "type": "gross_value",
                            "description": "Valor bruto",
                        }
                    )

                if publication.net_value:
                    publication_data["monetary_values"].append(
                        {
                            "value": publication.net_value.amount_cents,
                            "type": "net_value",
                            "description": "Valor l√≠quido",
                        }
                    )

                if publication.interest_value:
                    publication_data["monetary_values"].append(
                        {
                            "value": publication.interest_value.amount_cents,
                            "type": "interest_value",
                            "description": "Juros monet√°rios",
                        }
                    )

                if publication.attorney_fees:
                    publication_data["monetary_values"].append(
                        {
                            "value": publication.attorney_fees.amount_cents,
                            "type": "attorney_fees",
                            "description": "Honor√°rios advocat√≠cios",
                        }
                    )

                # Adicionar √† fila
                queue_key = self.settings.queue_name
                self.redis_client.lpush(queue_key, json.dumps(publication_data))
                enqueued_count += 1

                logger.debug(f"üìù Enfileirado: {publication.process_number}")

            except Exception as error:
                logger.error(
                    f"‚ùå Erro ao enfileirar {publication.process_number}: {error}"
                )
                failed_count += 1

        logger.info(
            f"üìä Enfileiramento conclu√≠do: {enqueued_count} sucesso, {failed_count} falhas"
        )
        return enqueued_count

    async def dequeue_publication(self) -> Optional[Dict[str, Any]]:
        """
        Remove publica√ß√£o da fila para processamento

        Returns:
            Dados da publica√ß√£o ou None se fila vazia
        """
        try:
            queue_key = self.settings.queue_name

            # Usar BRPOP para aguardar com timeout
            result = self.redis_client.brpop(queue_key, timeout=1)

            if result:
                _, publication_json = result
                publication_data = json.loads(publication_json)

                logger.debug(
                    f"üì• Removido da fila: {publication_data.get('process_number')}"
                )
                return publication_data

            return None

        except Exception as error:
            logger.error(f"‚ùå Erro ao remover da fila: {error}")
            return None

    async def requeue_publication(
        self, publication_data: Dict[str, Any], delay_seconds: int = None
    ):
        """
        Reenfileira publica√ß√£o ap√≥s falha

        Args:
            publication_data: Dados da publica√ß√£o
            delay_seconds: Delay antes de reprocessar (opcional)
        """
        try:
            # Incrementar contador de tentativas
            publication_data["retry_count"] = publication_data.get("retry_count", 0) + 1
            publication_data["requeued_at"] = asyncio.get_event_loop().time()

            if delay_seconds:
                # Usar fila com delay (sorted set com timestamp)
                delay_queue_key = f"{self.settings.queue_name}:delayed"
                score = asyncio.get_event_loop().time() + delay_seconds

                self.redis_client.zadd(
                    delay_queue_key, {json.dumps(publication_data): score}
                )

                logger.debug(
                    f"‚è∞ Reenfileirado com delay de {delay_seconds}s: {publication_data.get('process_number')}"
                )
            else:
                # Reenfileirar imediatamente
                queue_key = self.settings.queue_name
                self.redis_client.lpush(queue_key, json.dumps(publication_data))

                logger.debug(
                    f"üîÑ Reenfileirado: {publication_data.get('process_number')}"
                )

        except Exception as error:
            logger.error(f"‚ùå Erro ao reenfileirar: {error}")

    async def process_delayed_queue(self):
        """
        Move publica√ß√µes da fila com delay para fila principal quando apropriado
        """
        try:
            delay_queue_key = f"{self.settings.queue_name}:delayed"
            current_time = asyncio.get_event_loop().time()

            # Buscar publica√ß√µes prontas para processamento
            ready_items = self.redis_client.zrangebyscore(
                delay_queue_key, 0, current_time, withscores=True
            )

            if ready_items:
                logger.debug(
                    f"‚è∞ Processando {len(ready_items)} publica√ß√µes da fila com delay"
                )

                queue_key = self.settings.queue_name
                pipe = self.redis_client.pipeline()

                for item_json, _ in ready_items:
                    # Mover para fila principal
                    pipe.lpush(queue_key, item_json)
                    # Remover da fila com delay
                    pipe.zrem(delay_queue_key, item_json)

                pipe.execute()

                logger.debug(
                    f"‚úÖ {len(ready_items)} publica√ß√µes movidas para fila principal"
                )

        except Exception as error:
            logger.error(f"‚ùå Erro ao processar fila com delay: {error}")

    def get_queue_stats(self) -> Dict[str, int]:
        """
        Obt√©m estat√≠sticas da fila

        Returns:
            Estat√≠sticas da fila
        """
        try:
            queue_key = self.settings.queue_name
            delay_queue_key = f"{self.settings.queue_name}:delayed"

            stats = {
                "queue_size": self.redis_client.llen(queue_key),
                "delayed_queue_size": self.redis_client.zcard(delay_queue_key),
                "total_pending": 0,
            }

            stats["total_pending"] = stats["queue_size"] + stats["delayed_queue_size"]

            return stats

        except Exception as error:
            logger.error(f"‚ùå Erro ao obter estat√≠sticas da fila: {error}")
            return {"queue_size": 0, "delayed_queue_size": 0, "total_pending": 0}

    def clear_queue(self):
        """
        Limpa todas as filas (usar apenas em desenvolvimento/testes)
        """
        try:
            queue_key = self.settings.queue_name
            delay_queue_key = f"{self.settings.queue_name}:delayed"

            deleted_main = self.redis_client.delete(queue_key)
            deleted_delay = self.redis_client.delete(delay_queue_key)

            logger.warning(
                f"üßπ Filas limpas: {deleted_main + deleted_delay} chaves removidas"
            )

        except Exception as error:
            logger.error(f"‚ùå Erro ao limpar filas: {error}")

    def close(self):
        """Fecha conex√£o com Redis"""
        if self.redis_client:
            try:
                self.redis_client.close()
                logger.info("üîå Conex√£o Redis fechada")
            except Exception as error:
                logger.error(f"‚ùå Erro ao fechar conex√£o Redis: {error}")
