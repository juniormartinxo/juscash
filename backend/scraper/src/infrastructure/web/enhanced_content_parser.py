"""
Parser Aprimorado para Relat√≥rios DJE-SP
Segue instru√ß√µes espec√≠ficas para extra√ß√£o de autores baseada em padr√µes RPV/INSS
"""

import re
import asyncio
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path

from domain.entities.publication import Publication, Lawyer, MonetaryValue
from infrastructure.logging.logger import setup_logger
from infrastructure.web.content_parser import DJEContentParser

logger = setup_logger(__name__)


class EnhancedDJEContentParser(DJEContentParser):
    """
    Parser aprimorado que segue instru√ß√µes espec√≠ficas para extra√ß√£o de autores
    baseada nos padr√µes "RPV" e "pagamento pelo INSS"
    """

    def __init__(self):
        super().__init__()
        self.scraper_adapter = None  # Ser√° injetado para download de p√°ginas anteriores

    def set_scraper_adapter(self, scraper_adapter):
        """Injeta o scraper adapter para download de p√°ginas anteriores"""
        self.scraper_adapter = scraper_adapter

    async def parse_multiple_publications_enhanced(
        self, content: str, source_url: str = "", current_page_number: int = None
    ) -> List[Publication]:
        """
        Extrai m√∫ltiplas publica√ß√µes seguindo as instru√ß√µes espec√≠ficas:
        1. Busca por "RPV" ou "pagamento pelo INSS"
        2. Localiza in√≠cio do processo com "Processo NUMERO_DO_PROCESSO"
        3. Extrai autores no formato "- NOME_DO_AUTOR - Vistos"
        4. Baixa p√°gina anterior se necess√°rio
        """
        logger.info("üîç Iniciando extra√ß√£o aprimorada de autores")
        publications = []

        try:
            # Passo 1: Buscar por "RPV" ou "pagamento pelo INSS"
            rpv_inss_matches = self._find_rpv_inss_occurrences(content)

            if not rpv_inss_matches:
                logger.info(
                    "‚ùå Nenhuma ocorr√™ncia de 'RPV' ou 'pagamento pelo INSS' encontrada"
                )
                return []

            logger.info(
                f"‚úÖ Encontradas {len(rpv_inss_matches)} ocorr√™ncias de RPV/INSS"
            )

            # Passo 2-6: Para cada ocorr√™ncia, extrair processo e autores
            for match_info in rpv_inss_matches:
                publication = await self._extract_publication_from_match(
                    content, match_info, source_url, current_page_number
                )
                if publication:
                    publications.append(publication)

            logger.info(
                f"‚úÖ Extra√≠das {len(publications)} publica√ß√µes com padr√£o aprimorado"
            )
            return publications

        except Exception as error:
            logger.error(f"‚ùå Erro na extra√ß√£o aprimorada: {error}")
            return []

    def _find_rpv_inss_occurrences(self, content: str) -> List[Dict[str, Any]]:
        """
        Passo 1: Busca por "RPV" ou "pagamento pelo INSS" no relat√≥rio
        """
        matches = []

        # Padr√µes para buscar RPV e pagamento pelo INSS
        rpv_pattern = re.compile(r"\bRPV\b", re.IGNORECASE)
        inss_payment_pattern = re.compile(r"pagamento\s+pelo\s+INSS", re.IGNORECASE)

        # Buscar todas as ocorr√™ncias de RPV
        for match in rpv_pattern.finditer(content):
            matches.append(
                {
                    "type": "RPV",
                    "position": match.start(),
                    "text": match.group(),
                    "context": content[max(0, match.start() - 100) : match.end() + 100],
                }
            )

        # Buscar todas as ocorr√™ncias de "pagamento pelo INSS"
        for match in inss_payment_pattern.finditer(content):
            matches.append(
                {
                    "type": "pagamento pelo INSS",
                    "position": match.start(),
                    "text": match.group(),
                    "context": content[max(0, match.start() - 100) : match.end() + 100],
                }
            )

        # Ordenar por posi√ß√£o no documento
        matches.sort(key=lambda x: x["position"])

        logger.info(f"üîç Encontradas {len(matches)} ocorr√™ncias de RPV/INSS")
        for i, match in enumerate(matches):
            logger.debug(f"   {i+1}. {match['type']} na posi√ß√£o {match['position']}")

        return matches

    async def _extract_publication_from_match(
        self,
        content: str,
        match_info: Dict[str, Any],
        source_url: str,
        current_page_number: int,
    ) -> Optional[Publication]:
        """
        Passos 2-6: Extrai publica√ß√£o completa a partir de uma ocorr√™ncia RPV/INSS
        """
        try:
            match_position = match_info["position"]

            # Passo 2: Localizar in√≠cio do processo "Processo NUMERO_DO_PROCESSO"
            process_start_info = self._find_process_start(content, match_position)

            if not process_start_info:
                logger.warning(
                    f"‚ö†Ô∏è N√£o foi poss√≠vel localizar in√≠cio do processo para {match_info['type']}"
                )

                # Passo 4: Se n√£o encontrou, tentar baixar p√°gina anterior
                if current_page_number and self.scraper_adapter:
                    logger.info("üîÑ Tentando baixar p√°gina anterior...")
                    previous_content = await self._download_previous_page(
                        current_page_number
                    )
                    if previous_content:
                        # Buscar processo na p√°gina anterior
                        process_start_info = self._find_process_start_in_previous(
                            previous_content, content, match_position
                        )

                if not process_start_info:
                    return None

            # Passo 3: Determinar fim do processo
            process_end_position = self._find_process_end(
                content, process_start_info["end_position"]
            )

            # Extrair conte√∫do completo do processo
            if process_start_info.get("from_previous"):
                # Processo come√ßou na p√°gina anterior
                process_content = (
                    process_start_info["previous_content"]
                    + content[:process_end_position]
                )
            else:
                # Processo est√° na p√°gina atual
                process_content = content[
                    process_start_info["start_position"] : process_end_position
                ]

            # Passo 5-6: Extrair autores no formato "- NOME_DO_AUTOR - Vistos"
            authors = self._extract_authors_enhanced_pattern(process_content)

            if not authors:
                logger.warning(
                    f"‚ö†Ô∏è Nenhum autor encontrado no padr√£o '- NOME - Vistos' para processo {process_start_info.get('process_number', 'desconhecido')}"
                )
                return None

            # Extrair outros dados do processo
            process_number = process_start_info.get("process_number")
            if not process_number:
                process_number = self._extract_process_number(process_content)

            if not process_number:
                logger.warning("‚ö†Ô∏è N√∫mero do processo n√£o encontrado")
                return None

            # Extrair dados complementares
            publication_date = self._extract_publication_date(process_content)
            availability_date = (
                self._extract_availabilityDate(process_content) or datetime.now()
            )
            lawyers = self._extract_lawyers(process_content)
            monetary_values = self._extract_all_monetary_values(process_content)

            # Criar publica√ß√£o
            publication = Publication(
                process_number=process_number,
                publication_date=publication_date,
                availability_date=availability_date,
                authors=authors,
                lawyers=lawyers,
                gross_value=monetary_values.get("gross"),
                net_value=monetary_values.get("net"),
                interest_value=monetary_values.get("interest"),
                attorney_fees=monetary_values.get("fees"),
                content=process_content,
                extraction_metadata={
                    "extraction_date": datetime.now().isoformat(),
                    "source_url": source_url,
                    "extraction_method": "enhanced_rpv_inss_pattern",
                    "match_type": match_info["type"],
                    "match_position": match_position,
                    "process_spans_pages": process_start_info.get(
                        "from_previous", False
                    ),
                    "text_length": len(process_content),
                },
            )

            logger.info(
                f"‚úÖ Publica√ß√£o extra√≠da: {process_number} - Autores: {', '.join(authors)}"
            )
            return publication

        except Exception as error:
            logger.error(f"‚ùå Erro ao extrair publica√ß√£o: {error}")
            return None

    def _find_process_start(
        self, content: str, match_position: int
    ) -> Optional[Dict[str, Any]]:
        """
        Passo 2: Localiza in√≠cio do processo "Processo NUMERO_DO_PROCESSO"
        Busca para tr√°s a partir da posi√ß√£o da ocorr√™ncia RPV/INSS
        """
        # Padr√£o para "Processo NUMERO_DO_PROCESSO"
        process_pattern = re.compile(
            r"Processo\s+(\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})", re.IGNORECASE
        )

        # Buscar para tr√°s a partir da posi√ß√£o da ocorr√™ncia
        search_start = max(0, match_position - 5000)  # Buscar at√© 5000 chars para tr√°s
        search_content = content[
            search_start : match_position + 1000
        ]  # Incluir um pouco √† frente

        # Encontrar todas as ocorr√™ncias de processo na √°rea de busca
        process_matches = list(process_pattern.finditer(search_content))

        if not process_matches:
            logger.debug(
                "üîç Nenhum 'Processo NUMERO' encontrado antes da ocorr√™ncia RPV/INSS"
            )
            return None

        # Pegar o √∫ltimo processo antes da ocorr√™ncia (mais pr√≥ximo)
        last_process = process_matches[-1]
        process_number = last_process.group(1)

        # Calcular posi√ß√µes absolutas
        absolute_start = search_start + last_process.start()
        absolute_end = search_start + last_process.end()

        logger.debug(
            f"‚úÖ Processo encontrado: {process_number} na posi√ß√£o {absolute_start}"
        )

        return {
            "process_number": process_number,
            "start_position": absolute_start,
            "end_position": absolute_end,
            "from_previous": False,
        }

    def _find_process_end(self, content: str, start_position: int) -> int:
        """
        Passo 3: Determina fim do processo (pr√≥ximo "Processo NUMERO" ou fim do documento)
        """
        # Buscar pr√≥ximo processo a partir da posi√ß√£o atual
        process_pattern = re.compile(
            r"Processo\s+\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4}", re.IGNORECASE
        )

        search_content = content[start_position + 50 :]  # Pular o processo atual
        next_process = process_pattern.search(search_content)

        if next_process:
            end_position = start_position + 50 + next_process.start()
            logger.debug(
                f"‚úÖ Fim do processo na posi√ß√£o {end_position} (pr√≥ximo processo)"
            )
        else:
            end_position = len(content)
            logger.debug(
                f"‚úÖ Fim do processo no final do documento (posi√ß√£o {end_position})"
            )

        return end_position

    async def _download_previous_page(self, current_page_number: int) -> Optional[str]:
        """
        Passo 4: Baixa p√°gina anterior (nuSeqpagina - 1) se necess√°rio
        """
        if not self.scraper_adapter or current_page_number <= 1:
            return None

        try:
            logger.info(f"üì• Baixando p√°gina anterior: {current_page_number - 1}")

            # Construir URL da p√°gina anterior
            # Assumindo que a URL atual tem nuSeqpagina=X, mudamos para X-1
            previous_page_number = current_page_number - 1

            # Aqui voc√™ precisaria implementar a l√≥gica espec√≠fica para baixar
            # a p√°gina anterior baseada no padr√£o de URL do DJE-SP
            # Por enquanto, retorno None para indicar que n√£o foi implementado

            logger.warning("‚ö†Ô∏è Download de p√°gina anterior n√£o implementado ainda")
            return None

        except Exception as error:
            logger.error(f"‚ùå Erro ao baixar p√°gina anterior: {error}")
            return None

    def _find_process_start_in_previous(
        self, previous_content: str, current_content: str, match_position: int
    ) -> Optional[Dict[str, Any]]:
        """
        Busca in√≠cio do processo na p√°gina anterior quando n√£o encontrado na atual
        """
        # Implementar l√≥gica para encontrar processo que come√ßou na p√°gina anterior
        # e continua na p√°gina atual

        logger.warning("‚ö†Ô∏è Busca em p√°gina anterior n√£o implementada ainda")
        return None

    def _extract_authors_enhanced_pattern(self, process_content: str) -> List[str]:
        """
        Passo 5-6: Extrai autores no formato "- NOME_DO_AUTOR - Vistos"
        """
        authors = []

        # Padr√£o espec√≠fico: "- NOME_DO_AUTOR - Vistos"
        author_pattern = re.compile(
            r"-\s+([^-]+?)\s+-\s+Vistos", re.IGNORECASE | re.MULTILINE
        )

        matches = author_pattern.findall(process_content)

        for match in matches:
            author_name = self._clean_author_name(match)
            if (
                author_name and len(author_name) > 2
            ):  # Nome deve ter pelo menos 3 caracteres
                authors.append(author_name)
                logger.debug(f"‚úÖ Autor extra√≠do: '{author_name}'")

        # Remover duplicatas mantendo ordem
        unique_authors = []
        for author in authors:
            if author not in unique_authors:
                unique_authors.append(author)

        logger.info(
            f"‚úÖ Extra√≠dos {len(unique_authors)} autores √∫nicos no padr√£o '- NOME - Vistos'"
        )

        return unique_authors

    def _clean_author_name(self, name: str) -> str:
        """
        Limpa e normaliza nome do autor
        """
        if not name:
            return ""

        # Remover espa√ßos extras
        name = re.sub(r"\s+", " ", name.strip())

        # Remover caracteres especiais no in√≠cio/fim
        name = re.sub(r"^[^\w\s]+|[^\w\s]+$", "", name)

        # Converter para formato t√≠tulo (primeira letra mai√∫scula)
        name = name.title()

        # Filtrar nomes muito curtos ou inv√°lidos
        if len(name) < 3:
            return ""

        # Filtrar palavras que n√£o s√£o nomes
        invalid_words = [
            "vistos",
            "processo",
            "inss",
            "instituto",
            "nacional",
            "seguro",
            "social",
        ]
        if name.lower() in invalid_words:
            return ""

        return name


# Fun√ß√£o para alertar sobre edge cases
def alert_edge_case(message: str):
    """
    Alerta sobre edge cases ou dificuldades na extra√ß√£o
    """
    logger.warning(f"üö® EDGE CASE DETECTADO: {message}")
    print(f"üö® EDGE CASE: {message}")
