"""
Adaptador otimizado do DJE Scraper - Extrai apenas n√∫meros de processo sem baixar PDFs
"""

import asyncio
import re
from typing import List, AsyncGenerator, Optional
from datetime import datetime

from playwright.async_api import async_playwright, Browser, Page
from domain.entities.publication import Publication, MonetaryValue
from domain.ports.web_scraper import WebScraperPort
from infrastructure.logging.logger import setup_logger
from infrastructure.config.settings import get_settings

logger = setup_logger(__name__)


class DJEScraperOptimized(WebScraperPort):
    """
    Vers√£o otimizada do scraper DJE que extrai apenas n√∫meros de processo
    sem fazer download de PDFs - mais r√°pido e eficiente
    """

    def __init__(self):
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.settings = get_settings()
        self.base_url = "https://esaj.tjsp.jus.br/cdje/index.do"
        self._target_date = None  # Data espec√≠fica para buscar

    async def initialize(self) -> None:
        """Inicializa o browser e navegador"""
        logger.info("üöÄ Inicializando DJE Scraper Otimizado (sem PDFs)")

        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=True, args=["--no-sandbox", "--disable-setuid-sandbox"]
        )

        self.page = await self.browser.new_page()
        self.page.set_default_timeout(30000)

        logger.info("‚úÖ Browser inicializado - modo otimizado")

    async def cleanup(self) -> None:
        """Limpa recursos do browser"""
        if self.page:
            await self.page.close()
        if self.browser:
            await self.browser.close()
        logger.info("üßπ Recursos liberados")

    async def scrape_publications(
        self, search_terms: List[str], max_pages: int = 10
    ) -> AsyncGenerator[Publication, None]:
        """
        Extrai publica√ß√µes apenas com n√∫meros de processo (sem baixar PDFs)
        """
        logger.info(f"üîç Iniciando busca otimizada por: {search_terms}")
        logger.info(f"üìÑ M√°ximo de p√°ginas: {max_pages}")

        try:
            # Navegar para p√°gina de busca
            await self._navigate_to_advanced_search()

            # Preencher formul√°rio
            await self._fill_advanced_search_form(search_terms)

            # Extrair n√∫meros de processo das p√°ginas de resultado
            async for publication in self._extract_process_numbers(max_pages):
                yield publication

        except Exception as error:
            logger.error(f"‚ùå Erro durante scraping otimizado: {error}")
            raise

    async def _navigate_to_advanced_search(self) -> None:
        """Navega para p√°gina de consulta avan√ßada"""
        logger.info(f"üìç Navegando para {self.base_url}")

        await self.page.goto(self.base_url, wait_until="networkidle")
        await asyncio.sleep(2)

        # Tentar m√∫ltiplos seletores para "Consulta Avan√ßada"
        selectors = [
            'a:text("Consulta Avan√ßada")',
            'a:has-text("Consulta Avan√ßada")',
            'a[href*="consultaAvancada"]',
            "text=Consulta Avan√ßada",
            '//a[contains(text(), "Consulta Avan√ßada")]',
        ]

        clicked = False
        for selector in selectors:
            try:
                logger.debug(f"üîç Tentando selector: {selector}")
                element = await self.page.wait_for_selector(selector, timeout=5000)
                if element:
                    await element.click()
                    clicked = True
                    logger.info(f"‚úÖ Clicou em Consulta Avan√ßada usando: {selector}")
                    break
            except:
                continue

        if not clicked:
            # Se n√£o encontrou, tentar navegar direto para URL de consulta avan√ßada
            logger.warning("‚ö†Ô∏è N√£o encontrou link, tentando URL direta")
            advanced_url = "https://esaj.tjsp.jus.br/cdje/consultaAvancada.do"
            await self.page.goto(advanced_url, wait_until="networkidle")

        await self.page.wait_for_load_state("networkidle")
        await asyncio.sleep(1)

        logger.info("‚úÖ P√°gina de consulta avan√ßada carregada")

    async def _fill_advanced_search_form(self, search_terms: List[str]) -> None:
        """Preenche formul√°rio de busca avan√ßada"""
        logger.info("üìù Preenchendo formul√°rio de busca")

        # Configurar datas
        if self._target_date:
            logger.info(f"üìÖ Usando data espec√≠fica: {self._target_date}")
            await self.page.fill(
                'input[name="dadosConsulta.dtInicio"]', self._target_date
            )
            await self.page.fill('input[name="dadosConsulta.dtFim"]', self._target_date)
        else:
            # Usar data de hoje
            today = datetime.now().strftime("%d/%m/%Y")
            await self.page.fill('input[name="dadosConsulta.dtInicio"]', today)
            await self.page.fill('input[name="dadosConsulta.dtFim"]', today)

        # Selecionar caderno
        await self.page.select_option(
            'select[name="dadosConsulta.cdCaderno"]',
            value="12",  # Judicial - 1¬™ Inst√¢ncia - Capital
        )

        # Preencher palavras-chave
        keywords = " e ".join([f'"{term}"' for term in search_terms])
        await self.page.fill('textarea[name="dadosConsulta.pesquisaLivre"]', keywords)

        # Submeter formul√°rio
        await self.page.click('input[type="submit"][value="Pesquisar"]')
        await self.page.wait_for_load_state("networkidle")

        logger.info("‚úÖ Busca executada")

    async def _extract_process_numbers(
        self, max_pages: int
    ) -> AsyncGenerator[Publication, None]:
        """
        Extrai n√∫meros de processo diretamente da p√°gina de resultados
        sem baixar PDFs
        """
        current_page = 1
        process_numbers_found = set()  # Para evitar duplicatas

        while current_page <= max_pages:
            logger.info(f"üìÑ Processando p√°gina {current_page}/{max_pages}")

            try:
                # Aguardar elementos carregarem
                await self.page.wait_for_selector("tr.ementaClass", timeout=10000)

                # Buscar todos os elementos com links
                elements = await self.page.query_selector_all("tr.ementaClass")
                logger.info(f"‚úÖ Encontrados {len(elements)} elementos na p√°gina")

                # Extrair texto de cada elemento
                for i, element in enumerate(elements):
                    try:
                        # Pegar texto completo do elemento
                        text_content = await element.text_content()

                        if text_content:
                            # Buscar n√∫meros de processo no texto
                            # Padr√£o: XXXXXXX-XX.XXXX.X.XX.XXXX
                            process_pattern = (
                                r"\b\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4}\b"
                            )
                            matches = re.findall(process_pattern, text_content)

                            for process_number in matches:
                                if process_number not in process_numbers_found:
                                    process_numbers_found.add(process_number)

                                    # Criar publica√ß√£o b√°sica
                                    publication = self._create_basic_publication(
                                        process_number, text_content
                                    )

                                    logger.info(
                                        f"‚úÖ Processo encontrado: {process_number}"
                                    )
                                    yield publication

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao processar elemento {i}: {e}")
                        continue

                # Verificar se h√° pr√≥xima p√°gina
                has_next = await self._navigate_to_next_page()
                if not has_next:
                    logger.info("üìÑ N√£o h√° mais p√°ginas")
                    break

                current_page += 1
                await asyncio.sleep(1)  # Delay entre p√°ginas

            except Exception as error:
                logger.error(f"‚ùå Erro na p√°gina {current_page}: {error}")
                break

        logger.info(
            f"‚úÖ Total de processos √∫nicos encontrados: {len(process_numbers_found)}"
        )

    def _create_basic_publication(
        self, process_number: str, content: str
    ) -> Publication:
        """
        Cria uma publica√ß√£o b√°sica com dados m√≠nimos
        Os dados completos ser√£o obtidos do e-SAJ posteriormente
        """
        # Extrair autores do conte√∫do se poss√≠vel
        authors = []
        author_pattern = r"(?:Autor|Requerente|Exequente):\s*([^-\n]+)"
        author_matches = re.findall(author_pattern, content, re.IGNORECASE)
        if author_matches:
            authors = [
                match.strip() for match in author_matches[:3]
            ]  # M√°ximo 3 autores

        # Criar publica√ß√£o com dados b√°sicos
        return Publication(
            process_number=process_number,
            publication_date=datetime.now(),  # Ser√° atualizada pelo e-SAJ
            availability_date=datetime.now(),  # Ser√° atualizada pelo e-SAJ
            authors=authors if authors else ["A definir"],
            lawyers=[],  # Ser√° preenchido pelo e-SAJ
            gross_value=MonetaryValue.from_real(0),  # Ser√° atualizado pelo e-SAJ
            interest_value=MonetaryValue.from_real(0),  # Ser√° atualizado pelo e-SAJ
            attorney_fees=MonetaryValue.from_real(0),  # Ser√° atualizado pelo e-SAJ
            content=content[:500] if content else "",  # Primeiros 500 chars
            defendant="Instituto Nacional do Seguro Social - INSS",
            status="NOVA",
            scraping_source="DJE-SP",
            caderno="3",
            instancia="1",
            local="Capital",
            parte="1",
        )

    async def _navigate_to_next_page(self) -> bool:
        """Navega para pr√≥xima p√°gina de resultados"""
        try:
            next_selectors = ['a:text("Pr√≥xima")', 'a:text(">")', 'a[title*="pr√≥xima"]']

            for selector in next_selectors:
                next_element = await self.page.query_selector(selector)
                if next_element:
                    await next_element.click()
                    await self.page.wait_for_load_state("networkidle")
                    logger.info("‚úÖ Navegou para pr√≥xima p√°gina")
                    return True

            return False

        except Exception as error:
            logger.warning(f"‚ö†Ô∏è Erro ao navegar: {error}")
            return False
