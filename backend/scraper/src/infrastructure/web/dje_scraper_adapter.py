"""
Adapter - Implementa√ß√£o do web scraper para DJE-SP
"""

import re
import asyncio
import tempfile
import os
import shutil
from typing import List, AsyncGenerator, Optional
from datetime import datetime
from decimal import Decimal, InvalidOperation
from pathlib import Path
from playwright.async_api import async_playwright, Browser, Page, Download

from domain.ports.web_scraper import WebScraperPort
from domain.entities.publication import Publication, Lawyer, MonetaryValue
from infrastructure.web.content_parser import DJEContentParser
from infrastructure.web.enhanced_content_parser import EnhancedDJEContentParser

# from infrastructure.files.report_txt_saver import ReportTxtSaver  # Temporariamente desabilitado
from infrastructure.logging.logger import setup_logger
from infrastructure.config.settings import get_settings

logger = setup_logger(__name__)


class DJEScraperAdapter(WebScraperPort):
    """
    Implementa√ß√£o do scraper para o DJE de S√£o Paulo
    Fluxo correto: acessa consultaAvancada.do, encontra links em tr[class="ementaClass"], baixa PDFs
    """

    def __init__(self):
        self.settings = get_settings()
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        self.parser = DJEContentParser()
        self.enhanced_parser = EnhancedDJEContentParser()
        self.enhanced_parser.set_scraper_adapter(self)
        self.temp_dir = Path(tempfile.gettempdir()) / "dje_scraper_pdfs"
        self.temp_dir.mkdir(exist_ok=True)

        # üÜï Pasta para salvar PDFs para debug
        self.pdf_debug_dir = Path("reports/pdf")
        self.pdf_debug_dir.mkdir(parents=True, exist_ok=True)

        # Controle de PDFs problem√°ticos
        self.failed_pdfs = set()  # URLs que falharam m√∫ltiplas vezes

        # Instanciar o salvador de relat√≥rios JSON
        from infrastructure.files.report_json_saver import ReportJsonSaver

        self.json_saver = ReportJsonSaver()

    async def initialize(self) -> None:
        """Inicializa o browser e navega√ß√£o"""
        logger.info("üåê Inicializando browser Playwright")

        self.playwright = await async_playwright().start()

        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
            ],
        )

        self.page = await self.browser.new_page()

        # Configurar timeout e user agent
        await self.page.set_extra_http_headers(
            {"User-Agent": self.settings.browser.user_agent}
        )

        self.page.set_default_timeout(self.settings.browser.timeout)

        logger.info("‚úÖ Browser inicializado com sucesso")

    async def cleanup(self) -> None:
        """Limpeza de recursos"""
        logger.info("üßπ Limpando recursos do browser")

        # Limpar PDFs tempor√°rios (mas N√ÉO os de debug)
        try:
            for pdf_file in self.temp_dir.glob("*.pdf"):
                pdf_file.unlink()
            logger.info("üóëÔ∏è PDFs tempor√°rios removidos")
            logger.info(f"üêõ PDFs de debug mantidos em: {self.pdf_debug_dir}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao limpar PDFs: {e}")

        # Cleanup do page
        try:
            if self.page and not self.page.is_closed():
                await self.page.close()
                logger.debug("üìÑ P√°gina fechada")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao fechar p√°gina: {e}")

        # Cleanup do browser
        try:
            if self.browser:
                await self.browser.close()
                logger.debug("üåê Browser fechado")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao fechar browser: {e}")

        # Cleanup do playwright
        try:
            if self.playwright:
                await self.playwright.stop()
                logger.debug("üé≠ Playwright parado")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao parar playwright: {e}")

    async def scrape_publications(
        self, search_terms: List[str], max_pages: int = 10
    ) -> AsyncGenerator[Publication, None]:
        """
        Extrai publica√ß√µes do DJE-SP com o fluxo correto:
        1. Acessa consultaAvancada.do
        2. Preenche formul√°rio de pesquisa avan√ßada
        3. Encontra links em tr[class="ementaClass"]
        4. Baixa PDFs dos links onclick
        5. Processa PDFs para extrair publica√ß√µes
        """
        logger.info(f"üï∑Ô∏è Iniciando scraping DJE-SP com termos: {search_terms}")

        try:
            # Navegar para p√°gina de consulta avan√ßada
            await self._navigate_to_advanced_search()

            # Preencher formul√°rio de pesquisa avan√ßada
            await self._fill_advanced_search_form(search_terms)

            async for publication in self._extract_publications_from_pdf_links(
                max_pages
            ):
                yield publication

        except Exception as error:
            logger.error(f"‚ùå Erro durante scraping: {error}")
            raise

    async def _navigate_to_advanced_search(self) -> None:
        """Navega para a p√°gina de consulta avan√ßada do DJE"""
        target_url = "https://esaj.tjsp.jus.br/cdje/consultaAvancada.do#buscaavancada"
        logger.info(f"üìç Navegando para {target_url}")

        await self.page.goto(target_url)
        await self.page.wait_for_load_state("networkidle")

        logger.info("‚úÖ P√°gina de consulta avan√ßada carregada")

    async def _fill_advanced_search_form(self, search_terms: List[str]) -> None:
        """
        Preenche o formul√°rio de pesquisa avan√ßada com crit√©rios espec√≠ficos
        Suporta data din√¢mica atrav√©s do atributo _target_date
        """
        logger.info("üìù Preenchendo formul√°rio de pesquisa avan√ßada")

        try:
            # Verificar se p√°gina ainda est√° v√°lida
            if not self.page or self.page.is_closed():
                raise Exception("P√°gina do browser foi fechada")

            # Aguardar carregamento completo do formul√°rio
            await self.page.wait_for_load_state("networkidle")
            await asyncio.sleep(3)

            # 1. CONFIGURAR DATA ESPEC√çFICA (din√¢mica ou padr√£o)
            target_date = getattr(self, "_target_date", "17/03/2025")
            logger.info(f"üìÖ Configurando data espec√≠fica: {target_date}...")

            # For√ßar data in√≠cio
            data_inicio_script = f"""
            (() => {{
                const dataInicio = document.querySelector('#dtInicioString');
                if (dataInicio) {{
                    dataInicio.removeAttribute('readonly');
                    dataInicio.disabled = false;
                    dataInicio.value = '{target_date}';
                    dataInicio.dispatchEvent(new Event('change', {{ bubbles: true }}));
                    return dataInicio.value;
                }}
                return null;
            }})()
            """

            # For√ßar data fim
            data_fim_script = f"""
            (() => {{
                const dataFim = document.querySelector('#dtFimString');
                if (dataFim) {{
                    dataFim.removeAttribute('readonly');
                    dataFim.disabled = false;
                    dataFim.value = '{target_date}';
                    dataFim.dispatchEvent(new Event('change', {{ bubbles: true }}));
                    return dataFim.value;
                }}
                return null;
            }})()
            """

            try:
                data_inicio = await self.page.evaluate(data_inicio_script)
                data_fim = await self.page.evaluate(data_fim_script)
                logger.info(f"‚úÖ Datas configuradas: {data_inicio} at√© {data_fim}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao for√ßar datas: {e}")

            # 2. SELECIONAR CADERNO - Caderno 3 - Judicial - 1¬™ Inst√¢ncia - Capital - Parte I
            caderno_selector = 'select[name="dadosConsulta.cdCaderno"]'
            await self.page.wait_for_selector(caderno_selector)

            # Usar value="12" que corresponde ao Caderno 3
            try:
                await self.page.select_option(caderno_selector, value="12")

                # Verificar sele√ß√£o
                selected_option = await self.page.evaluate(
                    """
                    (() => {
                        const select = document.querySelector('select[name="dadosConsulta.cdCaderno"]');
                        const selectedOption = select.options[select.selectedIndex];
                        return {
                            value: select.value,
                            text: selectedOption.text
                        };
                    })()
                    """
                )

                logger.info(
                    f"‚úÖ Caderno selecionado: {selected_option['text']} (value: {selected_option['value']})"
                )
            except Exception as e:
                logger.error(f"‚ùå Erro ao selecionar caderno: {e}")

            # 3. PREENCHER PALAVRAS-CHAVE EXATAS
            logger.info("üîç Preenchendo palavras-chave...")
            search_query = '"RPV" e "pagamento pelo INSS"'

            # Aguardar campo estar dispon√≠vel
            await self.page.wait_for_selector("#procura", timeout=10000)

            # Preencher usando JavaScript para garantir
            keywords_script = f"""
            (() => {{
                const campo = document.querySelector('#procura');
                if (campo) {{
                    campo.value = '{search_query}';
                    campo.dispatchEvent(new Event('input', {{ bubbles: true }}));
                    campo.dispatchEvent(new Event('change', {{ bubbles: true }}));
                    return campo.value;
                }}
                return null;
            }})()
            """

            filled_value = await self.page.evaluate(keywords_script)
            logger.info(f"‚úÖ Palavras-chave preenchidas: '{filled_value}'")

            # 4. AGUARDAR UM POUCO ANTES DE SUBMETER
            await asyncio.sleep(2)

            # 5. SUBMETER FORMUL√ÅRIO COM TRATAMENTO MELHORADO
            submit_selectors = [
                'input[value="Pesquisar"]',
                'input[name="pbConsultar"]',
                'button:text("Pesquisar")',
                'input[type="submit"]',
                'button[type="submit"]',
                ".botaoPesquisar",
                "#pbConsultar",
                '[onclick*="consultar"]',
            ]

            submitted = False
            last_error = None

            for selector in submit_selectors:
                try:
                    # Verificar se o elemento existe
                    element = await self.page.query_selector(selector)
                    if element:
                        # Verificar se √© vis√≠vel e habilitado
                        is_visible = await element.is_visible()
                        is_enabled = await element.is_enabled()

                        if is_visible and is_enabled:
                            logger.info(
                                f"üéØ Tentando submeter com selector: {selector}"
                            )
                            await element.click()

                            # Aguardar navega√ß√£o ou carregamento
                            try:
                                await self.page.wait_for_load_state(
                                    "networkidle", timeout=15000
                                )
                                await asyncio.sleep(3)  # Aguardar resultados carregarem

                                # Verificar se a submiss√£o foi bem-sucedida
                                current_url = self.page.url
                                if (
                                    "consultaAvancada" not in current_url
                                    or "resultado" in current_url.lower()
                                ):
                                    logger.info(
                                        f"‚úÖ Formul√°rio submetido com sucesso (selector: {selector})"
                                    )
                                    submitted = True
                                    break
                                else:
                                    logger.debug(
                                        f"‚ö†Ô∏è URL n√£o mudou ap√≥s submiss√£o: {current_url}"
                                    )
                            except Exception as e:
                                logger.debug(
                                    f"‚ö†Ô∏è Timeout aguardando resposta para {selector}: {e}"
                                )
                                # Continuar tentando outros seletores
                        else:
                            logger.debug(
                                f"‚ùå Elemento {selector} n√£o est√° vis√≠vel ou habilitado"
                            )
                except Exception as e:
                    last_error = e
                    logger.debug(f"‚ùå Falha ao tentar {selector}: {e}")
                    continue

            if not submitted:
                error_msg = f"Falha ao submeter formul√°rio. √öltimo erro: {last_error}"
                logger.error(f"‚ùå {error_msg}")

                # Salvar screenshot para debug
                await self._save_debug_screenshot("form_submit_error")

                # Tentar submiss√£o por JavaScript como √∫ltimo recurso
                try:
                    logger.info("üîÑ Tentando submiss√£o via JavaScript...")
                    submit_result = await self.page.evaluate("""
                        () => {
                            const forms = document.querySelectorAll('form');
                            for (const form of forms) {
                                if (form.action && form.action.includes('consulta')) {
                                    form.submit();
                                    return 'Formul√°rio submetido via JavaScript';
                                }
                            }
                            return 'Nenhum formul√°rio encontrado';
                        }
                    """)
                    logger.info(f"üìù Resultado JavaScript: {submit_result}")

                    if "submetido" in submit_result:
                        await asyncio.sleep(5)  # Aguardar mais tempo para JS
                        submitted = True
                        logger.info("‚úÖ Formul√°rio submetido via JavaScript")
                except Exception as js_error:
                    logger.error(f"‚ùå Falha na submiss√£o JavaScript: {js_error}")

            if not submitted:
                raise Exception(error_msg)

            logger.info("‚úÖ Pesquisa executada com crit√©rios espec√≠ficos")

        except Exception as error:
            logger.error(f"‚ùå Erro ao preencher formul√°rio: {error}")
            # Debug: capturar screenshot
            await self._save_debug_screenshot("form_error")
            raise

    async def _extract_publications_from_pdf_links(
        self, max_pages: int
    ) -> AsyncGenerator[Publication, None]:
        """
        Encontra os links em tr[class="ementaClass"] e baixa os PDFs para processamento
        """
        logger.info("üîç Buscando links de PDF nos resultados")

        current_page = 1

        while current_page <= max_pages:
            logger.info(f"üìÑ Processando p√°gina {current_page}/{max_pages}")

            try:
                # Aguardar carregamento dos resultados
                await asyncio.sleep(3)

                # Tentar aguardar elementos aparecerem
                try:
                    await self.page.wait_for_selector("tr.ementaClass", timeout=10000)
                except:
                    logger.warning("‚ö†Ô∏è Timeout aguardando tr.ementaClass")

                # Encontrar todos os elementos tr com class="ementaClass"
                ementa_elements = await self.page.query_selector_all("tr.ementaClass")

                if not ementa_elements:
                    logger.warning("‚ö†Ô∏è Nenhum elemento tr.ementaClass encontrado")

                    # Debug: verificar se h√° outros elementos
                    all_tr = await self.page.query_selector_all("tr")
                    logger.info(f"üîç Total de elementos tr: {len(all_tr)}")

                    onclick_elements = await self.page.query_selector_all(
                        '[onclick*="consultaSimples.do"]'
                    )
                    logger.info(
                        f"üîç Elementos com consultaSimples.do: {len(onclick_elements)}"
                    )

                    if onclick_elements:
                        logger.info(
                            "‚úÖ Encontrados elementos com consultaSimples.do, processando diretamente..."
                        )
                        # Processar elementos com onclick diretamente
                        for i, element in enumerate(onclick_elements):
                            try:
                                onclick_attr = await element.get_attribute("onclick")
                                if (
                                    onclick_attr
                                    and "consultaSimples.do" in onclick_attr
                                ):
                                    pdf_url = await self._extract_pdf_url_from_onclick(
                                        onclick_attr
                                    )
                                    if pdf_url:
                                        # Verificar se este PDF j√° falhou antes
                                        if pdf_url in self.failed_pdfs:
                                            logger.warning(
                                                f"‚è≠Ô∏è Pulando PDF que falhou anteriormente: {pdf_url}"
                                            )
                                            continue

                                        async for (
                                            publication
                                        ) in self._download_and_process_pdf(pdf_url):
                                            yield publication
                            except Exception as e:
                                logger.warning(
                                    f"‚ö†Ô∏è Erro ao processar elemento onclick {i + 1}: {e}"
                                )
                                continue

                    break

                logger.info(
                    f"‚úÖ Encontrados {len(ementa_elements)} elementos com links"
                )

                # Processar cada elemento para extrair links
                for i, element in enumerate(ementa_elements):
                    try:
                        # Buscar elementos com onclick que cont√©m links para PDF
                        onclick_elements = await element.query_selector_all(
                            '[onclick*="popup"]'
                        )

                        for onclick_element in onclick_elements:
                            onclick_attr = await onclick_element.get_attribute(
                                "onclick"
                            )

                            if onclick_attr and "consultaSimples.do" in onclick_attr:
                                # Extrair URL do PDF do atributo onclick
                                pdf_url = await self._extract_pdf_url_from_onclick(
                                    onclick_attr
                                )

                                if pdf_url:
                                    # Verificar se este PDF j√° falhou antes
                                    if pdf_url in self.failed_pdfs:
                                        logger.warning(
                                            f"‚è≠Ô∏è Pulando PDF que falhou anteriormente: {pdf_url}"
                                        )
                                        continue

                                    # Baixar e processar PDF
                                    async for (
                                        publication
                                    ) in self._download_and_process_pdf(pdf_url):
                                        yield publication

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Erro ao processar elemento {i + 1}: {e}")
                        continue

                # Tentar navegar para pr√≥xima p√°gina
                has_next = await self._navigate_to_next_page()
                if not has_next:
                    logger.info("üìÑ N√£o h√° mais p√°ginas dispon√≠veis")
                    break

                current_page += 1

            except Exception as error:
                logger.error(f"‚ùå Erro na p√°gina {current_page}: {error}")
                break

    async def _extract_pdf_url_from_onclick(self, onclick_attr: str) -> Optional[str]:
        """
        Extrai URL do PDF do atributo onclick
        Exemplo: onclick="return popup('/cdje/consultaSimples.do?cdVolume=19&nuDiario=4092&cdCaderno=12&nuSeqpagina=3710');"
        """
        try:
            # Usar regex para extrair a URL do popup
            match = re.search(r"popup\('([^']+)'\)", onclick_attr)
            if match:
                relative_url = match.group(1)
                # Construir URL completa
                base_url = "https://esaj.tjsp.jus.br"
                full_url = f"{base_url}{relative_url}"
                logger.debug(f"üìÑ URL do PDF extra√≠da: {full_url}")
                return full_url
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao extrair URL do PDF: {e}")

        return None

    async def _download_and_process_pdf(
        self, pdf_url: str
    ) -> AsyncGenerator[Publication, None]:
        """
        Baixa o PDF e processa seu conte√∫do para extrair publica√ß√µes
        Com retry e timeouts configur√°veis
        """
        max_retries = 3
        base_delay = 2

        for attempt in range(max_retries):
            try:
                logger.info(
                    f"üì• Baixando PDF (tentativa {attempt + 1}/{max_retries}): {pdf_url}"
                )

                # Abrir nova aba para download
                pdf_page = await self.browser.new_page()

                try:
                    # Configurar timeouts mais longos para PDFs problem√°ticos
                    pdf_page.set_default_timeout(60000)  # 60 segundos
                    pdf_page.set_default_navigation_timeout(60000)  # 60 segundos

                    # Configurar para interceptar downloads
                    download_info = None

                    async def handle_download(download: Download):
                        nonlocal download_info
                        download_info = download

                    pdf_page.on("download", handle_download)

                    # Navegar para URL do PDF com timeout espec√≠fico
                    try:
                        await pdf_page.goto(
                            pdf_url, timeout=60000, wait_until="domcontentloaded"
                        )

                        # Aguardar um pouco para o download come√ßar
                        await asyncio.sleep(2)

                        # Tentar aguardar networkidle com timeout menor
                        try:
                            await pdf_page.wait_for_load_state(
                                "networkidle", timeout=10000
                            )
                        except:
                            logger.debug("‚è∞ Timeout no networkidle, continuando...")

                    except Exception as nav_error:
                        if "Timeout" in str(nav_error):
                            logger.warning(
                                f"‚è∞ Timeout na navega√ß√£o (tentativa {attempt + 1}): {pdf_url}"
                            )
                            if attempt < max_retries - 1:
                                delay = base_delay * (2**attempt)
                                logger.info(
                                    f"üîÑ Aguardando {delay}s antes da pr√≥xima tentativa..."
                                )
                                await asyncio.sleep(delay)
                                continue
                        raise nav_error

                    # Se houve download, processar o arquivo
                    if download_info:
                        pdf_path = (
                            self.temp_dir
                            / f"dje_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}.pdf"
                        )
                        await download_info.save_as(pdf_path)
                        logger.info(f"‚úÖ PDF baixado: {pdf_path}")

                        # Processar PDF para extrair publica√ß√µes
                        async for publication in self._process_pdf_content(pdf_path):
                            yield publication

                        # üêõ MODO DEBUG: Mover PDF para pasta de debug ao inv√©s de apagar
                        debug_filename = (
                            f"debug_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}.pdf"
                        )
                        debug_path = self.pdf_debug_dir / debug_filename
                        try:
                            shutil.move(str(pdf_path), str(debug_path))
                            logger.info(f"üêõ PDF salvo para debug: {debug_path}")
                        except Exception as move_error:
                            logger.warning(
                                f"‚ö†Ô∏è Erro ao mover PDF para debug: {move_error}"
                            )
                            # Fallback: copiar e depois apagar
                            try:
                                shutil.copy2(str(pdf_path), str(debug_path))
                                pdf_path.unlink()
                                logger.info(f"üêõ PDF copiado para debug: {debug_path}")
                            except Exception as copy_error:
                                logger.error(f"‚ùå Erro ao copiar PDF: {copy_error}")
                                pdf_path.unlink()  # Apagar original em caso de falha total
                        return  # Sucesso, sair do loop de retry

                    else:
                        # Se n√£o houve download, tentar extrair conte√∫do da p√°gina
                        content = await pdf_page.content()
                        if content and len(content) > 100:
                            logger.info("üìÑ Processando conte√∫do HTML como fallback")
                            # Processar conte√∫do HTML como fallback
                            publications = self.parser.parse_multiple_publications(
                                content, pdf_url
                            )
                            for publication in publications:
                                # Salvar como JSON
                                try:
                                    json_path = (
                                        await self.json_saver.save_publication_json(
                                            publication
                                        )
                                    )
                                    if json_path:
                                        logger.info(
                                            f"üìã JSON salvo (HTML fallback): {json_path}"
                                        )
                                    else:
                                        logger.warning(
                                            f"‚ö†Ô∏è Falha ao salvar JSON para {publication.process_number}"
                                        )
                                except Exception as json_error:
                                    logger.error(
                                        f"‚ùå Erro ao salvar JSON (HTML fallback): {json_error}"
                                    )

                finally:
                    await pdf_page.close()

            except Exception as error:
                logger.warning(
                    f"‚ö†Ô∏è Erro na tentativa {attempt + 1} para PDF {pdf_url}: {error}"
                )

                if attempt < max_retries - 1:
                    delay = base_delay * (2**attempt)
                    logger.info(f"üîÑ Aguardando {delay}s antes da pr√≥xima tentativa...")
                    await asyncio.sleep(delay)
                else:
                    logger.error(
                        f"‚ùå Falha definitiva ao baixar/processar PDF ap√≥s {max_retries} tentativas: {pdf_url}"
                    )
                    logger.error(f"   √öltimo erro: {error}")
                    # Marcar PDF como problem√°tico para evitar tentativas futuras
                    self.failed_pdfs.add(pdf_url)
                    logger.info(f"üö´ PDF marcado como problem√°tico: {pdf_url}")
                    # N√£o yieldar nada em caso de falha total

    async def _process_pdf_content(
        self, pdf_path: Path
    ) -> AsyncGenerator[Publication, None]:
        """
        Processa o conte√∫do do PDF para extrair publica√ß√µes
        """
        logger.info(f"üìñ Processando conte√∫do do PDF: {pdf_path}")

        try:
            # Importar PyPDF2 ou usar alternativa para extrair texto do PDF
            try:
                import PyPDF2

                with open(pdf_path, "rb") as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    text_content = ""

                    for page in pdf_reader.pages:
                        text_content += page.extract_text() + "\n"

                logger.info(f"‚úÖ Texto extra√≠do do PDF ({len(text_content)} chars)")

            except ImportError:
                logger.warning("‚ö†Ô∏è PyPDF2 n√£o dispon√≠vel, tentando m√©todo alternativo")
                # Fallback: usar pdfplumber ou similar
                try:
                    import pdfplumber

                    with pdfplumber.open(pdf_path) as pdf:
                        text_content = ""
                        for page in pdf.pages:
                            text_content += page.extract_text() + "\n"

                    logger.info(
                        f"‚úÖ Texto extra√≠do com pdfplumber ({len(text_content)} chars)"
                    )

                except ImportError:
                    logger.error("‚ùå Nenhuma biblioteca de PDF dispon√≠vel")
                    return

            # Usar o parser aprimorado para extrair publica√ß√µes
            if text_content and len(text_content.strip()) > 50:
                # Tentar primeiro com o parser aprimorado (padr√£o RPV/INSS)
                try:
                    # Extrair n√∫mero da p√°gina da URL se poss√≠vel
                    page_number = self._extract_page_number_from_url(str(pdf_path))

                    enhanced_publications = (
                        await self.enhanced_parser.parse_multiple_publications_enhanced(
                            text_content, str(pdf_path), page_number
                        )
                    )

                    if enhanced_publications:
                        logger.info(
                            f"‚úÖ Parser aprimorado extraiu {len(enhanced_publications)} publica√ß√µes"
                        )
                        for publication in enhanced_publications:
                            logger.info(
                                f"‚úÖ Publica√ß√£o extra√≠da (aprimorado): {publication.process_number}"
                            )

                            # Salvar como JSON
                            try:
                                json_path = await self.json_saver.save_publication_json(
                                    publication
                                )
                                if json_path:
                                    logger.info(f"üìã JSON salvo: {json_path}")
                                else:
                                    logger.warning(
                                        f"‚ö†Ô∏è Falha ao salvar JSON para {publication.process_number}"
                                    )
                            except Exception as json_error:
                                logger.error(f"‚ùå Erro ao salvar JSON: {json_error}")

                            yield publication
                        logger.info(
                            f"‚úÖ Parser aprimorado extraiu {len(enhanced_publications)} publica√ß√µes"
                        )
                        for publication in enhanced_publications:
                            logger.info(
                                f"‚úÖ Publica√ß√£o extra√≠da (aprimorado): {publication.process_number}"
                            )

                            # Salvar como JSON
                            try:
                                json_path = await self.json_saver.save_publication_json(
                                    publication
                                )
                                if json_path:
                                    logger.info(f"üìã JSON salvo: {json_path}")
                                else:
                                    logger.warning(
                                        f"‚ö†Ô∏è Falha ao salvar JSON para {publication.process_number}"
                                    )
                            except Exception as json_error:
                                logger.error(f"‚ùå Erro ao salvar JSON: {json_error}")

                            yield publication
                    else:
                        # Fallback para parser tradicional
                        logger.info("üîÑ Usando parser tradicional como fallback")
                        publications = self.parser.parse_multiple_publications(
                            text_content, str(pdf_path)
                        )

                        for publication in publications:
                            logger.info(
                                f"‚úÖ Publica√ß√£o extra√≠da (tradicional): {publication.process_number}"
                            )

                            # Salvar como JSON
                            try:
                                json_path = await self.json_saver.save_publication_json(
                                    publication
                                )
                                if json_path:
                                    logger.info(f"üìã JSON salvo: {json_path}")
                                else:
                                    logger.warning(
                                        f"‚ö†Ô∏è Falha ao salvar JSON para {publication.process_number}"
                                    )
                            except Exception as json_error:
                                logger.error(f"‚ùå Erro ao salvar JSON: {json_error}")

                            yield publication

                except Exception as e:
                    logger.warning(
                        f"‚ö†Ô∏è Erro no parser aprimorado, usando tradicional: {e}"
                    )
                    # Fallback para parser tradicional
                    publications = self.parser.parse_multiple_publications(
                        text_content, str(pdf_path)
                    )

                    for publication in publications:
                        logger.info(
                            f"‚úÖ Publica√ß√£o extra√≠da (fallback): {publication.process_number}"
                        )

                        # Salvar como JSON
                        try:
                            json_path = await self.json_saver.save_publication_json(
                                publication
                            )
                            if json_path:
                                logger.info(f"üìã JSON salvo: {json_path}")
                            else:
                                logger.warning(
                                    f"‚ö†Ô∏è Falha ao salvar JSON para {publication.process_number}"
                                )
                        except Exception as json_error:
                            logger.error(f"‚ùå Erro ao salvar JSON: {json_error}")

                        yield publication
            else:
                logger.warning("‚ö†Ô∏è Conte√∫do do PDF muito pequeno ou vazio")

        except Exception as error:
            logger.error(f"‚ùå Erro ao processar PDF {pdf_path}: {error}")

    async def _navigate_to_next_page(self) -> bool:
        """Navega para a pr√≥xima p√°gina de resultados"""
        try:
            # Procurar por link de pr√≥xima p√°gina
            next_selectors = [
                'a:text("Pr√≥xima")',
                'a:text(">")',
                'a[title*="pr√≥xima"]',
                'input[value="Pr√≥xima"]',
            ]

            for selector in next_selectors:
                next_element = await self.page.query_selector(selector)
                if next_element:
                    await next_element.click()
                    await self.page.wait_for_load_state("networkidle")
                    logger.info("‚úÖ Navega√ß√£o para pr√≥xima p√°gina")
                    return True

            logger.info("üìÑ Nenhum link para pr√≥xima p√°gina encontrado")
            return False

        except Exception as error:
            logger.warning(f"‚ö†Ô∏è Erro ao navegar para pr√≥xima p√°gina: {error}")
            return False

    async def _save_debug_screenshot(self, name: str) -> None:
        """Salva screenshot para debug"""
        try:
            # Garantir que o diret√≥rio debug_images existe
            debug_dir = Path("logs/debug_images")
            debug_dir.mkdir(parents=True, exist_ok=True)

            screenshot_path = (
                debug_dir
                / f"debug_{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            )
            await self.page.screenshot(path=str(screenshot_path))
            logger.info(f"üêõ Screenshot de debug: {screenshot_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao salvar screenshot: {e}")

    # M√©todos legados removidos - agora tudo √© processado via PDF
    def _contains_all_terms(self, content: str, search_terms: List[str]) -> bool:
        """Verifica se o conte√∫do cont√©m todos os termos obrigat√≥rios"""
        content_lower = content.lower()
        return all(term.lower() in content_lower for term in search_terms)

    def _extract_page_number_from_url(self, url_or_path: str) -> Optional[int]:
        """Extrai n√∫mero da p√°gina da URL ou caminho do PDF"""
        try:
            # Buscar padr√£o nuSeqpagina=XXXX na URL
            match = re.search(r"nuSeqpagina=(\d+)", url_or_path)
            if match:
                return int(match.group(1))
        except Exception as e:
            logger.debug(f"Erro ao extrair n√∫mero da p√°gina: {e}")
        return None
