"""
üîÑ DJE Page Manager - Gerenciamento Inteligente de P√°ginas PDF

Resolve o problema de publica√ß√µes divididas entre p√°ginas atrav√©s de:
1. Download autom√°tico de p√°ginas anteriores
2. Cache em mem√≥ria para evitar re-downloads
3. Merge inteligente de conte√∫do dividido
4. Valida√ß√£o de qualidade do merge
"""

import re
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any, List
from urllib.parse import parse_qs, urlparse
from datetime import datetime

from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


class DJEPageManager:
    """
    Gerenciador inteligente de p√°ginas PDF do DJE-SP

    Features:
    - Download autom√°tico de p√°ginas anteriores quando necess√°rio
    - Cache em mem√≥ria para evitar downloads duplicados
    - Estat√≠sticas de performance (cache hit/miss)
    - Limpeza autom√°tica de cache
    """

    def __init__(self, scraper_adapter):
        self.scraper_adapter = scraper_adapter
        self.page_cache: Dict[str, str] = {}  # Cache de p√°ginas: {page_key: content}
        self.cache_stats = {"hits": 0, "misses": 0, "downloads": 0, "cache_size": 0}

        logger.info("üìÑ DJEPageManager inicializado")

    async def get_previous_page_content(
        self, current_url: str, current_page: int
    ) -> Optional[str]:
        """
        Obt√©m conte√∫do da p√°gina anterior com cache inteligente

        Args:
            current_url: URL da p√°gina atual
            current_page: N√∫mero da p√°gina atual

        Returns:
            Conte√∫do da p√°gina anterior ou None se n√£o conseguir obter
        """
        if current_page <= 1:
            logger.debug("üìÑ J√° est√° na primeira p√°gina, n√£o h√° p√°gina anterior")
            return None

        previous_page = current_page - 1
        cache_key = self._generate_cache_key(current_url, previous_page)

        # Verificar cache primeiro
        if cache_key in self.page_cache:
            self.cache_stats["hits"] += 1
            logger.info(f"‚úÖ Cache HIT para p√°gina {previous_page}")
            return self.page_cache[cache_key]

        # Cache miss - fazer download
        self.cache_stats["misses"] += 1
        logger.info(f"üì• Cache MISS - Baixando p√°gina {previous_page}")

        try:
            previous_url = self._build_previous_page_url(current_url, previous_page)

            if not previous_url:
                logger.error(
                    f"‚ùå N√£o foi poss√≠vel construir URL da p√°gina {previous_page}"
                )
                return None

            logger.info(f"üì• Baixando p√°gina anterior: {previous_page}")
            logger.debug(f"üîó URL: {previous_url}")

            # Download da p√°gina anterior
            content = await self._download_pdf_page_content(previous_url)

            if content:
                # Adicionar ao cache
                self.page_cache[cache_key] = content
                self.cache_stats["downloads"] += 1
                self.cache_stats["cache_size"] = len(self.page_cache)

                logger.info(
                    f"‚úÖ P√°gina {previous_page} baixada e armazenada no cache ({len(content)} chars)"
                )
                logger.info(
                    f"üìä Cache stats: {self.cache_stats['hits']} hits, {self.cache_stats['misses']} misses"
                )

                return content
            else:
                logger.warning(f"‚ö†Ô∏è Conte√∫do vazio para p√°gina {previous_page}")
                return None

        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar p√°gina {previous_page}: {e}")
            return None

    def _build_previous_page_url(
        self, current_url: str, target_page: int
    ) -> Optional[str]:
        """
        Constr√≥i URL da p√°gina anterior baseada na URL atual

        Exemplo de URL DJE:
        https://esaj.tjsp.jus.br/cdje/consultaSimples.do?cdVolume=19&nuDiario=4092&cdCaderno=12&nuSeqpagina=3710
        """
        try:
            # Usar regex para substituir nuSeqpagina
            pattern = r"nuSeqpagina=(\d+)"
            match = re.search(pattern, current_url)

            if match:
                previous_url = re.sub(
                    pattern, f"nuSeqpagina={target_page}", current_url
                )
                logger.debug(f"üîß URL constru√≠da: {previous_url}")
                return previous_url
            else:
                logger.error(
                    f"‚ùå Padr√£o nuSeqpagina n√£o encontrado na URL: {current_url}"
                )
                return None

        except Exception as e:
            logger.error(f"‚ùå Erro ao construir URL da p√°gina anterior: {e}")
            return None

    async def _download_pdf_page_content(self, url: str) -> Optional[str]:
        """
        Baixa e extrai conte√∫do de uma p√°gina PDF espec√≠fica
        """
        try:
            # Abrir nova aba no browser
            page = await self.scraper_adapter.browser.new_page()

            try:
                # Configurar timeouts
                page.set_default_timeout(30000)

                # Navegar para a URL
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)

                # Aguardar carregamento
                await asyncio.sleep(2)

                # Tentar aguardar networkidle
                try:
                    await page.wait_for_load_state("networkidle", timeout=10000)
                except:
                    logger.debug("‚è∞ Timeout no networkidle para p√°gina anterior")

                # Extrair conte√∫do HTML
                content = await page.content()

                if content and len(content) > 100:
                    logger.debug(
                        f"‚úÖ Conte√∫do extra√≠do da p√°gina: {len(content)} chars"
                    )
                    return content
                else:
                    logger.warning("‚ö†Ô∏è Conte√∫do muito pequeno ou vazio")
                    return None

            finally:
                await page.close()

        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar conte√∫do da p√°gina {url}: {e}")
            return None

    def _generate_cache_key(self, url: str, page_number: int) -> str:
        """
        Gera chave √∫nica para cache baseada na URL e n√∫mero da p√°gina
        """
        # Extrair par√¢metros relevantes da URL para criar chave √∫nica
        try:
            parsed = urlparse(url)
            query_params = parse_qs(parsed.query)

            # Usar par√¢metros cr√≠ticos para gerar chave
            volume = query_params.get("cdVolume", [""])[0]
            diario = query_params.get("nuDiario", [""])[0]
            caderno = query_params.get("cdCaderno", [""])[0]

            cache_key = f"page_{volume}_{diario}_{caderno}_{page_number}"
            return cache_key

        except Exception as e:
            logger.debug(f"Erro ao gerar chave de cache: {e}")
            # Fallback para chave simples
            return f"page_{page_number}_{hash(url) % 10000}"

    def extract_page_number_from_url(self, url: str) -> Optional[int]:
        """
        Extrai n√∫mero da p√°gina da URL
        """
        try:
            match = re.search(r"nuSeqpagina=(\d+)", url)
            if match:
                return int(match.group(1))
        except Exception as e:
            logger.debug(f"Erro ao extrair n√∫mero da p√°gina: {e}")
        return None

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas detalhadas do cache
        """
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = (
            (self.cache_stats["hits"] / total_requests * 100)
            if total_requests > 0
            else 0
        )

        return {
            "cache_hits": self.cache_stats["hits"],
            "cache_misses": self.cache_stats["misses"],
            "total_requests": total_requests,
            "hit_rate_percent": round(hit_rate, 2),
            "downloads_made": self.cache_stats["downloads"],
            "cache_size": self.cache_stats["cache_size"],
            "cache_memory_usage": sum(
                len(content) for content in self.page_cache.values()
            ),
            "cached_pages": list(self.page_cache.keys()),
        }

    def clear_cache(self):
        """
        Limpa cache de p√°ginas e reseta estat√≠sticas
        """
        cache_size_before = len(self.page_cache)
        self.page_cache.clear()

        # Manter estat√≠sticas hist√≥ricas, apenas resetar tamanho
        self.cache_stats["cache_size"] = 0

        logger.info(f"üßπ Cache limpo: {cache_size_before} p√°ginas removidas")

    def optimize_cache(self, max_cache_size: int = 50):
        """
        Otimiza cache removendo p√°ginas antigas se exceder limite
        """
        if len(self.page_cache) <= max_cache_size:
            return

        # Remover p√°ginas mais antigas (implementa√ß√£o simples - FIFO)
        pages_to_remove = len(self.page_cache) - max_cache_size
        oldest_keys = list(self.page_cache.keys())[:pages_to_remove]

        for key in oldest_keys:
            del self.page_cache[key]

        self.cache_stats["cache_size"] = len(self.page_cache)
        logger.info(f"üîß Cache otimizado: {pages_to_remove} p√°ginas removidas")


class PublicationContentMerger:
    """
    Gerenciador para merge inteligente de publica√ß√µes divididas entre p√°ginas

    Features:
    - Encontra √∫ltimo processo na p√°gina anterior
    - Merge com valida√ß√£o de qualidade
    - Detec√ß√£o de termos RPV/INSS no conte√∫do merged
    - Logs detalhados do processo de merge
    """

    def __init__(self):
        self.PROCESS_PATTERN = re.compile(
            r"Processo\s+(\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})", re.IGNORECASE
        )

        # Padr√µes para valida√ß√£o de merge
        self.RPV_PATTERNS = [
            re.compile(r"\bRPV\b", re.IGNORECASE),
            re.compile(r"requisi√ß√£o\s+de\s+pequeno\s+valor", re.IGNORECASE),
            re.compile(r"pagamento\s+pelo\s+INSS", re.IGNORECASE),
        ]

        logger.info("üîÑ PublicationContentMerger inicializado")

    def merge_cross_page_publication(
        self,
        previous_page_content: str,
        current_page_content: str,
        rpv_position_in_current: int,
    ) -> str:
        """
        Faz merge inteligente de uma publica√ß√£o dividida entre duas p√°ginas

        Args:
            previous_page_content: Conte√∫do da p√°gina anterior
            current_page_content: Conte√∫do da p√°gina atual
            rpv_position_in_current: Posi√ß√£o do termo RPV na p√°gina atual

        Returns:
            Conte√∫do completo da publica√ß√£o merged
        """
        logger.info("üîÑ Iniciando merge de publica√ß√£o dividida entre p√°ginas")

        try:
            # 1. Encontrar o √∫ltimo processo na p√°gina anterior
            last_process_in_previous = self._find_last_process_in_content(
                previous_page_content
            )

            if not last_process_in_previous:
                logger.warning("‚ö†Ô∏è √öltimo processo n√£o encontrado na p√°gina anterior")
                return current_page_content

            logger.info(
                f"üìã √öltimo processo encontrado: {last_process_in_previous['process_number']}"
            )

            # 2. Extrair conte√∫do da p√°gina anterior a partir do √∫ltimo processo
            previous_part = previous_page_content[
                last_process_in_previous["start_pos"] :
            ]

            # 3. Extrair conte√∫do da p√°gina atual at√© antes do primeiro processo (se houver)
            first_process_in_current = self._find_first_process_in_content(
                current_page_content
            )

            if first_process_in_current:
                # Se h√° um processo na p√°gina atual, usar conte√∫do at√© esse processo
                current_part = current_page_content[
                    : first_process_in_current["start_pos"]
                ]
                logger.info(
                    f"üìã Primeiro processo na p√°gina atual: {first_process_in_current['process_number']}"
                )
            else:
                # Se n√£o h√° processo na p√°gina atual, usar toda a p√°gina
                current_part = current_page_content
                logger.info(
                    "üìã Nenhum processo na p√°gina atual - usando todo o conte√∫do"
                )

            # 4. Fazer merge
            merged_content = previous_part + "\n" + current_part

            logger.info(
                f"‚úÖ Merge conclu√≠do: {len(previous_part)} + {len(current_part)} = {len(merged_content)} chars"
            )

            # 5. Log detalhado para debugging
            self._log_merge_details(
                last_process_in_previous,
                first_process_in_current,
                len(previous_part),
                len(current_part),
            )

            return merged_content

        except Exception as e:
            logger.error(f"‚ùå Erro no merge de p√°ginas: {e}")
            return current_page_content

    def _find_last_process_in_content(self, content: str) -> Optional[Dict[str, Any]]:
        """
        Encontra o √∫ltimo processo no conte√∫do
        """
        matches = list(self.PROCESS_PATTERN.finditer(content))

        if matches:
            last_match = matches[-1]
            process_number = last_match.group(1)
            start_pos = last_match.start()

            logger.debug(
                f"üîç √öltimo processo encontrado: {process_number} na posi√ß√£o {start_pos}"
            )

            return {
                "process_number": process_number,
                "start_pos": start_pos,
                "match": last_match,
            }

        logger.debug("üîç Nenhum processo encontrado no conte√∫do")
        return None

    def _find_first_process_in_content(self, content: str) -> Optional[Dict[str, Any]]:
        """
        Encontra o primeiro processo no conte√∫do
        """
        match = self.PROCESS_PATTERN.search(content)

        if match:
            process_number = match.group(1)
            start_pos = match.start()

            logger.debug(
                f"üîç Primeiro processo encontrado: {process_number} na posi√ß√£o {start_pos}"
            )

            return {
                "process_number": process_number,
                "start_pos": start_pos,
                "match": match,
            }

        logger.debug("üîç Nenhum processo encontrado no conte√∫do")
        return None

    def validate_merged_content(
        self, merged_content: str, expected_rpv_terms: List[str]
    ) -> bool:
        """
        Valida se o conte√∫do merged cont√©m os termos RPV esperados e tem qualidade adequada

        Args:
            merged_content: Conte√∫do ap√≥s merge
            expected_rpv_terms: Lista de termos que devem estar presentes

        Returns:
            True se o merge √© v√°lido, False caso contr√°rio
        """
        logger.info("üîç Validando qualidade do merge")

        # 1. Verificar se conte√∫do n√£o est√° vazio ou muito pequeno
        if not merged_content or len(merged_content.strip()) < 50:
            logger.warning("‚ùå Merge inv√°lido: conte√∫do muito pequeno")
            return False

        # 2. Verificar se cont√©m pelo menos um termo RPV
        rpv_found = False
        for pattern in self.RPV_PATTERNS:
            if pattern.search(merged_content):
                rpv_found = True
                break

        if not rpv_found:
            logger.warning("‚ùå Merge inv√°lido: nenhum termo RPV encontrado")
            return False

        # 3. Verificar se cont√©m n√∫mero de processo v√°lido
        process_found = self.PROCESS_PATTERN.search(merged_content)
        if not process_found:
            logger.warning("‚ùå Merge inv√°lido: nenhum n√∫mero de processo encontrado")
            return False

        # 4. Verifica√ß√µes adicionais de qualidade
        quality_score = self._calculate_content_quality(merged_content)

        if quality_score < 0.7:  # 70% de qualidade m√≠nima
            logger.warning(f"‚ùå Merge inv√°lido: qualidade baixa ({quality_score:.2f})")
            return False

        logger.info(f"‚úÖ Merge v√°lido: qualidade {quality_score:.2f}")
        return True

    def _calculate_content_quality(self, content: str) -> float:
        """
        Calcula score de qualidade do conte√∫do (0.0 a 1.0)
        """
        score = 0.0

        # Verificar presen√ßa de elementos esperados
        if self.PROCESS_PATTERN.search(content):
            score += 0.3  # 30% por ter processo

        if any(pattern.search(content) for pattern in self.RPV_PATTERNS):
            score += 0.3  # 30% por ter RPV

        # Verificar presen√ßa de advogado
        if re.search(r"ADV\.|advogado", content, re.IGNORECASE):
            score += 0.2  # 20% por ter advogado

        # Verificar valores monet√°rios
        if re.search(r"R\$\s*[\d.,]+", content):
            score += 0.2  # 20% por ter valores

        return min(score, 1.0)

    def _log_merge_details(
        self,
        last_process: Dict[str, Any],
        first_process: Optional[Dict[str, Any]],
        previous_length: int,
        current_length: int,
    ):
        """
        Log detalhado do processo de merge para debugging
        """
        logger.debug("üìä Detalhes do merge:")
        logger.debug(
            f"   üìã √öltimo processo p√°gina anterior: {last_process['process_number']}"
        )

        if first_process:
            logger.debug(
                f"   üìã Primeiro processo p√°gina atual: {first_process['process_number']}"
            )
        else:
            logger.debug("üìã Nenhum processo na p√°gina atual")

        logger.debug(f"   üìè Tamanho conte√∫do anterior: {previous_length} chars")
        logger.debug(f"   üìè Tamanho conte√∫do atual: {current_length} chars")
        logger.debug(
            f"   üìè Tamanho total merged: {previous_length + current_length} chars"
        )

    def get_merge_statistics(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas de merge (placeholder para implementa√ß√£o futura)
        """
        return {
            "total_merges": 0,
            "successful_merges": 0,
            "failed_merges": 0,
            "average_merge_size": 0,
        }
