"""
Gerenciador de p√°ginas para lidar com publica√ß√µes divididas entre m√∫ltiplas p√°ginas PDF
"""

import re
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any
from urllib.parse import parse_qs, urlparse

from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


class DJEPageManager:
    """
    Gerencia o download e processamento de p√°ginas PDF sequenciais do DJE
    Para casos onde publica√ß√µes est√£o divididas entre p√°ginas
    """

    def __init__(self, scraper_adapter):
        self.scraper_adapter = scraper_adapter
        self.page_cache = {}  # Cache para evitar downloads desnecess√°rios
        
    async def get_previous_page_content(self, current_url: str, current_page: int) -> Optional[str]:
        """
        Baixa e extrai conte√∫do da p√°gina anterior
        
        Args:
            current_url: URL da p√°gina atual
            current_page: N√∫mero da p√°gina atual
            
        Returns:
            Conte√∫do da p√°gina anterior ou None se n√£o conseguir baixar
        """
        if current_page <= 1:
            logger.info("üìÑ J√° est√° na primeira p√°gina, n√£o h√° p√°gina anterior")
            return None
            
        previous_page = current_page - 1
        cache_key = f"page_{previous_page}"
        
        # Verificar cache primeiro
        if cache_key in self.page_cache:
            logger.info(f"üìã Usando p√°gina {previous_page} do cache")
            return self.page_cache[cache_key]
        
        try:
            # Construir URL da p√°gina anterior
            previous_url = self._build_previous_page_url(current_url, previous_page)
            
            if not previous_url:
                logger.error(f"‚ùå N√£o foi poss√≠vel construir URL da p√°gina {previous_page}")
                return None
            
            logger.info(f"üì• Baixando p√°gina anterior: {previous_page}")
            logger.debug(f"üîó URL: {previous_url}")
            
            # Baixar p√°gina anterior usando o mesmo m√©todo do scraper
            content = await self._download_pdf_page_content(previous_url)
            
            if content:
                # Adicionar ao cache
                self.page_cache[cache_key] = content
                logger.info(f"‚úÖ P√°gina {previous_page} baixada e armazenada no cache ({len(content)} chars)")
                return content
            else:
                logger.warning(f"‚ö†Ô∏è Conte√∫do vazio para p√°gina {previous_page}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar p√°gina {previous_page}: {e}")
            return None

    def _build_previous_page_url(self, current_url: str, target_page: int) -> Optional[str]:
        """
        Constr√≥i URL da p√°gina anterior baseada na URL atual
        
        Exemplo de URL DJE:
        https://esaj.tjsp.jus.br/cdje/consultaSimples.do?cdVolume=19&nuDiario=4092&cdCaderno=12&nuSeqpagina=3710
        """
        try:
            # Usar regex para substituir nuSeqpagina
            pattern = r'nuSeqpagina=(\d+)'
            match = re.search(pattern, current_url)
            
            if match:
                previous_url = re.sub(pattern, f'nuSeqpagina={target_page}', current_url)
                logger.debug(f"üîß URL constru√≠da: {previous_url}")
                return previous_url
            else:
                logger.error(f"‚ùå Padr√£o nuSeqpagina n√£o encontrado na URL: {current_url}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao construir URL da p√°gina anterior: {e}")
            return None

    async def _download_pdf_page_content(self, url: str) -> Optional[str]:
        """
        Baixa e extrai conte√∫do de uma p√°gina PDF espec√≠fica
        """
        try:
            # Abrir nova aba no browser
            page = await self.scraper_adapter.browser.new_page()
            
            try:
                # Configurar timeouts
                page.set_default_timeout(30000)
                
                # Navegar para a URL
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                # Aguardar carregamento
                await asyncio.sleep(2)
                
                # Tentar aguardar networkidle
                try:
                    await page.wait_for_load_state("networkidle", timeout=10000)
                except:
                    logger.debug("‚è∞ Timeout no networkidle para p√°gina anterior")
                
                # Extrair conte√∫do HTML
                content = await page.content()
                
                if content and len(content) > 100:
                    logger.debug(f"‚úÖ Conte√∫do extra√≠do da p√°gina: {len(content)} chars")
                    return content
                else:
                    logger.warning("‚ö†Ô∏è Conte√∫do muito pequeno ou vazio")
                    return None
                    
            finally:
                await page.close()
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao baixar conte√∫do da p√°gina {url}: {e}")
            return None

    def extract_page_number_from_url(self, url: str) -> Optional[int]:
        """
        Extrai n√∫mero da p√°gina da URL
        """
        try:
            match = re.search(r'nuSeqpagina=(\d+)', url)
            if match:
                return int(match.group(1))
        except Exception as e:
            logger.debug(f"Erro ao extrair n√∫mero da p√°gina: {e}")
        return None

    def clear_cache(self):
        """Limpa cache de p√°ginas"""
        self.page_cache.clear()
        logger.info("üßπ Cache de p√°ginas limpo")

    def get_cache_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas do cache"""
        return {
            "cached_pages": len(self.page_cache),
            "cache_size_chars": sum(len(content) for content in self.page_cache.values()),
            "cached_page_numbers": [int(key.split("_")[1]) for key in self.page_cache.keys()]
        }


class PublicationContentMerger:
    """
    Lida com a l√≥gica de merge de conte√∫do de publica√ß√µes divididas entre p√°ginas
    """

    def __init__(self):
        self.PROCESS_PATTERN = re.compile(r'Processo\s+(\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})', re.IGNORECASE)

    def merge_cross_page_publication(
        self, 
        previous_page_content: str, 
        current_page_content: str,
        rpv_position_in_current: int
    ) -> str:
        """
        Faz merge de uma publica√ß√£o que est√° dividida entre duas p√°ginas
        
        Args:
            previous_page_content: Conte√∫do da p√°gina anterior
            current_page_content: Conte√∫do da p√°gina atual  
            rpv_position_in_current: Posi√ß√£o do termo RPV na p√°gina atual
            
        Returns:
            Conte√∫do completo da publica√ß√£o merged
        """
        try:
            # 1. Encontrar o √∫ltimo processo na p√°gina anterior
            last_process_in_previous = self._find_last_process_in_content(previous_page_content)
            
            if not last_process_in_previous:
                logger.warning("‚ö†Ô∏è √öltimo processo n√£o encontrado na p√°gina anterior")
                return current_page_content
            
            # 2. Extrair conte√∫do da p√°gina anterior a partir do √∫ltimo processo
            previous_part = previous_page_content[last_process_in_previous['start_pos']:]
            
            # 3. Extrair conte√∫do da p√°gina atual at√© antes do primeiro processo (se houver)
            first_process_in_current = self._find_first_process_in_content(current_page_content)
            
            if first_process_in_current:
                current_part = current_page_content[:first_process_in_current['start_pos']]
            else:
                current_part = current_page_content
            
            # 4. Fazer merge
            merged_content = previous_part + "\n" + current_part
            
            logger.info(f"‚úÖ Merge conclu√≠do: {len(previous_part)} + {len(current_part)} = {len(merged_content)} chars")
            
            return merged_content
            
        except Exception as e:
            logger.error(f"‚ùå Erro no merge de p√°ginas: {e}")
            return current_page_content

    def _find_last_process_in_content(self, content: str) -> Optional[Dict[str, Any]]:
        """Encontra o √∫ltimo processo no conte√∫do"""
        matches = list(self.PROCESS_PATTERN.finditer(content))
        
        if matches:
            last_match = matches[-1]
            return {
                'process_number': last_match.group(1),
                'start_pos': last_match.start(),
                'match': last_match
            }
        
        return None

    def _find_first_process_in_content(self, content: str) -> Optional[Dict[str, Any]]:
        """Encontra o primeiro processo no conte√∫do"""
        match = self.PROCESS_PATTERN.search(content)
        
        if match:
            return {
                'process_number': match.group(1),
                'start_pos': match.start(),
                'match': match
            }
        
        return None

    def validate_merged_content(self, merged_content: str, expected_rpv_terms: list) -> bool:
        """
        Valida se o conte√∫do merged cont√©m os termos RPV esperados
        """
        content_lower = merged_content.lower()
        
        # Verificar se cont√©m pelo menos um dos termos RPV
        rpv_found = any(term.lower() in content_lower for term in ['rpv', 'pagamento pelo inss'])
        
        # Verificar se cont√©m pelo menos um processo
        process_found = bool(self.PROCESS_PATTERN.search(merged_content))
        
        is_valid = rpv_found and process_found
        
        if is_valid:
            logger.info("‚úÖ Conte√∫do merged validado com sucesso")
        else:
            logger.warning(f"‚ö†Ô∏è Valida√ß√£o falhou - RPV: {rpv_found}, Processo: {process_found}")
        
        return is_valid
