"""
Parser aprimorado ATUALIZADO com integra√ß√£o completa do Page Manager
Implementa a l√≥gica completa de download de p√°ginas anteriores
"""

import re
import unicodedata
from datetime import datetime
from typing import List, Optional, Dict, Any, Tuple
from decimal import Decimal, InvalidOperation

from domain.entities.publication import Publication, Lawyer, MonetaryValue
from infrastructure.logging.logger import setup_logger

logger = setup_logger(__name__)


class EnhancedDJEContentParser:
    """
    Parser especializado para extrair publica√ß√µes do DJE-SP seguindo fluxo espec√≠fico:
    1. Busca por "RPV" ou "pagamento pelo INSS" 
    2. Localiza in√≠cio com "Processo NUMERO_DO_PROCESSO"
    3. Extrai autores no formato "- NOME_DO_AUTOR - Vistos"
    4. Lida com publica√ß√µes divididas entre p√°ginas usando PageManager
    """

    # Padr√µes regex compilados
    PROCESS_NUMBER_PATTERN = re.compile(r"Processo\s+(\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})", re.IGNORECASE)
    
    # Padr√µes para buscar termos obrigat√≥rios
    RPV_PATTERNS = [
        re.compile(r"\bRPV\b", re.IGNORECASE),
        re.compile(r"requisi√ß√£o\s+de\s+pequeno\s+valor", re.IGNORECASE),
        re.compile(r"pagamento\s+pelo\s+INSS", re.IGNORECASE),
        re.compile(r"pagamento\s+de\s+benef√≠cio", re.IGNORECASE)
    ]
    
    # Padr√£o para autores no formato espec√≠fico "- NOME - Vistos"
    AUTHOR_PATTERN = re.compile(
        r"-\s+([A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á][A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á\s]{2,60}[A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á])\s+-\s+(?:Vistos|Visto|ADV)", 
        re.IGNORECASE
    )
    
    # Padr√µes para advogados aprimorados
    LAWYER_PATTERNS = [
        re.compile(r"ADV\.\s+([A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á][A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á\s]{2,50}[A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á])\s*\(\s*OAB\s+(\d+)\/?\w*\)", re.IGNORECASE),
        re.compile(r"([A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á][A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á\s]{2,50}[A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á])\s*\(\s*OAB\s+(\d+)\/?\w*\)", re.IGNORECASE),
    ]
    
    # Padr√µes monet√°rios espec√≠ficos para RPV
    MONETARY_PATTERNS = {
        "gross": [
            re.compile(r"valor\s+(?:principal|bruto|total|devido)[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
            re.compile(r"principal[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
            re.compile(r"valor\s+da\s+RPV[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
            re.compile(r"(?:quantia|import√¢ncia)\s+de\s+R\$?\s*([\d.,]+)", re.IGNORECASE),
        ],
        "net": [
            re.compile(r"valor\s+l√≠quido[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
            re.compile(r"l√≠quido[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
        ],
        "interest": [
            re.compile(r"juros\s+morat√≥rios[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
            re.compile(r"corre√ß√£o\s+monet√°ria[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
            re.compile(r"atualiza√ß√£o[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
        ],
        "fees": [
            re.compile(r"honor√°rios\s+advocat√≠cios[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
            re.compile(r"honor√°rios[:\s]*R\$?\s*([\d.,]+)", re.IGNORECASE),
        ]
    }

    def __init__(self):
        self.scraper_adapter = None
        self.page_manager = None
        self.content_merger = None
        self.confidence_threshold = 0.8
    
    def set_scraper_adapter(self, scraper_adapter):
        """Define o adapter do scraper e inicializa managers auxiliares"""
        self.scraper_adapter = scraper_adapter
        
        # Importar aqui para evitar depend√™ncia circular
        from infrastructure.web.page_manager import DJEPageManager, PublicationContentMerger
        
        self.page_manager = DJEPageManager(scraper_adapter)
        self.content_merger = PublicationContentMerger()

    async def parse_multiple_publications_enhanced(
        self, 
        content: str, 
        source_url: str = "", 
        current_page_number: Optional[int] = None
    ) -> List[Publication]:
        """
        Extrai m√∫ltiplas publica√ß√µes seguindo as instru√ß√µes espec√≠ficas
        Lida com publica√ß√µes divididas entre p√°ginas
        """
        logger.info("üîç Iniciando extra√ß√£o aprimorada de publica√ß√µes DJE-SP")
        
        publications = []
        
        # Normalizar conte√∫do
        normalized_content = self._normalize_text(content)
        
        # 1. Buscar todas as ocorr√™ncias de termos RPV/INSS
        rpv_matches = self._find_all_rpv_occurrences(normalized_content)
        
        if not rpv_matches:
            logger.info("‚ùå Nenhuma ocorr√™ncia de RPV ou pagamento pelo INSS encontrada")
            return publications
        
        logger.info(f"‚úÖ Encontradas {len(rpv_matches)} ocorr√™ncias de RPV/INSS")
        
        # 2. Para cada ocorr√™ncia, buscar o processo correspondente
        for i, rpv_match in enumerate(rpv_matches):
            logger.info(f"üìã Processando ocorr√™ncia {i + 1}/{len(rpv_matches)} na posi√ß√£o {rpv_match['position']}")
            
            try:
                publication = await self._extract_publication_for_rpv_occurrence(
                    normalized_content, 
                    rpv_match, 
                    source_url, 
                    current_page_number
                )
                
                if publication:
                    publications.append(publication)
                    logger.info(f"‚úÖ Publica√ß√£o extra√≠da: {publication.process_number}")
                else:
                    logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel extrair publica√ß√£o para ocorr√™ncia {i + 1}")
                    
            except Exception as e:
                logger.error(f"‚ùå Erro ao processar ocorr√™ncia {i + 1}: {e}")
                continue
        
        logger.info(f"üìä Total de publica√ß√µes extra√≠das: {len(publications)}")
        return publications

    def _find_all_rpv_occurrences(self, content: str) -> List[Dict[str, Any]]:
        """Encontra todas as ocorr√™ncias de RPV ou pagamento pelo INSS"""
        occurrences = []
        
        for pattern in self.RPV_PATTERNS:
            for match in pattern.finditer(content):
                occurrences.append({
                    'term': match.group(0),
                    'position': match.start(),
                    'pattern': pattern.pattern
                })
        
        # Ordenar por posi√ß√£o
        occurrences.sort(key=lambda x: x['position'])
        
        # Remover duplicatas pr√≥ximas (dentro de 50 caracteres)
        filtered_occurrences = []
        for occurrence in occurrences:
            if not filtered_occurrences or (occurrence['position'] - filtered_occurrences[-1]['position']) > 50:
                filtered_occurrences.append(occurrence)
        
        return filtered_occurrences

    async def _extract_publication_for_rpv_occurrence(
        self, 
        content: str, 
        rpv_match: Dict[str, Any], 
        source_url: str, 
        current_page_number: Optional[int]
    ) -> Optional[Publication]:
        """
        Extrai uma publica√ß√£o para uma ocorr√™ncia espec√≠fica de RPV/INSS
        Implementa a l√≥gica de busca para tr√°s e download de p√°gina anterior se necess√°rio
        """
        rpv_position = rpv_match['position']
        working_content = content
        content_was_merged = False
        
        # 1. Buscar processo para tr√°s a partir da posi√ß√£o do RPV
        process_match = self._find_process_before_position(working_content, rpv_position)
        
        if not process_match:
            logger.info(f"‚ùå Processo n√£o encontrado antes da posi√ß√£o {rpv_position}")
            
            # Verificar se √© necess√°rio baixar p√°gina anterior
            if current_page_number and current_page_number > 1 and self.page_manager:
                logger.info(f"üîÑ Tentando baixar p√°gina anterior ({current_page_number - 1}) para encontrar in√≠cio do processo")
                
                try:
                    previous_page_content = await self.page_manager.get_previous_page_content(
                        source_url, current_page_number
                    )
                    
                    if previous_page_content:
                        # Fazer merge do conte√∫do usando o ContentMerger
                        working_content = self.content_merger.merge_cross_page_publication(
                            previous_page_content, content, rpv_position
                        )
                        
                        # Validar merge
                        if self.content_merger.validate_merged_content(working_content, ['rpv', 'pagamento pelo inss']):
                            logger.info("‚úÖ Merge de p√°ginas validado com sucesso")
                            content_was_merged = True
                            
                            # Recalcular posi√ß√£o do RPV no conte√∫do merged
                            adjusted_rpv_position = len(previous_page_content) + rpv_position
                            
                            # Buscar processo novamente no conte√∫do merged
                            process_match = self._find_process_before_position(working_content, adjusted_rpv_position)
                            
                            if process_match:
                                logger.info("‚úÖ Processo encontrado ap√≥s merge de p√°ginas")
                                rpv_position = adjusted_rpv_position  # Usar posi√ß√£o ajustada
                            else:
                                logger.warning("‚ö†Ô∏è Processo ainda n√£o encontrado mesmo ap√≥s merge")
                                return None
                        else:
                            logger.warning("‚ö†Ô∏è Merge de p√°ginas falhou na valida√ß√£o")
                            return None
                    else:
                        logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel baixar p√°gina anterior")
                        return None
                        
                except Exception as e:
                    logger.error(f"‚ùå Erro ao processar p√°gina anterior: {e}")
                    return None
            else:
                logger.info("‚ÑπÔ∏è N√£o √© poss√≠vel baixar p√°gina anterior (primeira p√°gina ou page_manager n√£o dispon√≠vel)")
                return None
        
        if not process_match:
            return None
        
        process_number = process_match['process_number']
        process_start = process_match['start_position']
        
        # 2. Determinar fim da publica√ß√£o (pr√≥ximo processo ou fim do documento)
        publication_end = self._find_publication_end(working_content, process_start)
        
        # 3. Extrair conte√∫do completo da publica√ß√£o
        publication_content = working_content[process_start:publication_end]
        
        logger.info(f"üìÑ Extraindo publica√ß√£o: {process_number} ({len(publication_content)} chars)")
        if content_was_merged:
            logger.info(f"üîó Publica√ß√£o recuperada atrav√©s de merge de p√°ginas")
        
        # 4. Extrair dados estruturados
        publication_data = self._extract_structured_data(publication_content, process_number)
        
        if not publication_data:
            logger.warning(f"‚ö†Ô∏è Falha ao extrair dados estruturados para {process_number}")
            return None
        
        # 5. Criar entidade Publication
        try:
            publication = Publication(
                process_number=process_number,
                authors=publication_data['authors'],
                content=publication_content,
                publication_date=publication_data.get('publication_date'),
                availability_date=publication_data.get('availability_date') or datetime.now(),
                lawyers=publication_data.get('lawyers', []),
                gross_value=publication_data.get('gross_value'),
                net_value=publication_data.get('net_value'),
                interest_value=publication_data.get('interest_value'),
                attorney_fees=publication_data.get('attorney_fees'),
                extraction_metadata={
                    'extraction_method': 'enhanced_parser_v2',
                    'source_url': source_url,
                    'rpv_term_found': rpv_match['term'],
                    'extraction_date': datetime.now().isoformat(),
                    'confidence_score': publication_data.get('confidence_score', 0.8),
                    'content_was_merged': content_was_merged,
                    'current_page_number': current_page_number
                }
            )
            
            return publication
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar entidade Publication para {process_number}: {e}")
            return None

    def _find_process_before_position(self, content: str, position: int) -> Optional[Dict[str, Any]]:
        """
        Busca o √∫ltimo processo antes de uma posi√ß√£o espec√≠fica
        Retorna o processo mais pr√≥ximo para tr√°s
        """
        # Buscar todos os processos no conte√∫do at√© a posi√ß√£o especificada
        search_content = content[:position]
        
        process_matches = []
        for match in self.PROCESS_NUMBER_PATTERN.finditer(search_content):
            process_matches.append({
                'process_number': match.group(1),
                'start_position': match.start(),
                'match': match
            })
        
        if not process_matches:
            return None
        
        # Retornar o √∫ltimo processo encontrado (mais pr√≥ximo da posi√ß√£o)
        return process_matches[-1]

    def _find_publication_end(self, content: str, start_position: int) -> int:
        """
        Encontra o fim da publica√ß√£o (pr√≥ximo processo ou fim do documento)
        """
        # Buscar pr√≥ximo processo ap√≥s a posi√ß√£o inicial
        search_content = content[start_position + 1:]  # +1 para n√£o encontrar o mesmo processo
        
        next_process_match = self.PROCESS_NUMBER_PATTERN.search(search_content)
        
        if next_process_match:
            return start_position + 1 + next_process_match.start()
        else:
            # Se n√£o encontrar pr√≥ximo processo, vai at√© o fim do documento
            return len(content)

    def _extract_structured_data(self, content: str, process_number: str) -> Optional[Dict[str, Any]]:
        """
        Extrai dados estruturados da publica√ß√£o seguindo padr√µes espec√≠ficos
        """
        data = {
            'process_number': process_number,
            'authors': [],
            'lawyers': [],
            'confidence_score': 0.0
        }
        
        confidence_score = 0.0
        
        # 1. Extrair autores no formato "- NOME - Vistos"
        authors = self._extract_authors_enhanced(content)
        if authors:
            data['authors'] = authors
            confidence_score += 0.3
        else:
            logger.warning(f"‚ö†Ô∏è Autores n√£o encontrados para {process_number}")
            return None
        
        # 2. Extrair advogados
        lawyers = self._extract_lawyers_enhanced(content)
        data['lawyers'] = lawyers
        if lawyers:
            confidence_score += 0.15
        
        # 3. Extrair valores monet√°rios
        monetary_values = self._extract_monetary_values_enhanced(content)
        data.update(monetary_values)
        if any(monetary_values.values()):
            confidence_score += 0.25
        
        # 4. Extrair datas (simplificado por ora)
        dates = self._extract_dates_enhanced(content)
        data.update(dates)
        if dates.get('availability_date'):
            confidence_score += 0.1
        
        # 5. Verificar se cont√©m termos RPV
        if any(pattern.search(content) for pattern in self.RPV_PATTERNS):
            confidence_score += 0.2
        
        data['confidence_score'] = confidence_score
        
        # S√≥ retornar se tiver confian√ßa m√≠nima
        if confidence_score >= self.confidence_threshold:
            return data
        else:
            logger.warning(f"‚ö†Ô∏è Confian√ßa muito baixa ({confidence_score:.2f}) para {process_number}")
            return None

    def _extract_authors_enhanced(self, content: str) -> List[str]:
        """
        Extrai autores no formato espec√≠fico "- NOME - Vistos"
        """
        authors = []
        
        for match in self.AUTHOR_PATTERN.finditer(content):
            author_name = match.group(1).strip()
            cleaned_name = self._clean_author_name(author_name)
            
            if cleaned_name and len(cleaned_name) > 3:
                authors.append(cleaned_name)
        
        # Remove duplicatas mantendo ordem
        seen = set()
        unique_authors = []
        for author in authors:
            if author not in seen:
                seen.add(author)
                unique_authors.append(author)
        
        return unique_authors[:10]  # M√°ximo 10 autores

    def _extract_lawyers_enhanced(self, content: str) -> List[Lawyer]:
        """
        Extrai advogados com padr√µes aprimorados
        """
        lawyers = []
        seen_oabs = set()
        
        for pattern in self.LAWYER_PATTERNS:
            for match in pattern.finditer(content):
                try:
                    name = match.group(1).strip()
                    oab = match.group(2) if len(match.groups()) >= 2 else "N√£o informado"
                    
                    cleaned_name = self._clean_lawyer_name(name)
                    
                    if cleaned_name and len(cleaned_name) > 3 and oab not in seen_oabs:
                        lawyers.append(Lawyer(name=cleaned_name, oab=oab))
                        seen_oabs.add(oab)
                        
                        if len(lawyers) >= 5:  # M√°ximo 5 advogados
                            break
                            
                except Exception as e:
                    logger.debug(f"Erro ao extrair advogado: {e}")
                    continue
        
        return lawyers

    def _extract_monetary_values_enhanced(self, content: str) -> Dict[str, Optional[MonetaryValue]]:
        """
        Extrai valores monet√°rios com padr√µes espec√≠ficos para RPV
        """
        values = {}
        
        for value_type, patterns in self.MONETARY_PATTERNS.items():
            for pattern in patterns:
                match = pattern.search(content)
                if match:
                    try:
                        value_str = match.group(1)
                        decimal_value = self._parse_monetary_string(value_str)
                        
                        if decimal_value and decimal_value > 0:
                            values[value_type] = MonetaryValue.from_real(decimal_value)
                            break
                    except (ValueError, InvalidOperation):
                        continue
            
            if value_type not in values:
                values[value_type] = None
        
        return values

    def _extract_dates_enhanced(self, content: str) -> Dict[str, Optional[datetime]]:
        """
        Extrai datas da publica√ß√£o (simplificado)
        """
        # Por ora, usar data atual como availability_date
        # Pode ser aprimorado com padr√µes espec√≠ficos do DJE
        return {
            'publication_date': None,
            'availability_date': datetime.now()
        }

    def _normalize_text(self, text: str) -> str:
        """Normaliza texto mantendo estrutura para parsing"""
        # Remover caracteres de controle mas manter quebras de linha importantes
        normalized = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        
        # Normalizar espa√ßos mas manter quebras de linha
        normalized = re.sub(r'[ \t]+', ' ', normalized)
        normalized = re.sub(r'\n\s*\n', '\n\n', normalized)
        
        return normalized.strip()

    def _clean_author_name(self, name: str) -> str:
        """Limpa nome do autor removendo prefixos e sufixos desnecess√°rios"""
        # Remover prefixos
        name = re.sub(r'^(sr\.?|sra\.?|dr\.?|dra\.?)\s*', '', name, flags=re.IGNORECASE)
        
        # Remover sufixos com documentos
        name = re.sub(r'\s*(cpf|rg|cnh)[:.\s]*\d+.*$', '', name, flags=re.IGNORECASE)
        
        # Limpar caracteres especiais preservando acentos
        name = re.sub(r'[^\w\s√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á√°√©√≠√≥√∫√†√¢√™√¥√£√µ√ß]', '', name)
        name = re.sub(r'\s+', ' ', name)
        
        return name.strip().title()

    def _clean_lawyer_name(self, name: str) -> str:
        """Limpa nome do advogado"""
        # Remover prefixos profissionais
        name = re.sub(r'^(dr\.?|dra\.?|advogad[oa]|adv\.?)\s*', '', name, flags=re.IGNORECASE)
        
        # Remover sufixos
        name = re.sub(r'\s*(oab|advogad[oa]).*$', '', name, flags=re.IGNORECASE)
        
        # Limitar tamanho
        if len(name) > 60:
            words = name.split()
            name = ' '.join(words[:4]) if len(words) > 4 else name[:60]
        
        # Limpar caracteres especiais
        name = re.sub(r'[^\w\s√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á√°√©√≠√≥√∫√†√¢√™√¥√£√µ√ß]', '', name)
        name = re.sub(r'\s+', ' ', name)
        
        return name.strip().title()

    def _parse_monetary_string(self, value_str: str) -> Optional[Decimal]:
        """Converte string monet√°ria brasileira para Decimal"""
        cleaned = re.sub(r'[^\d.,]', '', value_str)
        
        if not cleaned:
            return None
        
        # Tratar diferentes formatos brasileiros
        if ',' in cleaned and '.' in cleaned:
            if cleaned.rfind(',') > cleaned.rfind('.'):
                cleaned = cleaned.replace('.', '').replace(',', '.')
            else:
                cleaned = cleaned.replace(',', '')
        elif ',' in cleaned:
            parts = cleaned.split(',')
            if len(parts) == 2 and len(parts[1]) <= 2:
                cleaned = cleaned.replace(',', '.')
            else:
                cleaned = cleaned.replace(',', '')
        
        try:
            return Decimal(cleaned)
        except InvalidOperation:
            return None
