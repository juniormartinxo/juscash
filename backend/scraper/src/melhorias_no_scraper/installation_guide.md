# üöÄ Guia de Instala√ß√£o e Configura√ß√£o - Sistema DJE-SP

## üìã Pr√©-requisitos

### Requisitos do Sistema
- **Python 3.11+** (testado em 3.11 e 3.12)
- **Node.js 18+** (para a API)
- **PostgreSQL 13+**
- **Redis 6+** (para cache e filas)
- **Docker e Docker Compose** (recomendado)

### Depend√™ncias Python Obrigat√≥rias
```bash
pip install playwright loguru httpx pydantic pydantic-settings
```

### Depend√™ncias Python Opcionais (para PDFs)
```bash
# Escolha uma das op√ß√µes:
pip install PyPDF2         # Op√ß√£o 1: PyPDF2
pip install pdfplumber     # Op√ß√£o 2: pdfplumber (recomendado)
```

## üîß Instala√ß√£o Passo a Passo

### 1. Clone e Configure o Reposit√≥rio

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/juscash-dje-system.git
cd juscash-dje-system

# Copie o arquivo de environment
cp .env.example .env

# Edite as configura√ß√µes
nano .env
```

### 2. Configure o Arquivo .env

```bash
# ===========================================
# CONFIGURA√á√ïES DO SCRAPER
# ===========================================
SCRAPER_INTERVAL=86400
SCRAPER_TIMEOUT=30
SCRAPER_MAX_RETRIES=3
SCRAPER_RETRY_DELAY=5

# ===========================================
# CONFIGURA√á√ïES DO BROWSER
# ===========================================
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=30000
BROWSER_USER_AGENT='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# ===========================================
# CONFIGURA√á√ïES DE LOG
# ===========================================
LOG_LEVEL=INFO
LOG_DIR=./logs
LOG_ROTATION_DAYS=7
LOG_MAX_SIZE_MB=10

# ===========================================
# CONFIGURA√á√ïES DA API (se usar integra√ß√£o)
# ===========================================
API_BASE_URL=http://localhost:8000
SCRAPER_API_KEY=sua_api_key_aqui
```

### 3. Instalar Depend√™ncias do Playwright

```bash
# Instalar browsers do Playwright
python -m playwright install chromium --with-deps

# Para ambiente Docker/produ√ß√£o
python -m playwright install --deps chromium
```

### 4. Criar Estrutura de Diret√≥rios

```bash
mkdir -p logs/debug_images
mkdir -p logs/test_reports  
mkdir -p data/json_reports
mkdir -p data/temp_pdfs
mkdir -p src/scrap_workrs
```

## üê≥ Instala√ß√£o com Docker (Recomendada)

### 1. Build e Execu√ß√£o

```bash
# Build de todos os servi√ßos
docker-compose build

# Executar apenas o scraper
docker-compose up scraper

# Executar sistema completo (scraper + API + frontend)
docker-compose up
```

### 2. Verificar Status dos Containers

```bash
# Verificar containers ativos
docker-compose ps

# Ver logs do scraper
docker-compose logs -f scraper

# Acessar container do scraper
docker-compose exec scraper bash
```

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### 1. Configura√ß√£o de Performance

```python
# Em infrastructure/config/settings.py

class BrowserSettings:
    headless: bool = True
    timeout: int = 30000  # 30 segundos
    user_agent: str = "Mozilla/5.0..."
    
    # Configura√ß√µes de performance
    disable_images: bool = True  # Desabilitar imagens para velocidade
    disable_javascript: bool = False  # Manter JS habilitado
    concurrent_pages: int = 1  # P√°ginas simult√¢neas (usar com cuidado)

class ScrapingSettings:
    max_retries: int = 3
    retry_delay: int = 5
    timeout: int = 30
    
    # Rate limiting
    min_request_interval: float = 2.0  # 2 segundos entre requisi√ß√µes
    max_concurrent_downloads: int = 1  # Downloads simult√¢neos
```

### 2. Configura√ß√£o de Logging Detalhado

```python
# Em infrastructure/logging/logger.py

def setup_logger_advanced(name: str, level: str = "INFO"):
    """Configura√ß√£o avan√ßada de logging"""
    
    # Remover handler padr√£o
    logger.remove()
    
    # Handler para console com cores
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
               "<level>{level: <8}</level> | "
               "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
               "<level>{message}</level>",
        level=level,
        colorize=True,
        backtrace=True,
        diagnose=True,
    )
    
    # Logs por m√≥dulo
    logger.add(
        "logs/scraper_{time:YYYY-MM-DD}.log",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
        level="DEBUG",
        rotation="1 day",
        retention="7 days",
        compression="zip",
        filter=lambda record: "scraper" in record["name"]
    )
    
    # Logs de erro separados
    logger.add(
        "logs/errors_{time:YYYY-MM-DD}.log",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
        level="ERROR",
        rotation="1 day",
        retention="30 days",
        compression="zip",
    )
    
    return logger
```

## üß™ Testes e Verifica√ß√£o

### 1. Teste B√°sico de Funcionamento

```bash
# Executar teste completo
python test_system.py

# Teste apenas do parser
python -c "
from infrastructure.web.enhanced_content_parser import EnhancedDJEContentParser
parser = EnhancedDJEContentParser()
print('‚úÖ Parser carregado com sucesso')
"

# Teste do browser
python -c "
import asyncio
from playwright.async_api import async_playwright

async def test():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto('https://www.google.com')
        title = await page.title()
        print(f'‚úÖ Browser funcionando: {title}')
        await browser.close()

asyncio.run(test())
"
```

### 2. Teste do Multi-Date Scraper

```bash
# Teste com per√≠odo espec√≠fico
python multi_date_scraper.py

# Ou modificar as datas no arquivo:
START_DATE = "17/03/2025"
END_DATE = "20/03/2025"  # Teste com poucos dias
NUM_WORKERS = 1
```

### 3. Verificar Arquivos Gerados

```bash
# Verificar JSONs gerados
ls -la data/json_reports/

# Verificar logs
tail -f logs/scraper_$(date +%Y-%m-%d).log

# Verificar progresso do multi-date
cat src/scrap_workrs.json | jq '.metadata'
```

## üîç Troubleshooting

### Problema: Browser n√£o inicializa

```bash
# Reinstalar browsers
python -m playwright uninstall
python -m playwright install chromium --with-deps

# Verificar depend√™ncias do sistema (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install -y \
    libnss3 \
    libatk-bridge2.0-0 \
    libdrm2 \
    libxkbcommon0 \
    libgtk-3-0 \
    libgbm1
```

### Problema: Timeout nos downloads

```python
# Aumentar timeouts em settings.py
class BrowserSettings:
    timeout: int = 60000  # 60 segundos
    navigation_timeout: int = 60000
    
class ScrapingSettings:
    pdf_download_timeout: int = 120  # 2 minutos
```

### Problema: Muitos PDFs falhando

```python
# Reduzir concorr√™ncia e aumentar delays
class ScrapingSettings:
    max_concurrent_downloads: int = 1
    min_request_interval: float = 3.0  # 3 segundos
    max_retries: int = 5
```

### Problema: Parser n√£o encontra publica√ß√µes

```python
# Verificar se os padr√µes est√£o corretos
# Em enhanced_content_parser.py, adicionar logs debug:

def _find_all_rpv_occurrences(self, content: str) -> List[Dict[str, Any]]:
    logger.debug(f"üîç Buscando RPV em conte√∫do de {len(content)} chars")
    
    # Mostrar trecho do conte√∫do para debug
    logger.debug(f"üìÑ Primeiros 200 chars: {content[:200]}")
    
    occurrences = []
    for pattern in self.RPV_PATTERNS:
        matches = list(pattern.finditer(content))
        logger.debug(f"üéØ Padr√£o '{pattern.pattern}': {len(matches)} matches")
        # ... resto do c√≥digo
```

## üìä Monitoramento e M√©tricas

### 1. Dashboard de Status

```python
# Em monitoring/dashboard.py

class ScrapingDashboard:
    def get_daily_stats(self):
        """Retorna estat√≠sticas do dia"""
        return {
            "publications_found": self._count_todays_publications(),
            "successful_downloads": self._count_successful_downloads(),
            "failed_downloads": len(self.scraper.failed_pdfs),
            "processing_time": self._calculate_avg_processing_time(),
            "cache_hit_rate": self._calculate_cache_hit_rate()
        }
```

### 2. Alertas Autom√°ticos

```python
# Em monitoring/alerts.py

class AlertManager:
    def check_scraping_health(self):
        """Verifica sa√∫de do scraping"""
        issues = []
        
        # Verificar se h√° publica√ß√µes sendo encontradas
        if self._no_publications_found_today():
            issues.append("‚ö†Ô∏è Nenhuma publica√ß√£o encontrada hoje")
        
        # Verificar rate de falhas
        failure_rate = self._calculate_failure_rate()
        if failure_rate > 0.5:  # 50% de falhas
            issues.append(f"‚ùå Alta taxa de falhas: {failure_rate:.1%}")
        
        return issues
```

## üöÄ Deploy em Produ√ß√£o

### 1. Configura√ß√µes de Produ√ß√£o

```bash
# .env.production
WORK_MODE=production
BROWSER_HEADLESS=true
LOG_LEVEL=WARNING
ENABLE_METRICS=true

# Configura√ß√µes de seguran√ßa
SCRAPER_API_KEY=sua_chave_segura_aqui
DATABASE_URL=postgresql://user:pass@prod-db:5432/juscash_db
```

### 2. Docker Compose para Produ√ß√£o

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  scraper:
    build:
      context: ./backend/scraper
      dockerfile: .docker/Dockerfile
      target: production
    environment:
      - PYTHONPATH=/app
      - LOG_LEVEL=WARNING
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
```

### 3. Cron Jobs

```bash
# Adicionar ao crontab para execu√ß√£o di√°ria
0 8 * * * cd /path/to/project && docker-compose -f docker-compose.prod.yml run scraper python multi_date_scraper.py

# Limpeza de logs antigos
0 2 * * 0 find /path/to/logs -name "*.log" -mtime +30 -delete
```

## üìù Pr√≥ximos Passos

1. **Execute o teste completo** para verificar se tudo est√° funcionando
2. **Configure o cron job** para execu√ß√£o autom√°tica
3. **Monitore os logs** nas primeiras execu√ß√µes
4. **Ajuste os timeouts** conforme necess√°rio
5. **Implemente alertas** para monitoramento em produ√ß√£o

## üìû Suporte

- **Logs detalhados**: `logs/scraper_YYYY-MM-DD.log`
- **Relat√≥rios de teste**: `logs/test_reports/`
- **Debug images**: `logs/debug_images/` (capturas de tela em caso de erro)
- **Arquivos JSON**: `data/json_reports/` (publica√ß√µes extra√≠das)
