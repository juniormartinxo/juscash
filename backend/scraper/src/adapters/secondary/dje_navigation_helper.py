"""
Arquivo: src/adapters/secondary/dje_navigation_helper.py

Helper principal para navega√ß√£o e orchestra√ß√£o de opera√ß√µes no DJE.
Segue princ√≠pios da Arquitetura Hexagonal como componente de infraestrutura.

Responsabilidades:
- Orchestra√ß√£o do fluxo completo de scraping
- Coordena√ß√£o entre search handler e content parser
- Gerenciamento de navega√ß√£o entre p√°ginas
- Implementa√ß√£o de estrat√©gias de retry e error handling
"""

from typing import List, Optional, Dict, Any, Iterator
from datetime import datetime
from playwright.async_api import Page

from src.core.entities.publication import Publication
from src.shared.value_objects import ScrapingCriteria, DJEUrl
from src.shared.exceptions import (
    NavigationException, ParsingException, BrowserException, TimeoutException
)
from src.shared.logger import get_logger

from .dje_search_handler import DJEAdvancedSearchHandler
from .dje_content_parser import DJEContentParser

logger = get_logger(__name__)


class DJENavigationHelper:
    """
    Helper principal para navega√ß√£o no DJE.
    
    Responsabilidade: Agregar e coordenar os diferentes handlers especializados
    para fornecer uma interface unificada de scraping do DJE.
    
    Princ√≠pios da Arquitetura Hexagonal:
    - Componente de infraestrutura (adapter secund√°rio)
    - Orchestra√ß√£o de opera√ß√µes complexas
    - Interface simplificada para o adapter principal
    - Encapsulamento da complexidade espec√≠fica do DJE
    """
    
    def __init__(self, page: Page):
        """
        Inicializa o helper de navega√ß√£o.
        
        Args:
            page: Inst√¢ncia da p√°gina Playwright
        """
        self.page = page
        self.search_handler = DJEAdvancedSearchHandler(page)
        self.content_parser = DJEContentParser(page)
        
        # Configura√ß√µes do helper
        self.config = {
            'max_retries': 3,
            'retry_delay': 2000,  # ms
            'max_pages_per_session': 10,
            'max_publications_per_page': 50,
            'enable_pagination': True,
            'enable_detail_extraction': False  # Para otimiza√ß√£o de performance
        }
        
        # Estado do scraping
        self.session_stats = {
            'pages_processed': 0,
            'publications_found': 0,
            'publications_extracted': 0,
            'errors_count': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def execute_full_scraping_flow(self, 
                                       criteria: ScrapingCriteria,
                                       max_publications: Optional[int] = None) -> List[Publication]:
        """
        Executa o fluxo completo de scraping do DJE.
        
        Fluxo:
        1. Executa pesquisa avan√ßada
        2. Extrai publica√ß√µes da primeira p√°gina
        3. Navega por p√°ginas adicionais (se habilitado)
        4. Consolida resultados
        
        Args:
            criteria: Crit√©rios de scraping
            max_publications: Limite m√°ximo de publica√ß√µes
            
        Returns:
            List[Publication]: Publica√ß√µes extra√≠das e validadas
            
        Raises:
            NavigationException: Se erro na navega√ß√£o
            ParsingException: Se erro na extra√ß√£o
        """
        logger.info("üöÄ Iniciando fluxo completo de scraping do DJE...")
        
        # Inicializar estat√≠sticas
        self.session_stats['start_time'] = datetime.now()
        self.session_stats['pages_processed'] = 0
        self.session_stats['publications_found'] = 0
        self.session_stats['publications_extracted'] = 0
        self.session_stats['errors_count'] = 0
        
        all_publications: List[Publication] = []
        
        try:
            # === FASE 1: EXECUTAR PESQUISA ===
            logger.info("üìç Fase 1: Executando pesquisa avan√ßada...")
            search_success = await self._execute_search_with_retry(criteria)
            
            if not search_success:
                raise NavigationException("Falha na pesquisa avan√ßada ap√≥s m√∫ltiplas tentativas")
            
            # === FASE 2: PROCESSAR P√ÅGINAS DE RESULTADOS ===
            logger.info("üìç Fase 2: Processando p√°ginas de resultados...")
            
            page_count = 0
            has_more_pages = True
            
            while (has_more_pages and 
                   page_count < self.config['max_pages_per_session'] and
                   (not max_publications or len(all_publications) < max_publications)):
                
                page_count += 1
                logger.info(f"üìÑ Processando p√°gina {page_count}...")
                
                # Extrair publica√ß√µes da p√°gina atual
                page_publications = await self._extract_publications_from_current_page(
                    criteria, 
                    max_publications - len(all_publications) if max_publications else None
                )
                
                if page_publications:
                    all_publications.extend(page_publications)
                    self.session_stats['publications_extracted'] += len(page_publications)
                    logger.info(f"‚úÖ {len(page_publications)} publica√ß√µes extra√≠das da p√°gina {page_count}")
                else:
                    logger.warning(f"‚ö†Ô∏è Nenhuma publica√ß√£o extra√≠da da p√°gina {page_count}")
                
                self.session_stats['pages_processed'] += 1
                
                # Verificar se deve continuar para pr√≥xima p√°gina
                if self.config['enable_pagination']:
                    has_more_pages = await self._navigate_to_next_page_safely()
                else:
                    has_more_pages = False
                
                # Verificar limite de publica√ß√µes
                if max_publications and len(all_publications) >= max_publications:
                    logger.info(f"Limite de {max_publications} publica√ß√µes atingido")
                    break
            
            # === FASE 3: FINALIZA√á√ÉO ===
            self._finalize_session_stats()
            
            logger.info(f"üéâ Scraping conclu√≠do:")
            logger.info(f"  üìä P√°ginas processadas: {self.session_stats['pages_processed']}")
            logger.info(f"  üìã Publica√ß√µes extra√≠das: {len(all_publications)}")
            logger.info(f"  ‚è±Ô∏è Tempo total: {self._get_session_duration()}")
            
            return all_publications
            
        except Exception as e:
            self.session_stats['errors_count'] += 1
            self._finalize_session_stats()
            
            logger.error(f"‚ùå Erro no fluxo de scraping: {str(e)}")
            raise
    
    async def _execute_search_with_retry(self, criteria: ScrapingCriteria) -> bool:
        """
        Executa pesquisa com retry autom√°tico.
        
        Args:
            criteria: Crit√©rios de scraping
            
        Returns:
            bool: True se pesquisa foi bem-sucedida
        """
        for attempt in range(self.config['max_retries']):
            try:
                logger.info(f"üîç Tentativa de pesquisa {attempt + 1}/{self.config['max_retries']}")
                
                success = await self.search_handler.execute_project_search()
                
                if success:
                    logger.info("‚úÖ Pesquisa executada com sucesso")
                    return True
                else:
                    logger.warning(f"‚ö†Ô∏è Pesquisa falhou na tentativa {attempt + 1}")
                    
            except Exception as e:
                logger.error(f"‚ùå Erro na pesquisa (tentativa {attempt + 1}): {str(e)}")
                self.session_stats['errors_count'] += 1
            
            # Aguardar antes da pr√≥xima tentativa (exceto na √∫ltima)
            if attempt < self.config['max_retries'] - 1:
                logger.info(f"‚è≥ Aguardando {self.config['retry_delay']}ms antes da pr√≥xima tentativa...")
                await self.page.wait_for_timeout(self.config['retry_delay'])
        
        return False
    
    async def _extract_publications_from_current_page(self, 
                                                    criteria: ScrapingCriteria,
                                                    max_publications: Optional[int] = None) -> List[Publication]:
        """
        Extrai publica√ß√µes da p√°gina atual com tratamento de erro.
        
        Args:
            criteria: Crit√©rios de scraping
            max_publications: Limite para esta p√°gina
            
        Returns:
            List[Publication]: Publica√ß√µes extra√≠das
        """
        try:
            # Aguardar p√°gina carregar
            await self.page.wait_for_load_state('domcontentloaded')
            
            # Extrair publica√ß√µes usando o content parser
            publications = await self.content_parser.extract_publications_from_results(
                criteria, 
                max_publications or self.config['max_publications_per_page']
            )
            
            return publications
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair publica√ß√µes da p√°gina atual: {str(e)}")
            self.session_stats['errors_count'] += 1
            return []
    
    async def _navigate_to_next_page_safely(self) -> bool:
        """
        Navega para pr√≥xima p√°gina com tratamento de erro.
        
        Returns:
            bool: True se navega√ß√£o foi bem-sucedida
        """
        try:
            # Verificar se h√° pr√≥xima p√°gina
            pagination_info = await self.content_parser.check_pagination()
            
            if not pagination_info.get('has_next_page'):
                logger.info("üìÑ N√£o h√° mais p√°ginas para processar")
                return False
            
            # Navegar para pr√≥xima p√°gina
            success = await self.content_parser.navigate_to_next_page()
            
            if success:
                logger.info("‚û°Ô∏è Navega√ß√£o para pr√≥xima p√°gina realizada")
                # Aguardar carregamento
                await self.page.wait_for_load_state('networkidle', timeout=30000)
                return True
            else:
                logger.warning("‚ö†Ô∏è Falha na navega√ß√£o para pr√≥xima p√°gina")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao navegar para pr√≥xima p√°gina: {str(e)}")
            self.session_stats['errors_count'] += 1
            return False
    
    async def execute_targeted_publication_extraction(self, 
                                                    criteria: ScrapingCriteria,
                                                    publication_links: List[str]) -> List[Publication]:
        """
        Extrai publica√ß√µes espec√≠ficas a partir de lista de links.
        
        Args:
            criteria: Crit√©rios de valida√ß√£o
            publication_links: URLs das publica√ß√µes espec√≠ficas
            
        Returns:
            List[Publication]: Publica√ß√µes extra√≠das
        """
        logger.info(f"üéØ Extraindo {len(publication_links)} publica√ß√µes espec√≠ficas...")
        
        publications = []
        
        for i, link in enumerate(publication_links):
            try:
                logger.info(f"üìÑ Processando publica√ß√£o {i+1}/{len(publication_links)}")
                
                # Navegar para p√°gina espec√≠fica
                success = await self.content_parser.navigate_to_publication_detail(link)
                
                if not success:
                    logger.warning(f"‚ö†Ô∏è Falha ao navegar para {link}")
                    continue
                
                # Extrair publica√ß√£o detalhada
                publication = await self.content_parser.extract_detailed_publication(criteria)
                
                if publication:
                    publications.append(publication)
                    logger.info(f"‚úÖ Publica√ß√£o extra√≠da: {publication.process_number}")
                else:
                    logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel extrair publica√ß√£o de {link}")
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao processar {link}: {str(e)}")
                self.session_stats['errors_count'] += 1
                continue
        
        logger.info(f"üéâ Extra√ß√£o direcionada conclu√≠da: {len(publications)} publica√ß√µes")
        return publications
    
    async def get_search_results_summary(self) -> Dict[str, Any]:
        """
        Obt√©m resumo dos resultados da pesquisa atual.
        
        Returns:
            Dict com resumo dos resultados
        """
        try:
            # Obter metadados dos resultados
            result_metadata = await self.content_parser._extract_result_metadata()
            
            # Obter informa√ß√µes de pagina√ß√£o
            pagination_info = await self.content_parser.check_pagination()
            
            # Obter links de publica√ß√µes
            publication_links = await self.content_parser.extract_publication_links()
            
            summary = {
                'result_metadata': result_metadata,
                'pagination_info': pagination_info,
                'publication_links_count': len(publication_links),
                'publication_links': publication_links[:10],  # Primeiros 10 para preview
                'session_stats': self.session_stats.copy(),
                'timestamp': datetime.now().isoformat()
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Erro ao obter resumo: {e}")
            return {'error': str(e)}
    
    async def execute_search_only(self, criteria: ScrapingCriteria) -> bool:
        """
        Executa apenas a pesquisa, sem extra√ß√£o de conte√∫do.
        
        √ötil para valida√ß√£o de par√¢metros ou contagem de resultados.
        
        Args:
            criteria: Crit√©rios de scraping
            
        Returns:
            bool: True se pesquisa foi bem-sucedida
        """
        logger.info("üîç Executando apenas pesquisa (sem extra√ß√£o)...")
        
        try:
            success = await self._execute_search_with_retry(criteria)
            
            if success:
                # Obter resumo dos resultados
                summary = await self.get_search_results_summary()
                logger.info("‚úÖ Pesquisa executada - Resumo dispon√≠vel")
                logger.info(f"üìä Resultados encontrados: {summary.get('result_metadata', {}).get('total_results', 'N/A')}")
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Erro na execu√ß√£o da pesquisa: {str(e)}")
            return False
    
    def configure_scraping_session(self, config_updates: Dict[str, Any]) -> None:
        """
        Configura par√¢metros da sess√£o de scraping.
        
        Args:
            config_updates: Dicion√°rio com configura√ß√µes a atualizar
        """
        logger.info("‚öôÔ∏è Configurando sess√£o de scraping...")
        
        valid_configs = {
            'max_retries', 'retry_delay', 'max_pages_per_session',
            'max_publications_per_page', 'enable_pagination', 'enable_detail_extraction'
        }
        
        for key, value in config_updates.items():
            if key in valid_configs:
                old_value = self.config.get(key)
                self.config[key] = value
                logger.info(f"  {key}: {old_value} ‚Üí {value}")
            else:
                logger.warning(f"‚ö†Ô∏è Configura√ß√£o inv√°lida ignorada: {key}")
        
        logger.info("‚úÖ Configura√ß√£o da sess√£o atualizada")
    
    def get_session_statistics(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas da sess√£o atual.
        
        Returns:
            Dict com estat√≠sticas detalhadas
        """
        stats = self.session_stats.copy()
        
        # Calcular m√©tricas adicionais
        if stats['start_time']:
            duration = self._get_session_duration()
            stats['duration_seconds'] = duration.total_seconds()
            stats['duration_formatted'] = str(duration)
            
            # Taxa de sucesso
            if stats['pages_processed'] > 0:
                stats['success_rate'] = (stats['pages_processed'] - stats['errors_count']) / stats['pages_processed']
                stats['publications_per_page'] = stats['publications_extracted'] / stats['pages_processed']
            else:
                stats['success_rate'] = 0.0
                stats['publications_per_page'] = 0.0
        
        return stats
    
    def reset_session_statistics(self) -> None:
        """Reset das estat√≠sticas da sess√£o."""
        logger.info("üîÑ Resetando estat√≠sticas da sess√£o...")
        
        self.session_stats = {
            'pages_processed': 0,
            'publications_found': 0,
            'publications_extracted': 0,
            'errors_count': 0,
            'start_time': None,
            'end_time': None
        }
        
        logger.info("‚úÖ Estat√≠sticas resetadas")
    
    def _finalize_session_stats(self) -> None:
        """Finaliza estat√≠sticas da sess√£o."""
        self.session_stats['end_time'] = datetime.now()
    
    def _get_session_duration(self) -> datetime:
        """
        Calcula dura√ß√£o da sess√£o.
        
        Returns:
            timedelta com dura√ß√£o da sess√£o
        """
        if not self.session_stats['start_time']:
            return datetime.now() - datetime.now()  # Zero duration
        
        end_time = self.session_stats['end_time'] or datetime.now()
        return end_time - self.session_stats['start_time']
    
    async def validate_search_criteria(self, criteria: ScrapingCriteria) -> Dict[str, Any]:
        """
        Valida crit√©rios de pesquisa antes da execu√ß√£o.
        
        Args:
            criteria: Crit√©rios a serem validados
            
        Returns:
            Dict com resultado da valida√ß√£o
        """
        logger.info("üîç Validando crit√©rios de pesquisa...")
        
        validation_result = {
            'is_valid': True,
            'warnings': [],
            'errors': [],
            'suggestions': []
        }
        
        try:
            # Validar termos de busca
            search_terms = getattr(criteria, 'search_terms', [])
            if not search_terms:
                validation_result['warnings'].append("Nenhum termo de busca especificado")
                validation_result['suggestions'].append("Adicionar termos espec√≠ficos pode melhorar a precis√£o")
            
            # Validar data in√≠cio
            start_date = getattr(criteria, 'start_date', None)
            if start_date and start_date < datetime(2025, 3, 17):
                validation_result['warnings'].append("Data anterior ao in√≠cio do projeto (17/03/2025)")
            
            # Validar se tem crit√©rios m√≠nimos para RPV + INSS
            content_check = criteria.get_search_string() if hasattr(criteria, 'get_search_string') else ""
            if 'rpv' not in content_check.lower() or 'inss' not in content_check.lower():
                validation_result['warnings'].append("Crit√©rios podem n√£o incluir termos obrigat√≥rios do projeto")
                validation_result['suggestions'].append("Verificar se inclui 'RPV' e 'INSS'")
            
            logger.info(f"‚úÖ Valida√ß√£o conclu√≠da - V√°lido: {validation_result['is_valid']}")
            if validation_result['warnings']:
                logger.warning(f"‚ö†Ô∏è {len(validation_result['warnings'])} avisos encontrados")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o: {str(e)}")
            validation_result['is_valid'] = False
            validation_result['errors'].append(f"Erro durante valida√ß√£o: {str(e)}")
            return validation_result
    
    async def execute_health_check(self) -> Dict[str, Any]:
        """
        Executa verifica√ß√£o de sa√∫de dos componentes.
        
        Returns:
            Dict com status dos componentes
        """
        logger.info("üè• Executando health check dos componentes...")
        
        health_status = {
            'overall_status': 'healthy',
            'components': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # Check da p√°gina
        try:
            page_url = self.page.url
            page_title = await self.page.title()
            
            health_status['components']['page'] = {
                'status': 'healthy',
                'url': page_url,
                'title': page_title
            }
        except Exception as e:
            health_status['components']['page'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
            health_status['overall_status'] = 'unhealthy'
        
        # Check do search handler
        try:
            search_metadata = await self.search_handler.get_search_metadata()
            health_status['components']['search_handler'] = {
                'status': 'healthy',
                'metadata': search_metadata
            }
        except Exception as e:
            health_status['components']['search_handler'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
            health_status['overall_status'] = 'unhealthy'
        
        # Check do content parser
        try:
            # Teste simples do parser
            test_content = "Teste b√°sico do parser"
            is_valid = self.content_parser._is_valid_publication_content(test_content)
            
            health_status['components']['content_parser'] = {
                'status': 'healthy',
                'test_result': f"Validation test: {is_valid}"
            }
        except Exception as e:
            health_status['components']['content_parser'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
            health_status['overall_status'] = 'unhealthy'
        
        logger.info(f"üè• Health check conclu√≠do - Status: {health_status['overall_status']}")
        return health_status
    
    async def cleanup_session(self) -> None:
        """
        Limpa recursos da sess√£o atual.
        """
        logger.info("üßπ Limpando recursos da sess√£o...")
        
        try:
            # Finalizar estat√≠sticas se n√£o finalizadas
            if not self.session_stats.get('end_time'):
                self._finalize_session_stats()
            
            # Log final das estat√≠sticas
            final_stats = self.get_session_statistics()
            logger.info("üìä Estat√≠sticas finais da sess√£o:")
            logger.info(f"  üìÑ P√°ginas processadas: {final_stats['pages_processed']}")
            logger.info(f"  üìã Publica√ß√µes extra√≠das: {final_stats['publications_extracted']}")
            logger.info(f"  ‚ùå Erros: {final_stats['errors_count']}")
            logger.info(f"  ‚è±Ô∏è Dura√ß√£o: {final_stats.get('duration_formatted', 'N/A')}")
            logger.info(f"  üìà Taxa de sucesso: {final_stats.get('success_rate', 0):.2%}")
            
            logger.info("‚úÖ Limpeza da sess√£o conclu√≠da")
            
        except Exception as e:
            logger.error(f"‚ùå Erro durante limpeza: {str(e)}")
    
    # === M√âTODOS DE CONVENI√äNCIA ===
    
    async def quick_search_and_count(self, criteria: ScrapingCriteria) -> int:
        """
        Executa pesquisa r√°pida e retorna apenas contagem de resultados.
        
        Args:
            criteria: Crit√©rios de pesquisa
            
        Returns:
            int: N√∫mero de resultados encontrados
        """
        try:
            success = await self.execute_search_only(criteria)
            
            if success:
                summary = await self.get_search_results_summary()
                return summary.get('result_metadata', {}).get('total_results', 0)
            
            return 0
            
        except Exception as e:
            logger.error(f"Erro na contagem r√°pida: {e}")
            return 0
    
    async def extract_sample_publications(self, 
                                        criteria: ScrapingCriteria, 
                                        sample_size: int = 5) -> List[Publication]:
        """
        Extrai uma amostra pequena de publica√ß√µes para teste.
        
        Args:
            criteria: Crit√©rios de pesquisa
            sample_size: Tamanho da amostra
            
        Returns:
            List[Publication]: Amostra de publica√ß√µes
        """
        logger.info(f"üß™ Extraindo amostra de {sample_size} publica√ß√µes...")
        
        # Temporariamente desabilitar pagina√ß√£o
        original_pagination = self.config['enable_pagination']
        self.config['enable_pagination'] = False
        
        try:
            publications = await self.execute_full_scraping_flow(criteria, sample_size)
            return publications[:sample_size]
            
        finally:
            # Restaurar configura√ß√£o original
            self.config['enable_pagination'] = original_pagination
    
    def get_configuration_summary(self) -> Dict[str, Any]:
        """
        Retorna resumo da configura√ß√£o atual.
        
        Returns:
            Dict com configura√ß√£o atual
        """
        return {
            'config': self.config.copy(),
            'session_stats': self.session_stats.copy(),
            'components': {
                'search_handler': type(self.search_handler).__name__,
                'content_parser': type(self.content_parser).__name__
            },
            'timestamp': datetime.now().isoformat()
        }