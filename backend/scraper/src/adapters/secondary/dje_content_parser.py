"""
Arquivo: src/adapters/secondary/dje_content_parser.py

Parser especializado para extra√ß√£o de conte√∫do das p√°ginas do DJE.
Segue princ√≠pios da Arquitetura Hexagonal como componente de infraestrutura.

Responsabilidades:
- Extra√ß√£o de publica√ß√µes dos resultados de pesquisa
- Parse de conte√∫do das publica√ß√µes individuais
- Normaliza√ß√£o de dados extra√≠dos
- Valida√ß√£o de crit√©rios de neg√≥cio
"""

import re
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime
from decimal import Decimal, InvalidOperation
from playwright.async_api import Page, ElementHandle

from src.core.entities.publication import Publication
from src.shared.value_objects import ScrapingCriteria, ProcessNumber, Status
from src.shared.exceptions import ParsingException
from src.shared.logger import get_logger

logger = get_logger(__name__)


class DJEContentParser:
    """
    Parser para extra√ß√£o e processamento de conte√∫do do DJE.
    
    Responsabilidade: Encapsular toda a l√≥gica de extra√ß√£o e parse
    de dados espec√≠ficos do DJE, transformando HTML em entidades de dom√≠nio.
    
    Princ√≠pios da Arquitetura Hexagonal:
    - Componente de infraestrutura (adapter secund√°rio)
    - Isolamento da l√≥gica de parsing espec√≠fica do DJE
    - Transforma√ß√£o de dados externos em entidades de dom√≠nio
    """
    
    def __init__(self, page: Page):
        """
        Inicializa o parser de conte√∫do.
        
        Args:
            page: Inst√¢ncia da p√°gina Playwright
        """
        self.page = page
        self.selectors = self._get_content_selectors()
        self.patterns = self._get_regex_patterns()
        
    def _get_content_selectors(self) -> Dict[str, str]:
        """
        Retorna seletores para diferentes tipos de conte√∫do no DJE.
        
        Returns:
            Dict com seletores CSS organizados por tipo de conte√∫do
        """
        return {
            # Resultados de pesquisa
            'search_results': 'body',
            'result_items': 'td:has-text("caderno 3")',
            'result_links': 'a:has-text("ocorr√™ncia")',
            'pagination_links': 'a:has-text("Pr√≥xima"), a:has-text("2"), a:has-text("3")',
            
            # Conte√∫do das publica√ß√µes
            'publication_containers': [
                '.publicacao',
                '.item-publicacao', 
                'div[data-publicacao]',
                '.conteudo-publicacao',
                'div:has-text("Processo")',
                'p:has-text("Processo")',
                'div:has-text("INSS")',
                'p:has-text("INSS")',
                # Seletores mais gen√©ricos
                'div',
                'p'
            ],
            
            # Elementos espec√≠ficos
            'process_number': '.numero-processo, .processo',
            'publication_date': '.data-publicacao, .data',
            'author_info': '.autor, .requerente',
            'lawyer_info': '.advogado, .oab'
        }
    
    def _get_regex_patterns(self) -> Dict[str, List[str]]:
        """
        Retorna padr√µes regex para extra√ß√£o de dados espec√≠ficos.
        
        Returns:
            Dict com padr√µes regex organizados por tipo de dado
        """
        return {
            # Padr√µes para n√∫mero de processo
            'process_number': [
                r'(\d{7}-\d{2}\.\d{4}\.\d{1}\.\d{2}\.\d{4})',  # Formato padr√£o CNJ
                r'(?:processo|autos)[:\s]*(\d+[.\-/]\d+[.\-/]\d+[.\-/]\d+[.\-/]\d+)',
                r'(\d+[.\-/]\d+[.\-/]\d+[.\-/]\d+[.\-/]\d+)',  # Formato mais gen√©rico
                r'(\d+[.\-/]\d+)'  # Formato simples
            ],
            
            # Padr√µes para datas
            'dates': [
                r'disponibiliza√ß√£o[:\s]*(\d{1,2}/\d{1,2}/\d{4})',
                r'data[:\s]*(\d{1,2}/\d{1,2}/\d{4})',
                r'(\d{1,2}/\d{1,2}/\d{4})'
            ],
            
            # Padr√µes para autores
            'authors': [
                r'autor(?:es)?[:\s]*([^,\n]+)',
                r'requerente[:\s]*([^,\n]+)',
                r'exequente[:\s]*([^,\n]+)'
            ],
            
            # Padr√µes para advogados
            'lawyers': [
                r'advogad[oa][:\s]*([^,\n]+)',
                r'oab[:\s]*(\d+[./]?\w*)',
                r'dr\.?\s+([^,\n]+)'
            ],
            
            # Padr√µes para valores monet√°rios
            'monetary_values': {
                'gross_value': [
                    r'valor\s*principal[:\s]*r?\$?\s*([\d.,]+)',
                    r'valor\s*bruto[:\s]*r?\$?\s*([\d.,]+)',
                    r'principal[:\s]*r?\$?\s*([\d.,]+)'
                ],
                'net_value': [
                    r'valor\s*l√≠quido[:\s]*r?\$?\s*([\d.,]+)',
                    r'l√≠quido[:\s]*r?\$?\s*([\d.,]+)'
                ],
                'interest_value': [
                    r'juros[:\s]*r?\$?\s*([\d.,]+)',
                    r'juros\s*morat√≥rios[:\s]*r?\$?\s*([\d.,]+)'
                ],
                'attorney_fees': [
                    r'honor√°rios[:\s]*r?\$?\s*([\d.,]+)',
                    r'honor√°rios\s*advocat√≠cios[:\s]*r?\$?\s*([\d.,]+)'
                ]
            },
            
            # Padr√µes para contagem de resultados
            'result_count': [
                r'(\d+)\s+ocorr√™ncia[s]?\s+encontrada[s]?',
                r'Resultados\s+(\d+)\s+a\s+(\d+)\s+de\s+(\d+)'
            ]
        }
    
    async def extract_publications_from_results(self, 
                                              criteria: ScrapingCriteria,
                                              max_publications: Optional[int] = None) -> List[Publication]:
        """
        Extrai publica√ß√µes dos resultados da pesquisa avan√ßada.
        
        Args:
            criteria: Crit√©rios de scraping para valida√ß√£o
            max_publications: Limite m√°ximo de publica√ß√µes a extrair
            
        Returns:
            List[Publication]: Lista de publica√ß√µes extra√≠das e validadas
            
        Raises:
            ParsingException: Se erro durante extra√ß√£o
        """
        logger.info("üìã Extraindo publica√ß√µes dos resultados da pesquisa...")
        publications: List[Publication] = []
        
        try:
            # Aguardar p√°gina carregar completamente
            await self.page.wait_for_load_state('domcontentloaded')
            
            # Extrair metadados dos resultados
            result_metadata = await self._extract_result_metadata()
            logger.info(f"Metadados: {result_metadata}")
            
            # Buscar elementos que cont√™m publica√ß√µes
            publication_elements = await self._find_publication_elements()
            
            if not publication_elements:
                logger.warning("Nenhum elemento de publica√ß√£o encontrado")
                return publications
            
            logger.info(f"Encontrados {len(publication_elements)} elementos de publica√ß√£o")
            
            # Processar cada elemento
            processed_count = 0
            for i, element in enumerate(publication_elements):
                if max_publications and processed_count >= max_publications:
                    logger.info(f"Limite de {max_publications} publica√ß√µes atingido")
                    break
                
                try:
                    # Extrair conte√∫do do elemento
                    content = await self._extract_element_content(element)
                    
                    if not self._is_valid_publication_content(content):
                        logger.debug(f"Elemento {i} n√£o cont√©m publica√ß√£o v√°lida")
                        continue
                    
                    # Verificar se atende aos crit√©rios
                    if not criteria.matches_content(content):
                        logger.debug(f"Elemento {i} n√£o atende aos crit√©rios de busca")
                        continue
                    
                    # Criar publica√ß√£o a partir do conte√∫do
                    publication = await self._create_publication_from_content(content)
                    
                    if publication:
                        publications.append(publication)
                        processed_count += 1
                        logger.info(f"Publica√ß√£o {processed_count} extra√≠da: {publication.process_number}")
                    
                except Exception as e:
                    logger.error(f"Erro ao processar elemento {i}: {str(e)}")
                    continue
            
            logger.info(f"‚úÖ Extra√ß√£o conclu√≠da: {len(publications)} publica√ß√µes v√°lidas")
            return publications
            
        except Exception as e:
            logger.error(f"‚ùå Erro durante extra√ß√£o de publica√ß√µes: {str(e)}")
            raise ParsingException("extract_publications_from_results", "p√°gina_resultados", str(e))
    
    async def _extract_result_metadata(self) -> Dict[str, Any]:
        """
        Extrai metadados dos resultados da pesquisa.
        
        Returns:
            Dict com metadados dos resultados
        """
        try:
            page_content = await self.page.content()
            
            metadata = {
                'page_url': self.page.url,
                'page_title': await self.page.title(),
                'timestamp': datetime.now().isoformat()
            }
            
            # Extrair n√∫mero total de resultados
            for pattern in self.patterns['result_count']:
                match = re.search(pattern, page_content, re.IGNORECASE)
                if match:
                    if 'ocorr√™ncia' in pattern:
                        metadata['total_results'] = int(match.group(1))
                    else:  # Padr√£o "X a Y de Z"
                        metadata['start_result'] = int(match.group(1))
                        metadata['end_result'] = int(match.group(2))
                        metadata['total_results'] = int(match.group(3))
                    break
            
            # Verificar se h√° mais p√°ginas
            has_pagination = 'Pr√≥xima' in page_content or 'pr√≥xima' in page_content
            metadata['has_pagination'] = has_pagination
            
            return metadata
            
        except Exception as e:
            logger.error(f"Erro ao extrair metadados: {e}")
            return {'error': str(e)}
    
    async def _find_publication_elements(self) -> List[ElementHandle]:
        """
        Encontra elementos que cont√™m publica√ß√µes na p√°gina.
        
        Returns:
            Lista de ElementHandle com potenciais publica√ß√µes
        """
        publication_elements = []
        
        # Tentar diferentes seletores
        for selector in self.selectors['publication_containers']:
            try:
                elements = await self.page.query_selector_all(selector)
                
                # Filtrar elementos que provavelmente cont√™m publica√ß√µes
                for element in elements:
                    try:
                        text = await element.inner_text()
                        if self._is_potential_publication_element(text):
                            publication_elements.append(element)
                    except:
                        continue
                
                if publication_elements:
                    logger.info(f"Elementos encontrados com seletor: {selector}")
                    break
                    
            except Exception as e:
                logger.debug(f"Erro com seletor {selector}: {str(e)}")
                continue
        
        # Remover duplicatas mantendo ordem
        unique_elements = self._remove_duplicate_elements(publication_elements)
        
        return unique_elements[:50]  # Limitar para performance
    
    def _is_potential_publication_element(self, text: str) -> bool:
        """
        Verifica se um elemento potencialmente cont√©m uma publica√ß√£o.
        
        Args:
            text: Texto do elemento
            
        Returns:
            bool: True se elemento pode conter publica√ß√£o
        """
        if len(text) < 100:  # Muito pequeno
            return False
        
        # Indicadores de publica√ß√£o
        indicators = [
            'processo',
            'inss',
            'instituto nacional do seguro social',
            'rpv',
            'pagamento',
            'advogado',
            'oab'
        ]
        
        text_lower = text.lower()
        return any(indicator in text_lower for indicator in indicators)
    
    def _remove_duplicate_elements(self, elements: List[ElementHandle]) -> List[ElementHandle]:
        """
        Remove elementos duplicados da lista.
        
        Args:
            elements: Lista de ElementHandle
            
        Returns:
            Lista sem duplicatas
        """
        seen = set()
        unique_elements = []
        
        for element in elements:
            element_id = id(element)
            if element_id not in seen:
                seen.add(element_id)
                unique_elements.append(element)
        
        return unique_elements
    
    async def _extract_element_content(self, element: ElementHandle) -> str:
        """
        Extrai o conte√∫do textual de um elemento.
        
        Args:
            element: ElementHandle do elemento
            
        Returns:
            String com conte√∫do do elemento
        """
        try:
            # Tentar diferentes m√©todos de extra√ß√£o
            methods = [
                lambda: element.inner_text(),
                lambda: element.text_content(),
                lambda: element.inner_html()
            ]
            
            for method in methods:
                try:
                    content = await method()
                    if content and len(content.strip()) > 10:
                        return content.strip()
                except:
                    continue
            
            return ""
            
        except Exception as e:
            logger.debug(f"Erro ao extrair conte√∫do do elemento: {str(e)}")
            return ""
    
    def _is_valid_publication_content(self, content: str) -> bool:
        """
        Valida se o conte√∫do representa uma publica√ß√£o v√°lida.
        
        Args:
            content: Conte√∫do textual
            
        Returns:
            bool: True se conte√∫do √© v√°lido
        """
        if not content or len(content.strip()) < 50:
            return False
        
        # Verificar se cont√©m indicadores m√≠nimos
        content_lower = content.lower()
        
        required_indicators = ['processo', 'inss']
        has_required = all(indicator in content_lower for indicator in required_indicators)
        
        return has_required
    
    async def _create_publication_from_content(self, content: str) -> Optional[Publication]:
        """
        Cria uma entidade Publication a partir do conte√∫do extra√≠do.
        
        Args:
            content: Conte√∫do textual da publica√ß√£o
            
        Returns:
            Publication ou None se n√£o conseguir criar
        """
        try:
            # Criar publica√ß√£o b√°sica
            publication = Publication(
                content=content,
                status=Status.NEW,
                defendant="Instituto Nacional do Seguro Social - INSS"
            )
            
            # Extrair informa√ß√µes espec√≠ficas
            await self._extract_publication_details(publication, content)
            
            return publication
            
        except Exception as e:
            logger.error(f"Erro ao criar publica√ß√£o: {str(e)}")
            return None
    
    async def _extract_publication_details(self, publication: Publication, content: str) -> None:
        """
        Extrai detalhes espec√≠ficos de uma publica√ß√£o.
        
        Args:
            publication: Entidade Publication a ser preenchida
            content: Conte√∫do textual para extra√ß√£o
        """
        try:
            # Extrair n√∫mero do processo
            self._extract_process_number(publication, content)
            
            # Extrair data de disponibiliza√ß√£o
            self._extract_availability_date(publication, content)
            
            # Extrair autores
            self._extract_authors(publication, content)
            
            # Extrair advogados
            self._extract_lawyers(publication, content)
            
            # Extrair valores monet√°rios
            self._extract_monetary_values(publication, content)
            
        except Exception as e:
            logger.debug(f"Erro ao extrair detalhes da publica√ß√£o: {str(e)}")
    
    def _extract_process_number(self, publication: Publication, content: str) -> None:
        """Extrai n√∫mero do processo."""
        for pattern in self.patterns['process_number']:
            match = re.search(pattern, content)
            if match:
                try:
                    process_number = match.group(1)
                    publication.process_number = ProcessNumber(process_number)
                    logger.debug(f"N√∫mero do processo extra√≠do: {publication.process_number}")
                    return
                except Exception as e:
                    logger.debug(f"Erro ao criar ProcessNumber {process_number}: {str(e)}")
                    continue
    
    def _extract_availability_date(self, publication: Publication, content: str) -> None:
        """Extrai data de disponibiliza√ß√£o."""
        for pattern in self.patterns['dates']:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                try:
                    date_str = match.group(1)
                    publication.availability_date = datetime.strptime(date_str, '%d/%m/%Y')
                    logger.debug(f"Data de disponibiliza√ß√£o extra√≠da: {publication.availability_date}")
                    return
                except Exception as e:
                    logger.debug(f"Erro ao parsear data {date_str}: {str(e)}")
                    continue
    
    def _extract_authors(self, publication: Publication, content: str) -> None:
        """Extrai autores/requerentes."""
        for pattern in self.patterns['authors']:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                author_name = match.group(1).strip()
                if author_name and len(author_name) > 2:
                    publication.authors.append(author_name)
                    logger.debug(f"Autor extra√≠do: {author_name}")
                    return
    
    def _extract_lawyers(self, publication: Publication, content: str) -> None:
        """Extrai advogados."""
        for pattern in self.patterns['lawyers']:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                lawyer_info = match.group(1).strip()
                if lawyer_info and len(lawyer_info) > 2:
                    publication.lawyers.append(lawyer_info)
                    logger.debug(f"Advogado extra√≠do: {lawyer_info}")
    
    def _extract_monetary_values(self, publication: Publication, content: str) -> None:
        """Extrai valores monet√°rios."""
        try:
            for field_name, patterns in self.patterns['monetary_values'].items():
                for pattern in patterns:
                    match = re.search(pattern, content, re.IGNORECASE)
                    if match:
                        try:
                            value_str = match.group(1).replace('.', '').replace(',', '.')
                            value = Decimal(value_str)
                            setattr(publication, field_name, value)
                            logger.debug(f"{field_name} extra√≠do: R$ {value}")
                            break
                        except (InvalidOperation, ValueError) as e:
                            logger.debug(f"Erro ao parsear {field_name}: {str(e)}")
                            continue
                            
        except Exception as e:
            logger.debug(f"Erro ao extrair valores monet√°rios: {str(e)}")
    
    async def extract_publication_links(self) -> List[Dict[str, str]]:
        """
        Extrai links para p√°ginas individuais de publica√ß√µes.
        
        Returns:
            Lista de dicts com informa√ß√µes dos links
        """
        try:
            links = []
            
            # Buscar links de ocorr√™ncias
            link_elements = await self.page.query_selector_all(self.selectors['result_links'])
            
            for element in link_elements:
                try:
                    href = await element.get_attribute('href')
                    text = await element.inner_text()
                    
                    if href:
                        links.append({
                            'url': href,
                            'text': text.strip(),
                            'type': 'publication_detail'
                        })
                        
                except Exception as e:
                    logger.debug(f"Erro ao extrair link: {e}")
                    continue
            
            logger.info(f"Encontrados {len(links)} links de publica√ß√µes")
            return links
            
        except Exception as e:
            logger.error(f"Erro ao extrair links: {e}")
            return []
    
    async def navigate_to_publication_detail(self, link_url: str) -> bool:
        """
        Navega para p√°gina de detalhes de uma publica√ß√£o espec√≠fica.
        
        Args:
            link_url: URL da p√°gina de detalhes
            
        Returns:
            bool: True se navega√ß√£o foi bem-sucedida
        """
        try:
            logger.info(f"Navegando para detalhes: {link_url}")
            
            await self.page.goto(link_url, wait_until='networkidle')
            await self.page.wait_for_load_state('domcontentloaded')
            
            logger.info("‚úÖ Navega√ß√£o para detalhes realizada")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao navegar para detalhes: {e}")
            return False
    
    async def extract_detailed_publication(self, 
                                         criteria: ScrapingCriteria) -> Optional[Publication]:
        """
        Extrai publica√ß√£o detalhada da p√°gina atual.
        
        Args:
            criteria: Crit√©rios para valida√ß√£o
            
        Returns:
            Publication ou None se n√£o conseguir extrair
        """
        try:
            # Aguardar carregamento
            await self.page.wait_for_load_state('domcontentloaded')
            
            # Extrair todo o conte√∫do da p√°gina
            page_content = await self.page.content()
            text_content = await self.page.evaluate('() => document.body.innerText')
            
            # Verificar se atende aos crit√©rios
            if not criteria.matches_content(text_content):
                logger.debug("P√°gina de detalhes n√£o atende aos crit√©rios")
                return None
            
            # Criar publica√ß√£o a partir do conte√∫do detalhado
            publication = await self._create_publication_from_content(text_content)
            
            if publication:
                logger.info(f"Publica√ß√£o detalhada extra√≠da: {publication.process_number}")
            
            return publication
            
        except Exception as e:
            logger.error(f"Erro ao extrair publica√ß√£o detalhada: {e}")
            return None
    
    async def check_pagination(self) -> Dict[str, Any]:
        """
        Verifica se h√° p√°ginas adicionais de resultados.
        
        Returns:
            Dict com informa√ß√µes de pagina√ß√£o
        """
        try:
            page_content = await self.page.content()
            
            pagination_info = {
                'has_next_page': False,
                'next_page_url': None,
                'current_page': 1,
                'total_pages': None
            }
            
            # Verificar se h√° link "Pr√≥xima"
            next_links = await self.page.query_selector_all('a:has-text("Pr√≥xima"), a:has-text("pr√≥xima")')
            
            if next_links:
                try:
                    next_url = await next_links[0].get_attribute('href')
                    pagination_info['has_next_page'] = True
                    pagination_info['next_page_url'] = next_url
                except:
                    pass
            
            # Tentar extrair n√∫mero da p√°gina atual
            page_match = re.search(r'P√°gina\s+(\d+)', page_content)
            if page_match:
                pagination_info['current_page'] = int(page_match.group(1))
            
            return pagination_info
            
        except Exception as e:
            logger.error(f"Erro ao verificar pagina√ß√£o: {e}")
            return {'error': str(e)}
    
    async def navigate_to_next_page(self) -> bool:
        """
        Navega para a pr√≥xima p√°gina de resultados.
        
        Returns:
            bool: True se navega√ß√£o foi bem-sucedida
        """
        try:
            # Verificar se h√° pr√≥xima p√°gina
            pagination_info = await self.check_pagination()
            
            if not pagination_info.get('has_next_page'):
                logger.info("N√£o h√° pr√≥xima p√°gina dispon√≠vel")
                return False
            
            next_url = pagination_info.get('next_page_url')
            if next_url:
                logger.info(f"Navegando para pr√≥xima p√°gina: {next_url}")
                await self.page.goto(next_url, wait_until='networkidle')
                await self.page.wait_for_load_state('domcontentloaded')
                return True
            else:
                # Tentar clicar no link "Pr√≥xima"
                next_link = await self.page.query_selector('a:has-text("Pr√≥xima"), a:has-text("pr√≥xima")')
                if next_link:
                    await next_link.click()
                    await self.page.wait_for_load_state('networkidle')
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Erro ao navegar para pr√≥xima p√°gina: {e}")
            return False