#!/usr/bin/env python3
"""
Servi√ßo Unificado de Scraper DJE + e-SAJ
Combina funcionalidades do multi_date_scraper e scraper_cli com enriquecimento integrado
"""

import asyncio
import json
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set
import argparse
from dataclasses import dataclass, asdict
from uuid import uuid4

# Adicionar o diret√≥rio src ao PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent / "src"))

from infrastructure.logging.logger import setup_logger
from infrastructure.config.settings import get_settings
from application.services.scraping_orchestrator import ScrapingOrchestrator
from application.services.process_enrichment_service import ProcessEnrichmentService
from application.usecases.extract_publications import ExtractPublicationsUseCase
from application.usecases.save_publications_to_files import (
    SavePublicationsToFilesUseCase,
)
from shared.container import Container
from domain.entities.publication import MonetaryValue, Lawyer
from domain.entities.scraping_execution import ScrapingExecution, ExecutionType

logger = setup_logger(__name__)


@dataclass
class ScraperConfig:
    """Configura√ß√£o do scraper"""

    start_date: datetime
    end_date: datetime
    search_terms: List[str]
    max_pages: int = 20
    enable_enrichment: bool = True
    save_to_files: bool = True
    save_to_api: bool = False
    progress_file: str = "scraper_progress.json"
    single_date: Optional[str] = None  # Para executar apenas uma data espec√≠fica


@dataclass
class DateProgress:
    """Progresso de uma data"""

    date: str
    processed: bool = False
    publications_found: int = 0
    publications_enriched: int = 0
    enrichment_errors: int = 0
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    error: Optional[str] = None


class UnifiedScraperService:
    """Servi√ßo unificado de scraping com enriquecimento"""

    def __init__(self, config: ScraperConfig):
        self.config = config
        self.container = Container(use_optimized_scraper=False)
        self.progress_data: Dict[str, DateProgress] = {}
        self.settings = get_settings()

        # Carregar ou criar arquivo de progresso
        self._load_progress()

        logger.info("üöÄ Servi√ßo Unificado de Scraper inicializado")
        logger.info(
            f"üìÖ Per√≠odo: {config.start_date.strftime('%d/%m/%Y')} at√© {config.end_date.strftime('%d/%m/%Y')}"
        )
        logger.info(f"üîç Termos de busca: {', '.join(config.search_terms)}")
        logger.info(
            f"‚ú® Enriquecimento: {'ATIVADO' if config.enable_enrichment else 'DESATIVADO'}"
        )

    def _load_progress(self):
        """Carrega arquivo de progresso se existir"""
        progress_path = Path(self.config.progress_file)

        if progress_path.exists():
            try:
                with open(progress_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for date_str, progress_dict in data.get("dates", {}).items():
                        self.progress_data[date_str] = DateProgress(**progress_dict)
                logger.info(f"üìÇ Progresso carregado: {len(self.progress_data)} datas")
            except Exception as e:
                logger.error(f"‚ùå Erro ao carregar progresso: {e}")
                self._initialize_progress()
        else:
            self._initialize_progress()

    def _initialize_progress(self):
        """Inicializa progresso para todas as datas"""
        current_date = self.config.start_date

        while current_date <= self.config.end_date:
            date_str = current_date.strftime("%d/%m/%Y")
            if date_str not in self.progress_data:
                self.progress_data[date_str] = DateProgress(date=date_str)
            current_date += timedelta(days=1)

        logger.info(f"üìä Inicializado progresso para {len(self.progress_data)} datas")

    def _save_progress(self):
        """Salva progresso em arquivo"""
        try:
            progress_dict = {
                "metadata": {
                    "last_updated": datetime.now().isoformat(),
                    "total_dates": len(self.progress_data),
                    "processed_dates": sum(
                        1 for p in self.progress_data.values() if p.processed
                    ),
                    "total_publications": sum(
                        p.publications_found for p in self.progress_data.values()
                    ),
                    "total_enriched": sum(
                        p.publications_enriched for p in self.progress_data.values()
                    ),
                },
                "dates": {
                    date: asdict(progress)
                    for date, progress in self.progress_data.items()
                },
            }

            with open(self.config.progress_file, "w", encoding="utf-8") as f:
                json.dump(progress_dict, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar progresso: {e}")

    async def scrape_date(self, date_str: str) -> Dict:
        """Executa scraping para uma data espec√≠fica"""
        logger.info(f"\n{'=' * 60}")
        logger.info(f"üìÖ PROCESSANDO DATA: {date_str}")
        logger.info(f"{'=' * 60}")

        progress = self.progress_data[date_str]
        progress.start_time = datetime.now().isoformat()

        stats = {
            "publications_found": 0,
            "publications_enriched": 0,
            "enrichment_errors": 0,
            "publications_saved": 0,
        }

        try:
            # 1. Configurar scraper para data espec√≠fica
            orchestrator = ScrapingOrchestrator(self.container)

            if hasattr(self.container.web_scraper, "_target_date"):
                self.container.web_scraper._target_date = date_str
            else:
                setattr(self.container.web_scraper, "_target_date", date_str)

            # 2. Extrair publica√ß√µes
            logger.info(f"\nüìÑ FASE 1: Extra√ß√£o das publica√ß√µes")
            extract_usecase = ExtractPublicationsUseCase(self.container.web_scraper)
            publications = []

            async for publication in extract_usecase.execute(
                self.config.search_terms, max_pages=self.config.max_pages
            ):
                publications.append(publication)
                stats["publications_found"] += 1

                if stats["publications_found"] % 10 == 0:
                    logger.info(
                        f"   üìä {stats['publications_found']} publica√ß√µes encontradas..."
                    )

            logger.info(f"‚úÖ Total extra√≠do: {stats['publications_found']} publica√ß√µes")

            # 3. Enriquecer publica√ß√µes (se habilitado)
            if self.config.enable_enrichment and publications:
                logger.info(f"\nüîç FASE 2: Enriquecimento com e-SAJ")

                async with ProcessEnrichmentService() as enrichment_service:
                    enriched_data_list = await enrichment_service.enrich_publications(
                        publications
                    )

                    # Processar dados enriquecidos
                    for i, enriched_data in enumerate(enriched_data_list):
                        if enriched_data and enriched_data.get("esaj_data"):
                            stats["publications_enriched"] += 1

                            # Atualizar publica√ß√£o com dados do e-SAJ
                            publication = publications[i]
                            esaj_data = enriched_data["esaj_data"]

                            # Atualizar valores monet√°rios
                            if esaj_data.get("movements", {}).get(
                                "homologation_details"
                            ):
                                homolog = esaj_data["movements"]["homologation_details"]

                                if homolog.get("gross_value"):
                                    try:
                                        value_str = (
                                            homolog["gross_value"]
                                            .replace("R$", "")
                                            .replace(".", "")
                                            .replace(",", ".")
                                            .strip()
                                        )
                                        publication.gross_value = (
                                            MonetaryValue.from_real(float(value_str))
                                        )
                                    except:
                                        pass

                                if homolog.get("interest_value"):
                                    try:
                                        value_str = (
                                            homolog["interest_value"]
                                            .replace("R$", "")
                                            .replace(".", "")
                                            .replace(",", ".")
                                            .strip()
                                        )
                                        publication.interest_value = (
                                            MonetaryValue.from_real(float(value_str))
                                        )
                                    except:
                                        pass

                                if homolog.get("attorney_fees"):
                                    try:
                                        value_str = (
                                            homolog["attorney_fees"]
                                            .replace("R$", "")
                                            .replace(".", "")
                                            .replace(",", ".")
                                            .strip()
                                        )
                                        publication.attorney_fees = (
                                            MonetaryValue.from_real(float(value_str))
                                        )
                                    except:
                                        pass

                            # Atualizar advogados
                            if esaj_data.get("parties", {}).get("lawyers"):
                                publication.lawyers = [
                                    Lawyer(
                                        name=lawyer.get("name", ""),
                                        oab=lawyer.get("oab", ""),
                                    )
                                    for lawyer in esaj_data["parties"]["lawyers"]
                                ]

                            # Atualizar data de disponibilidade
                            if esaj_data.get("movements", {}).get("availability_date"):
                                try:
                                    publication.availability_date = datetime.strptime(
                                        esaj_data["movements"]["availability_date"],
                                        "%d/%m/%Y",
                                    )
                                except:
                                    pass
                        else:
                            stats["enrichment_errors"] += 1

                    if (
                        stats["publications_enriched"] % 10 == 0
                        and stats["publications_enriched"] > 0
                    ):
                        logger.info(
                            f"   ‚ú® {stats['publications_enriched']} publica√ß√µes enriquecidas..."
                        )

                logger.info(
                    f"‚úÖ Enriquecimento conclu√≠do: {stats['publications_enriched']} sucessos, {stats['enrichment_errors']} falhas"
                )

            # 4. Salvar publica√ß√µes
            if self.config.save_to_files and publications:
                logger.info(f"\nüíæ FASE 3: Salvamento das publica√ß√µes")
                save_usecase = SavePublicationsToFilesUseCase()
                save_stats = await save_usecase.execute(publications)
                stats["publications_saved"] = save_stats["saved"]
                logger.info(
                    f"‚úÖ Salvamento conclu√≠do: {stats['publications_saved']} arquivos"
                )

            # Atualizar progresso
            progress.processed = True
            progress.publications_found = stats["publications_found"]
            progress.publications_enriched = stats["publications_enriched"]
            progress.enrichment_errors = stats["enrichment_errors"]
            progress.end_time = datetime.now().isoformat()

            logger.info(f"\nüìä RESUMO DA DATA {date_str}:")
            logger.info(f"   üìÑ Publica√ß√µes encontradas: {stats['publications_found']}")
            logger.info(
                f"   ‚ú® Publica√ß√µes enriquecidas: {stats['publications_enriched']}"
            )
            logger.info(f"   üíæ Publica√ß√µes salvas: {stats['publications_saved']}")

        except Exception as e:
            logger.error(f"‚ùå Erro ao processar data {date_str}: {e}")
            progress.error = str(e)
            import traceback

            traceback.print_exc()

        finally:
            self._save_progress()

        return stats

    async def run(self):
        """Executa o scraping para todas as datas ou data espec√≠fica"""
        try:
            if self.config.single_date:
                # Executar apenas para data espec√≠fica
                await self.scrape_date(self.config.single_date)
            else:
                # Executar para todas as datas n√£o processadas
                dates_to_process = [
                    date_str
                    for date_str, progress in self.progress_data.items()
                    if not progress.processed or progress.error
                ]

                # Ordenar datas
                dates_to_process.sort(key=lambda x: datetime.strptime(x, "%d/%m/%Y"))

                logger.info(f"\nüóìÔ∏è {len(dates_to_process)} datas para processar")

                for i, date_str in enumerate(dates_to_process, 1):
                    logger.info(f"\nüìç Processando {i}/{len(dates_to_process)}")
                    await self.scrape_date(date_str)

                    # Pequena pausa entre datas
                    if i < len(dates_to_process):
                        await asyncio.sleep(2)

            # Estat√≠sticas finais
            total_publications = sum(
                p.publications_found for p in self.progress_data.values()
            )
            total_enriched = sum(
                p.publications_enriched for p in self.progress_data.values()
            )
            total_processed = sum(1 for p in self.progress_data.values() if p.processed)

            logger.info(f"\n{'=' * 60}")
            logger.info(f"üèÅ EXECU√á√ÉO FINALIZADA")
            logger.info(f"{'=' * 60}")
            logger.info(f"üìä Estat√≠sticas Finais:")
            logger.info(
                f"   üìÖ Datas processadas: {total_processed}/{len(self.progress_data)}"
            )
            logger.info(f"   üìÑ Total de publica√ß√µes: {total_publications}")
            logger.info(f"   ‚ú® Total enriquecido: {total_enriched}")
            if total_publications > 0:
                logger.info(
                    f"   üìà Taxa de enriquecimento: {(total_enriched / total_publications) * 100:.1f}%"
                )

        except Exception as e:
            logger.error(f"‚ùå Erro durante execu√ß√£o: {e}")
            raise
        finally:
            await self.container.cleanup()


def create_parser():
    """Cria parser de argumentos"""
    parser = argparse.ArgumentParser(
        description="Servi√ßo Unificado de Scraper DJE + e-SAJ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:

  # Executar para um per√≠odo de datas:
  python unified_scraper_service.py --start-date 01/06/2025 --end-date 30/06/2025

  # Executar para uma data espec√≠fica:
  python unified_scraper_service.py --date 20/06/2025

  # Executar sem enriquecimento:
  python unified_scraper_service.py --date 20/06/2025 --no-enrichment

  # Executar com termos de busca customizados:
  python unified_scraper_service.py --date 20/06/2025 --search-terms "RPV" "precat√≥rio" "INSS"
""",
    )

    # Argumentos de data
    date_group = parser.add_mutually_exclusive_group(required=True)
    date_group.add_argument(
        "--date", help="Data espec√≠fica para processar (formato: DD/MM/YYYY)"
    )
    date_group.add_argument(
        "--start-date", help="Data inicial do per√≠odo (formato: DD/MM/YYYY)"
    )

    parser.add_argument(
        "--end-date", help="Data final do per√≠odo (formato: DD/MM/YYYY, padr√£o: hoje)"
    )

    # Argumentos de busca
    parser.add_argument(
        "--search-terms",
        nargs="+",
        default=["RPV", "pagamento pelo INSS"],
        help="Termos de busca (padr√£o: RPV 'pagamento pelo INSS')",
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        default=20,
        help="M√°ximo de p√°ginas por busca (padr√£o: 20)",
    )

    # Argumentos de funcionalidade
    parser.add_argument(
        "--no-enrichment",
        action="store_true",
        help="Desabilita o enriquecimento com e-SAJ",
    )

    parser.add_argument(
        "--no-save",
        action="store_true",
        help="N√£o salva arquivos JSON (apenas processa)",
    )

    parser.add_argument(
        "--progress-file",
        default="scraper_progress.json",
        help="Arquivo de progresso (padr√£o: scraper_progress.json)",
    )

    return parser


async def main():
    """Fun√ß√£o principal"""
    parser = create_parser()
    args = parser.parse_args()

    # Configurar datas
    if args.date:
        # Modo data √∫nica
        start_date = end_date = datetime.strptime(args.date, "%d/%m/%Y")
        single_date = args.date
    else:
        # Modo per√≠odo
        start_date = datetime.strptime(args.start_date, "%d/%m/%Y")
        end_date = (
            datetime.strptime(args.end_date, "%d/%m/%Y")
            if args.end_date
            else datetime.now()
        )
        single_date = None

    # Criar configura√ß√£o
    config = ScraperConfig(
        start_date=start_date,
        end_date=end_date,
        search_terms=args.search_terms,
        max_pages=args.max_pages,
        enable_enrichment=not args.no_enrichment,
        save_to_files=not args.no_save,
        progress_file=args.progress_file,
        single_date=single_date,
    )

    # Executar servi√ßo
    service = UnifiedScraperService(config)
    await service.run()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\n‚å®Ô∏è Interrompido pelo usu√°rio")
    except Exception as e:
        logger.error(f"‚ùå Erro fatal: {e}")
        sys.exit(1)
