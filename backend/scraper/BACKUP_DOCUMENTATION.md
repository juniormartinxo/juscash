# ğŸ“‹ DOCUMENTAÃ‡ÃƒO DE BACKUP - IMPLEMENTAÃ‡ÃƒO ATUAL DO SCRAPER

**Data do Backup**: 19 de dezembro de 2025  
**Branch**: backup/current-scraper-implementation  
**VersÃ£o**: 1.0.0 (prÃ©-melhorias)

## ğŸ—ï¸ **ARQUITETURA ATUAL**

### **Componentes Principais**

```txt
ğŸ“¦ DJE Scraper System v1.0
â”œâ”€â”€ ğŸ•·ï¸ DJEScraperAdapter (infrastructure/web/dje_scraper_adapter.py)
â”‚   â”œâ”€â”€ Browser: Playwright Chromium
â”‚   â”œâ”€â”€ Parser: DJEContentParser + EnhancedDJEContentParser
â”‚   â”œâ”€â”€ PDF Download: AutomÃ¡tico via onclick events
â”‚   â””â”€â”€ JSON Output: ReportJsonSaver
â”œâ”€â”€ ğŸ§  Parsers
â”‚   â”œâ”€â”€ DJEContentParser (bÃ¡sico)
â”‚   â”œâ”€â”€ EnhancedDJEContentParser (padrÃ£o RPV/INSS)
â”‚   â””â”€â”€ PublicationValidator
â”œâ”€â”€ ğŸ”„ Orchestrator
â”‚   â”œâ”€â”€ ScrapingOrchestrator
â”‚   â”œâ”€â”€ ExtractPublicationsUseCase
â”‚   â””â”€â”€ SavePublicationsToFilesUseCase
â”œâ”€â”€ ğŸ“Š Monitoring
â”‚   â”œâ”€â”€ APIWorker (Redis queue processing)
â”‚   â”œâ”€â”€ FileMonitor
â”‚   â””â”€â”€ PerformanceMonitor
â””â”€â”€ ğŸ”§ Infrastructure
    â”œâ”€â”€ Container (DI)
    â”œâ”€â”€ Settings/Config
    â”œâ”€â”€ Logging (loguru)
    â””â”€â”€ Health Checks
```

## ğŸ“‹ **FLUXO DE FUNCIONAMENTO ATUAL**

### **1. InicializaÃ§Ã£o**

1. Container injeta dependÃªncias
2. DJEScraperAdapter inicializa Playwright
3. Enhanced parser Ã© configurado com scraper adapter

### **2. Scraping Process**

1. Navega para consultaAvancada.do
2. Preenche formulÃ¡rio (data, termos de busca)
3. Encontra elementos tr.ementaClass
4. Extrai URLs de PDF via onclick events
5. Baixa PDFs em pÃ¡ginas separadas
6. Processa conteÃºdo via parsers
7. Salva publicaÃ§Ãµes como JSON

### **3. Parsing Strategy**

- **Parser bÃ¡sico**: Regex simples para dados gerais
- **Enhanced parser**: Busca por RPV/INSS com lÃ³gica reversa
- **Fallback**: Enhanced â†’ Basic se falhar

## ğŸ”§ **DEPENDÃŠNCIAS ATUAIS**

### **Python (Scraper)**

```txt
playwright==1.52.0          # AutomaÃ§Ã£o browser
loguru==0.7.3               # Logging avanÃ§ado
pydantic==2.11.5            # ValidaÃ§Ã£o de dados
httpx==0.28.1               # HTTP client
beautifulsoup4==4.13.4      # HTML parsing
PyPDF2==3.0.1               # PDF text extraction
pdfplumber==0.11.4          # PDF parsing alternativo
redis==6.2.0                # Queue sistema
psycopg2-binary==2.9.10     # PostgreSQL
SQLAlchemy==2.0.41          # ORM
APScheduler==3.11.0         # Task scheduling
watchdog>=6.0.0             # File monitoring
```

### **Node.js (API)**

```txt
typescript: 5.8.3           # Language
express: 5.1.0              # Web framework
prisma: 6.9.0               # ORM
zod: 3.22.4                 # Validation
winston: 3.11.0             # Logging
jsonwebtoken: 9.0.2        # Auth
swagger-ui-express: 5.0.0   # Documentation
```

## ğŸ“Š **LIMITAÃ‡Ã•ES IDENTIFICADAS**

### **1. PublicaÃ§Ãµes Divididas**

âŒ **Problema**: Parser nÃ£o consegue lidar com publicaÃ§Ãµes que comeÃ§am em uma pÃ¡gina e continuam na prÃ³xima  
ğŸ“‹ **Comportamento atual**: Perde inÃ­cio ou fim da publicaÃ§Ã£o  
ğŸ”§ **Tentativa existente**: Enhanced parser tem mÃ©todo `_download_previous_page()` mas implementaÃ§Ã£o incompleta

### **2. Cache de PÃ¡ginas**

âŒ **Problema**: NÃ£o hÃ¡ cache para pÃ¡ginas baixadas  
ğŸ“‹ **Comportamento atual**: Re-baixa a mesma pÃ¡gina mÃºltiplas vezes  
ğŸ”§ **Impacto**: Performance degradada e potencial rate limiting

### **3. ValidaÃ§Ã£o de Merge**

âŒ **Problema**: NÃ£o hÃ¡ validaÃ§Ã£o se conteÃºdo merged estÃ¡ correto  
ğŸ“‹ **Comportamento atual**: Pode fazer merge incorreto silenciosamente  
ğŸ”§ **Impacto**: Dados incorretos ou incompletos

### **4. Analytics Limitado**

âŒ **Problema**: Falta sistema de anÃ¡lise de qualidade das extraÃ§Ãµes  
ğŸ“‹ **Comportamento atual**: Apenas logs bÃ¡sicos  
ğŸ”§ **Impacto**: DifÃ­cil identificar problemas de qualidade

### **5. Rate Limiting BÃ¡sico**

âš ï¸ **Problema**: Rate limiting simples baseado apenas em delays  
ğŸ“‹ **Comportamento atual**: Delay fixo entre requisiÃ§Ãµes  
ğŸ”§ **Impacto**: Pode ser bloqueado pelo servidor

## ğŸ“ **ESTRUTURA DE ARQUIVOS ATUAL**

```txt
backend/scraper/src/
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ services/scraping_orchestrator.py
â”‚   â””â”€â”€ usecases/
â”‚       â”œâ”€â”€ extract_publications.py
â”‚       â”œâ”€â”€ save_publications_to_files.py
â”‚       â””â”€â”€ validate_extracted_data.py
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”œâ”€â”€ publication.py
â”‚   â”‚   â””â”€â”€ scraping_execution.py
â”‚   â”œâ”€â”€ ports/
â”‚   â”‚   â”œâ”€â”€ scraping_repository.py
â”‚   â”‚   â””â”€â”€ web_scraper.py
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ publication_validator.py
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ api/api_client_adapter.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â””â”€â”€ dynamic_config.py
â”‚   â”œâ”€â”€ database/repositories/
â”‚   â”œâ”€â”€ files/report_json_saver.py
â”‚   â”œâ”€â”€ logging/logger.py
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ api_worker.py
â”‚   â”‚   â”œâ”€â”€ file_monitor.py
â”‚   â”‚   â””â”€â”€ performance_monitor.py
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ content_parser.py
â”‚       â”œâ”€â”€ enhanced_content_parser.py
â”‚       â””â”€â”€ dje_scraper_adapter.py
â””â”€â”€ shared/container.py
```

## ğŸ”‘ **CONFIGURAÃ‡Ã•ES IMPORTANTES**

### **VariÃ¡veis de Ambiente**

```bash
# Scraper
SCRAPER_TARGET_URL=https://esaj.tjsp.jus.br/cdje/consultaAvancada.do#buscaavancada
SCRAPER_MAX_RETRIES=3
SCRAPER_RETRY_DELAY=5
SCRAPER_MAX_PAGES=20

# API
API_BASE_URL=http://localhost:3001
SCRAPER_API_KEY=scraper-dj-1t0blW7epxd72BnoGezVjjXUtmbE11WXp0oSDhXJUFNo3ZEC5UVDhYfjLJX1Jqb12fbRB4ZUjP

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/juscash
REDIS_URL=redis://localhost:6379

# Browser
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=30000
```

### **Seletores CSS Utilizados**

```python
# tr.ementaClass - Links para PDFs
ementa_elements = await page.query_selector_all("tr.ementaClass")

# onclick events com popup
onclick_elements = await element.query_selector_all('[onclick*="popup"]')

# NavegaÃ§Ã£o pÃ¡ginas
next_selectors = [
    'a:text("PrÃ³xima")',
    'a:text(">")',
    'a[title*="prÃ³xima"]',
    'input[value="PrÃ³xima"]',
]
```

## ğŸ“ˆ **MÃ‰TRICAS ATUAIS**

### **Performance TÃ­pica**

- **PDFs por minuto**: 10-20 (com delays)
- **Taxa de sucesso**: ~85-90%
- **Tempo por pÃ¡gina**: 5-10 segundos
- **Memory usage**: 200-400MB

### **LimitaÃ§Ãµes Conhecidas**

- **PublicaÃ§Ãµes perdidas**: ~10-15% (pÃ¡ginas divididas)
- **Re-downloads**: 20-30% das pÃ¡ginas
- **Rate limiting hits**: 2-5% das execuÃ§Ãµes

## ğŸš¨ **PONTOS CRÃTICOS PARA PRESERVAR**

### **âœ… Funcionalidades que FUNCIONAM**

1. **NavegaÃ§Ã£o automatizada** para consultaAvancada.do
2. **Preenchimento de formulÃ¡rio** com data especÃ­fica
3. **Download de PDFs** via onclick events
4. **ExtraÃ§Ã£o bÃ¡sica** de dados de publicaÃ§Ãµes
5. **Salvamento em JSON** estruturado
6. **Rate limiting bÃ¡sico** funcional
7. **Retry logic** em falhas de rede
8. **Logging detalhado** com loguru
9. **Container de DI** funcionando
10. **API integration** via queue Redis

### **âš ï¸ ConfiguraÃ§Ãµes CrÃ­ticas**

1. **User Agent**: Deve ser mantido para evitar bloqueio
2. **Timeouts**: Valores atuais funcionam bem
3. **Seletores CSS**: tr.ementaClass Ã© estÃ¡vel
4. **Termos de busca**: ["RPV", "pagamento pelo INSS"] validados
5. **Headers HTTP**: ConfiguraÃ§Ã£o atual evita detecÃ§Ã£o

## ğŸ“‹ **CHECKLIST DE RECUPERAÃ‡ÃƒO**

### **Se algo der errado com as melhorias:**

1. **Voltar para este branch**:

   ```bash
   git checkout backup/current-scraper-implementation
   ```

2. **Restaurar dependÃªncias**:

   ```bash
   cd backend/scraper
   pip install -r requirements.txt
   ```

3. **Verificar configuraÃ§Ãµes**:
   - VariÃ¡veis de ambiente (.env)
   - Redis funcionando
   - PostgreSQL funcionando

4. **Testar funcionamento bÃ¡sico**:

   ```bash
   python -m src.main
   ```

5. **Verificar logs**:
   - logs/scraper.log
   - logs/debug_images/ (screenshots)
   - data/json_reports/ (outputs)

## ğŸ¯ **PRÃ“XIMOS PASSOS (MELHORIAS)**

1. **Implementar Page Manager** para pÃ¡ginas divididas
2. **Sistema de cache** para evitar re-downloads
3. **ValidaÃ§Ã£o de merge** robusta
4. **Analytics avanÃ§ado** de qualidade
5. **Rate limiting inteligente** adaptativo
6. **Testes automatizados** completos
7. **Monitoramento em tempo real** melhorado

---

**ğŸ“… Criado em**: 19/12/2025  
**ğŸ‘¨â€ğŸ’» ResponsÃ¡vel**: Sistema automatizado de backup  
**ğŸ”„ VersÃ£o**: 1.0.0-backup  
**ğŸŒŸ Status**: âœ… Funcional e estÃ¡vel
