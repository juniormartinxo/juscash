# README.md

## ğŸ•·ï¸ DJE Scraper - SÃ£o Paulo

Sistema de web scraping automatizado para extrair publicaÃ§Ãµes do DiÃ¡rio da JustiÃ§a EletrÃ´nico (DJE) de SÃ£o Paulo, desenvolvido em Python com Arquitetura Hexagonal.

## ğŸ¯ Objetivo

Automatizar a extraÃ§Ã£o diÃ¡ria de publicaÃ§Ãµes do DJE-SP, especificamente do **Caderno 3 - Judicial - 1Âª InstÃ¢ncia - Capital Parte 1**, buscando por publicaÃ§Ãµes que contenham termos obrigatÃ³rios relacionados ao INSS.

## ğŸ—ï¸ Arquitetura

O projeto segue **Arquitetura Hexagonal** (Ports and Adapters):

```txt
src/
â”œâ”€â”€ domain/                 # ğŸ¯ Core Domain
â”‚   â”œâ”€â”€ entities/          # Entidades de negÃ³cio
â”‚   â”œâ”€â”€ ports/             # Interfaces/Contratos
â”‚   â””â”€â”€ services/          # ServiÃ§os de domÃ­nio
â”œâ”€â”€ application/           # ğŸ® Application Layer
â”‚   â”œâ”€â”€ usecases/         # Casos de uso
â”‚   â””â”€â”€ services/         # Orquestradores
â”œâ”€â”€ infrastructure/       # ğŸ”§ Infrastructure Layer
â”‚   â”œâ”€â”€ web/              # Web scraping adapters
â”‚   â”œâ”€â”€ api/              # API client adapters
â”‚   â”œâ”€â”€ config/           # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ logging/          # Sistema de logs
â”‚   â””â”€â”€ scheduler/        # Agendamento
â””â”€â”€ shared/               # ğŸ› ï¸ Shared Kernel
    â””â”€â”€ container.py      # InjeÃ§Ã£o de dependÃªncia
```

## ğŸš€ Funcionalidades

- âœ… **Scraping Automatizado**: ExtraÃ§Ã£o diÃ¡ria a partir de 17/03/2025
- âœ… **Busca Inteligente**: Filtra publicaÃ§Ãµes com termos obrigatÃ³rios
- âœ… **ExtraÃ§Ã£o Estruturada**: NÃºmero do processo, autores, advogados, valores
- âœ… **IntegraÃ§Ã£o com API**: Envia dados via endpoint `/api/scraper/publications`
- âœ… **Logs Detalhados**: Sistema completo de auditoria
- âœ… **Retry Logic**: RecuperaÃ§Ã£o automÃ¡tica de falhas
- âœ… **ValidaÃ§Ã£o de Dados**: ValidaÃ§Ã£o rigorosa antes do envio

## ğŸ“‹ PrÃ©-requisitos

- Python 3.12+
- API JusCash rodando (com endpoint `/api/scraper/publications`)
- Chave API configurada (`SCRAPER_API_KEY`)

## ğŸ› ï¸ InstalaÃ§Ã£o

1. **Clone e configure o ambiente**:

    ```bash
    # Instalar dependÃªncias
    pip install -r requirements.txt

    # Instalar browsers do Playwright
    python -m playwright install chromium --with-deps
    ```

2. **Configurar variÃ¡veis de ambiente**:

    ```bash
    # Copiar e editar configuraÃ§Ãµes
    cp .env.example .env

    # Configurar SCRAPER_API_KEY obrigatoriamente
    SCRAPER_API_KEY=sua-chave-api-muito-longa-e-segura-aqui
    ```

3. **Executar testes**:

    ```bash
    # Testar conexÃ£o com API
    python test_api_connection.py

    # Testar scraping manual
    python test_scraper_manual.py

    # Ou usar o script completo
    ./run_tests.sh
    ```

## ğŸš€ ExecuÃ§Ã£o

### Modo ProduÃ§Ã£o (Agendado)

```bash
# ExecuÃ§Ã£o automÃ¡tica diÃ¡ria
python src/main.py
```

### Modo Desenvolvimento (Manual)

```bash
# ExecuÃ§Ã£o Ãºnica para testes
python test_scraper_manual.py
```

### Docker

```bash
# Build e execuÃ§Ã£o
docker-compose -f docker-compose.scraper.yml up --build

# Para desenvolvimento
docker-compose -f docker-compose.scraper.yml run scraper python test_api_connection.py
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente Principais

```bash
# API
API_BASE_URL=http://localhost:8000
SCRAPER_API_KEY=sua-chave-api-aqui
API_TIMEOUT=30

# Browser
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=30000

# Scraper
SCRAPER_MAX_PAGES=20
SCRAPER_SEARCH_TERMS=aposentadoria,benefÃ­cio,INSS

# Logs
LOG_LEVEL=INFO
LOG_DIR=./logs
```

### Termos de Busca

Configure no `.env` os termos que devem estar presentes nas publicaÃ§Ãµes:

```bash
SCRAPER_SEARCH_TERMS=aposentadoria,benefÃ­cio,INSS,auxÃ­lio
```

## ğŸ“Š Monitoramento

### Logs

```bash
# Logs em tempo real
tail -f logs/scraper_$(date +%Y-%m-%d).log

# Logs de erro
tail -f logs/errors_$(date +%Y-%m-%d).log
```

### EstatÃ­sticas

O scraper registra automaticamente:

- PublicaÃ§Ãµes encontradas
- PublicaÃ§Ãµes novas vs duplicadas
- Taxa de sucesso/falha
- Tempo de execuÃ§Ã£o
- Erros detalhados

## ğŸ§ª Testes

### Testes Automatizados

```bash
# Teste completo
./run_tests.sh

# Testes individuais
python test_api_connection.py      # ConexÃ£o com API
python test_scraper_manual.py      # Scraping manual
```

### Dados de Teste

O sistema cria publicaÃ§Ãµes de teste com:

- NÃºmeros de processo Ãºnicos (timestamp)
- Prefixo "TESTE" para identificaÃ§Ã£o
- Metadados de teste

## ğŸ”§ Desenvolvimento

### Estrutura de Dados

**PublicaÃ§Ã£o extraÃ­da**:

```python
{
    "process_number": "1234567-89.2024.8.26.0100",
    "authors": ["JoÃ£o Silva Santos"],
    "defendant": "Instituto Nacional do Seguro Social - INSS",
    "lawyers": [{"name": "Dr. Carlos", "oab": "123456"}],
    "gross_value": 150000,  # em centavos
    "content": "Texto completo da publicaÃ§Ã£o...",
    "extraction_metadata": {
        "extraction_date": "2024-03-17T10:30:00Z",
        "source_url": "https://dje.tjsp.jus.br/...",
        "confidence_score": 0.95
    }
}
```

### Adicionando Novos Adapters

Para adicionar novos scrapers ou APIs:

1. **Criar Port** (interface) em `src/domain/ports/`
2. **Implementar Adapter** em `src/infrastructure/`
3. **Registrar no Container** em `src/shared/container.py`
4. **Configurar Use Case** em `src/application/usecases/`

### Debugging

```bash
# Logs detalhados
LOG_LEVEL=DEBUG python src/main.py

# Browser visÃ­vel (desenvolvimento)
BROWSER_HEADLESS=false python test_scraper_manual.py
```

## ğŸ”’ SeguranÃ§a

- âœ… **API Key obrigatÃ³ria** para todas as requisiÃ§Ãµes
- âœ… **Rate limiting** respeitado (1000 req/15min)
- âœ… **ValidaÃ§Ã£o rigorosa** de dados extraÃ­dos
- âœ… **Logs de auditoria** completos
- âœ… **Retry com backoff** para evitar sobrecarga

## ğŸ“ˆ Performance

- **PÃ¡ginas processadas**: ConfigurÃ¡vel (padrÃ£o: 20)
- **Timeout por pÃ¡gina**: 30 segundos
- **Retry automÃ¡tico**: 3 tentativas com delay progressivo
- **MemÃ³ria otimizada**: Processamento streaming das publicaÃ§Ãµes

## ğŸ†˜ Troubleshooting

### Problemas Comuns

**1. Erro de API Key**

```bash
âŒ Invalid API Key
# SoluÃ§Ã£o: Verificar SCRAPER_API_KEY no .env
```

**2. Timeout do Browser**

```bash
âŒ Timeout 30000ms exceeded
# SoluÃ§Ã£o: Aumentar BROWSER_TIMEOUT ou verificar conexÃ£o
```

**3. DependÃªncias Playwright**

```bash
âŒ Browser not found
# SoluÃ§Ã£o: python -m playwright install chromium --with-deps
```

### Logs de Debug

```bash
# Ativar debug completo
LOG_LEVEL=DEBUG
BROWSER_HEADLESS=false
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Siga a arquitetura hexagonal
4. Adicione testes
5. FaÃ§a commit das mudanÃ§as
6. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob licenÃ§a MIT. Veja o arquivo `LICENSE` para detalhes.

**ğŸ¯ Status**: âœ… Pronto para produÃ§Ã£o
**ğŸ“… InÃ­cio**: 17/03/2025 (execuÃ§Ã£o diÃ¡ria)
**ğŸ•·ï¸ Target**: DJE-SP Caderno 3 - Judicial - 1Âª InstÃ¢ncia - Capital Parte 1
