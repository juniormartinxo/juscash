# ðŸ”— Guia de IntegraÃ§Ã£o - DJE Scraper com API JusCash

## ðŸ“‹ VisÃ£o Geral da IntegraÃ§Ã£o

O scraper DJE-SP foi desenvolvido para trabalhar em conjunto com a API JusCash existente, enviando publicaÃ§Ãµes extraÃ­das via endpoint `/api/scraper/publications` usando autenticaÃ§Ã£o por API Key.

## ðŸ—ï¸ Arquitetura de IntegraÃ§Ã£o

```txt
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP POST      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DJE Scraper   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   API JusCash    â”‚ â”€â”€â–¶ â”‚   PostgreSQL    â”‚
â”‚   (Python)      â”‚   X-API-Key       â”‚   (Node.js)      â”‚     â”‚   Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                       â”‚
        â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DJE Website   â”‚                   â”‚   React Frontend â”‚
â”‚   (Playwright)  â”‚                   â”‚   (Kanban)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”§ ConfiguraÃ§Ã£o da IntegraÃ§Ã£o

### 1. Configurar API Key na API JusCash

**Arquivo**: `backend/api/.env`

```bash
# Adicionar a mesma chave nos dois projetos
SCRAPER_API_KEY=scraper_2024_a8f3k9m2x7q1w5e8r4t6y9u3i0p2s5d7f1g4h6j8k0l3z9x2c4v6b8n1m5q7w9e3r5t7y1u4i6o8p0
```

**Verificar endpoints ativos**:

```bash
# API deve estar rodando em:
http://localhost:8000/api/scraper/publications
```

### 2. Configurar Scraper

**Arquivo**: `backend/scraper/.env`

```bash
# ConfiguraÃ§Ã£o para comunicar com a API
API_BASE_URL=http://localhost:8000
SCRAPER_API_KEY=scraper_2024_a8f3k9m2x7q1w5e8r4t6y9u3i0p2s5d7f1g4h6j8k0l3z9x2c4v6b8n1m5q7w9e3r5t7y1u4i6o8p0

# Em Docker, usar o nome do serviÃ§o
# API_BASE_URL=http://juscash-api:8000
```

### 3. Estrutura de Pastas Recomendada

```txt
juscash-dje-system/
â”œâ”€â”€ docker-compose.yml           # Compose principal
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/                    # API Node.js existente
â”‚   â”‚   â”œâ”€â”€ .env
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ scraper/                # Novo scraper Python
â”‚       â”œâ”€â”€ .env
â”‚       â”œâ”€â”€ src/
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ test_api_connection.py
â”œâ”€â”€ frontend/                   # React app existente
â””â”€â”€ database/                   # PostgreSQL
```

## ðŸš€ Deploy Integrado

### Docker Compose Atualizado

**Arquivo**: `docker-compose.yml`

```yaml
services:
  # API existente
  api:
    build:
      context: ./backend/api
    container_name: juscash-api
    ports:
      - "8000:8000"
    environment:
      - SCRAPER_API_KEY=${SCRAPER_API_KEY}
    depends_on:
      - postgres
    networks:
      - juscash-net

  # Novo serviÃ§o do scraper
  scraper:
    build:
      context: ./backend/scraper
    container_name: juscash-scraper
    environment:
      - API_BASE_URL=http://juscash-api:8000
      - SCRAPER_API_KEY=${SCRAPER_API_KEY}
      - BROWSER_HEADLESS=true
      - LOG_LEVEL=INFO
    volumes:
      - ./backend/scraper/logs:/app/logs
    depends_on:
      - api
    networks:
      - juscash-net
    restart: unless-stopped

  # ServiÃ§os existentes
  postgres:
    # ... configuraÃ§Ã£o existente
  
  frontend:
    # ... configuraÃ§Ã£o existente

networks:
  juscash-net:
    driver: bridge
```

### Scripts de Deploy

**Arquivo**: `deploy.sh`

```bash
#!/bin/bash
# Script de deploy completo

echo "ðŸš€ Deploy JusCash DJE System"
echo "============================="

# Verificar se .env existe
if [ ! -f ".env" ]; then
    echo "âŒ Arquivo .env nÃ£o encontrado"
    exit 1
fi

# Carregar variÃ¡veis de ambiente
source .env

# Verificar SCRAPER_API_KEY
if [ -z "$SCRAPER_API_KEY" ]; then
    echo "âŒ SCRAPER_API_KEY nÃ£o definida"
    echo "ðŸ’¡ Configure no arquivo .env"
    exit 1
fi

echo "âœ… ConfiguraÃ§Ã£o validada"

# Build e deploy
echo "ðŸ—ï¸  Building containers..."
docker-compose build

echo "ðŸš€ Starting services..."
docker-compose up -d

# Aguardar API ficar pronta
echo "â³ Aguardando API ficar pronta..."
timeout 60 bash -c 'until curl -f http://localhost:8000/health > /dev/null 2>&1; do sleep 2; done'

if [ $? -eq 0 ]; then
    echo "âœ… API estÃ¡ rodando"
else
    echo "âŒ Timeout - API nÃ£o respondeu"
    exit 1
fi

# Testar integraÃ§Ã£o
echo "ðŸ§ª Testando integraÃ§Ã£o scraper -> API..."
docker-compose exec scraper python test_api_connection.py

if [ $? -eq 0 ]; then
    echo "âœ… IntegraÃ§Ã£o funcionando"
else
    echo "âŒ Falha na integraÃ§Ã£o"
    echo "ðŸ“‹ Logs do scraper:"
    docker-compose logs scraper
    exit 1
fi

echo ""
echo "ðŸŽ‰ Deploy concluÃ­do com sucesso!"
echo "ðŸ“Š Dashboard: http://localhost:3000"
echo "ðŸ”— API: http://localhost:8000"
echo "ðŸ“‹ API Docs: http://localhost:8000/api-docs"
echo "ðŸ“ˆ Health: http://localhost:8000/health"
```

## ðŸ§ª Testes de IntegraÃ§Ã£o

### Script de Teste Completo

**Arquivo**: `test_integration.py`

```python
#!/usr/bin/env python3
"""
Teste completo de integraÃ§Ã£o Scraper -> API -> Database
"""

import asyncio
import httpx
import os
from datetime import datetime

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
SCRAPER_API_KEY = os.getenv("SCRAPER_API_KEY")


async def test_full_integration():
    """Testa o fluxo completo de integraÃ§Ã£o"""
    print("ðŸ§ª Teste de IntegraÃ§Ã£o Completa")
    print("=" * 40)
    
    if not SCRAPER_API_KEY:
        print("âŒ SCRAPER_API_KEY nÃ£o configurada")
        return False
    
    # Dados de teste
    test_data = {
        "process_number": f"INTEGRATION-{int(datetime.now().timestamp())}-89.2024.8.26.0100",
        "publicationDate": "2024-03-15T00:00:00.000Z",
        "availabilityDate": "2024-03-17T00:00:00.000Z",
        "authors": ["JoÃ£o Silva Santos - INTEGRAÃ‡ÃƒO"],
        "defendant": "Instituto Nacional do Seguro Social - INSS",
        "lawyers": [{"name": "Dr. Teste IntegraÃ§Ã£o", "oab": "999999"}],
        "grossValue": 100000,
        "netValue": 90000,
        "interestValue": 5000,
        "attorneyFees": 5000,
        "content": "Teste de integraÃ§Ã£o completa entre scraper e API. Aposentadoria por invalidez.",
        "status": "NOVA",
        "scrapingSource": "DJE-SP-TEST",
        "extractionMetadata": {
            "test": True,
            "integration_test": True,
            "timestamp": datetime.now().isoformat()
        }
    }
    
    async with httpx.AsyncClient() as client:
        # Teste 1: Health check da API
        print("ðŸ¥ Teste 1: Health check da API")
        try:
            response = await client.get(f"{API_BASE_URL}/health")
            if response.status_code == 200:
                print("âœ… API estÃ¡ saudÃ¡vel")
            else:
                print(f"âš ï¸  API health check: {response.status_code}")
        except Exception as e:
            print(f"âŒ Erro no health check: {e}")
            return False
        
        # Teste 2: Endpoint de scraper disponÃ­vel
        print("\nðŸ” Teste 2: Endpoint do scraper")
        try:
            response = await client.post(
                f"{API_BASE_URL}/api/scraper/publications",
                json=test_data,
                headers={"X-API-Key": SCRAPER_API_KEY}
            )
            
            if response.status_code == 201:
                print("âœ… PublicaÃ§Ã£o criada via endpoint scraper")
                result = response.json()
                publication_id = result["data"]["publication"]["id"]
                print(f"ðŸ“„ ID da publicaÃ§Ã£o: {publication_id}")
            else:
                print(f"âŒ Erro ao criar publicaÃ§Ã£o: {response.status_code}")
                print(f"ðŸ“‹ Resposta: {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ Erro na criaÃ§Ã£o: {e}")
            return False
        
        # Teste 3: Verificar se publicaÃ§Ã£o estÃ¡ disponÃ­vel
        print("\nðŸ“‹ Teste 3: Consultar publicaÃ§Ã£o criada")
        try:
            response = await client.get(
                f"{API_BASE_URL}/api/publications",
                params={"search": test_data["process_number"], "limit": 1},
                headers={"Authorization": "Bearer dummy"}  # Usar token real se necessÃ¡rio
            )
            
            if response.status_code == 200:
                result = response.json()
                if result["data"]["total"] > 0:
                    print("âœ… PublicaÃ§Ã£o encontrada na API")
                else:
                    print("âš ï¸  PublicaÃ§Ã£o nÃ£o encontrada na busca")
            else:
                print(f"âš ï¸  Erro na consulta: {response.status_code}")
                
        except Exception as e:
            print(f"âš ï¸  Erro na consulta: {e}")
        
        # Teste 4: Endpoint normal (para comparaÃ§Ã£o)
        print("\nðŸ”„ Teste 4: Endpoint normal (comparaÃ§Ã£o)")
        try:
            test_data_2 = {**test_data}
            test_data_2["process_number"] = f"NORMAL-{int(datetime.now().timestamp())}-89.2024.8.26.0100"
            
            response = await client.post(
                f"{API_BASE_URL}/api/publications",
                json=test_data_2,
                headers={"Authorization": "Bearer dummy"}  # Usar token real
            )
            
            print(f"ðŸ“Š Endpoint normal status: {response.status_code}")
            
        except Exception as e:
            print(f"ðŸ“Š Endpoint normal: {e}")
    
    print("\nðŸŽ‰ Teste de integraÃ§Ã£o concluÃ­do!")
    return True


if __name__ == "__main__":
    asyncio.run(test_full_integration())
```

### Executar Testes

```bash
# Teste de integraÃ§Ã£o completa
python test_integration.py

# Teste via Docker
docker-compose exec scraper python test_integration.py

# Logs em tempo real
docker-compose logs -f scraper
```

## ðŸ“Š Monitoramento da IntegraÃ§Ã£o

### Logs Estruturados

**Scraper** -> API:

```json
{
  "timestamp": "2024-03-17T10:30:00Z",
  "level": "INFO",
  "message": "ðŸ“¤ Enviando publicaÃ§Ã£o: 1234567-89.2024.8.26.0100",
  "source": "scraper",
  "target": "api"
}
```

**API** <- Scraper:

```json
{
  "timestamp": "2024-03-17T10:30:01Z",
  "level": "INFO", 
  "message": "Publication creation from SCRAPER",
  "process_number": "1234567-89.2024.8.26.0100",
  "source": "SCRAPER",
  "ip": "172.18.0.3"
}
```

### MÃ©tricas de IntegraÃ§Ã£o

```bash
# Via API de mÃ©tricas (se habilitada)
curl http://localhost:8000/admin/metrics

# Logs de performance
grep "Scraping concluÃ­do" logs/scraper_*.log
grep "SCRAPER" logs/api_*.log
```

## ðŸ”§ Troubleshooting

### Problemas Comuns de IntegraÃ§Ã£o

**1. Erro de ConexÃ£o Refused**

```bash
âŒ Connection refused to http://localhost:8000
```

**SoluÃ§Ã£o**:

```bash
# Verificar se API estÃ¡ rodando
curl http://localhost:8000/health

# Em Docker, usar nome do serviÃ§o
API_BASE_URL=http://juscash-api:8000
```

**2. API Key InvÃ¡lida**

```bash
âŒ Invalid API Key
```

**SoluÃ§Ã£o**:

```bash
# Verificar se as chaves sÃ£o idÃªnticas
grep SCRAPER_API_KEY backend/api/.env
grep SCRAPER_API_KEY backend/scraper/.env

# Verificar configuraÃ§Ã£o na API
curl -H "X-API-Key: sua-chave" http://localhost:8000/api/scraper/publications
```

**3. Timeout de Rede**

```bash
âŒ Timeout 30s exceeded
```

**SoluÃ§Ã£o**:

```bash
# Aumentar timeout
API_TIMEOUT=60

# Verificar latÃªncia
ping localhost
docker-compose exec scraper ping juscash-api
```

**4. Erro de ValidaÃ§Ã£o**

```bash
âŒ Process number is required
```

**SoluÃ§Ã£o**:

```bash
# Verificar estrutura dos dados
cat logs/scraper_debug.log | grep "Enviando publicaÃ§Ã£o"

# Testar com dados mÃ­nimos
python test_api_connection.py
```

### Debug AvanÃ§ado

**Capturar todas as requisiÃ§Ãµes**:

```python
# Adicionar em infrastructure/api/api_client_adapter.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Logs detalhados da API**:

```bash
# No backend/api/.env
LOG_LEVEL=debug

# Verificar logs
docker-compose logs api | grep SCRAPER
```

**AnÃ¡lise de rede**:

```bash
# Monitor de trÃ¡fego HTTP
docker-compose exec scraper tcpdump -i any port 8000

# Status de conexÃµes
docker-compose exec scraper netstat -an | grep 8000
```

## ðŸš€ Deploy em ProduÃ§Ã£o

### ConfiguraÃ§Ãµes de ProduÃ§Ã£o

**Arquivo**: `.env.production`

```bash
ENVIRONMENT=production

# API URLs seguras
API_BASE_URL=https://api.juscash.com
SCRAPER_API_KEY=sua-chave-muito-segura-aqui

# Performance
BROWSER_HEADLESS=true
SCRAPER_MAX_PAGES=50
API_TIMEOUT=60

# Logs
LOG_LEVEL=INFO
LOG_RETENTION_DAYS=30
```

### Docker Compose ProduÃ§Ã£o

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  scraper:
    image: juscash/dje-scraper:latest
    environment:
      - API_BASE_URL=https://api.juscash.com
      - SCRAPER_API_KEY=${SCRAPER_API_KEY}
      - LOG_LEVEL=INFO
    volumes:
      - /var/log/juscash:/app/logs
    restart: always
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; httpx.get('${API_BASE_URL}/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Backup e Monitoramento

```bash
# Backup de logs
tar -czf scraper-logs-$(date +%Y%m%d).tar.gz logs/

# Monitoramento com Prometheus (opcional)
curl http://localhost:8000/admin/metrics

# Alertas personalizados
grep "âŒ" logs/scraper_*.log | wc -l
```

## ðŸ“ˆ OtimizaÃ§Ãµes

### Performance

1. **Cache de PublicaÃ§Ãµes**:

  ```python
  # Implementar cache local para evitar verificaÃ§Ãµes desnecessÃ¡rias
  cache_ttl = 3600  # 1 hora
  ```

2. **Batch Processing**:

  ```python
  # Enviar mÃºltiplas publicaÃ§Ãµes por requisiÃ§Ã£o
  batch_size = 10
  ```

3. **Rate Limiting Inteligente**:

  ```python
  # Respeitar limites da API
  requests_per_minute = 100
  ```

### Escalabilidade

1. **MÃºltiplos Workers**:

  ```yaml
  # docker-compose.yml
  scraper:
    deploy:
      replicas: 3
  ```

1. **Load Balancing**:

  ```python
  # Distribuir carga entre mÃºltiplas APIs
  api_endpoints = [
      "https://api1.juscash.com",
      "https://api2.juscash.com"
  ]
  ```

---

**ðŸ”— Status da IntegraÃ§Ã£o**: âœ… Pronto para produÃ§Ã£o  
**ðŸ“Š Compatibilidade**: API JusCash v1.0+  
**ðŸš€ Deploy**: Docker Compose + Kubernetes ready
