#!/bin/bash

# Script de configuraÃ§Ã£o para ambiente de produÃ§Ã£o
# DJE Scraper - Sistema de Web Scraping

set -e  # Sair em caso de erro

echo "ðŸš€ Configurando DJE Scraper para ProduÃ§Ã£o"
echo "=========================================="

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# FunÃ§Ã£o para log colorido
log_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

log_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

log_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

log_error() {
    echo -e "${RED}âŒ $1${NC}"
}

# Verificar se Ã© root
if [[ $EUID -eq 0 ]]; then
   log_error "Este script nÃ£o deve ser executado como root"
   exit 1
fi

# Verificar Python 3.12+
log_info "Verificando Python..."
python_version=$(python3 --version 2>&1 | awk '{print $2}')
required_version="3.12"

if python3 -c "import sys; exit(0 if sys.version_info >= (3, 12) else 1)" 2>/dev/null; then
    log_success "Python $python_version OK"
else
    log_error "Python 3.12+ Ã© obrigatÃ³rio. VersÃ£o atual: $python_version"
    exit 1
fi

# Verificar Node.js (para API)
log_info "Verificando Node.js..."
if command -v node >/dev/null 2>&1; then
    node_version=$(node --version)
    log_success "Node.js $node_version OK"
else
    log_warning "Node.js nÃ£o encontrado - necessÃ¡rio para API"
fi

# Verificar Docker
log_info "Verificando Docker..."
if command -v docker >/dev/null 2>&1; then
    docker_version=$(docker --version | awk '{print $3}' | sed 's/,//')
    log_success "Docker $docker_version OK"
else
    log_error "Docker Ã© obrigatÃ³rio para produÃ§Ã£o"
    exit 1
fi

# Verificar Docker Compose
log_info "Verificando Docker Compose..."
if command -v docker-compose >/dev/null 2>&1; then
    compose_version=$(docker-compose --version | awk '{print $3}' | sed 's/,//')
    log_success "Docker Compose $compose_version OK"
else
    log_error "Docker Compose Ã© obrigatÃ³rio"
    exit 1
fi

# Criar estrutura de diretÃ³rios
log_info "Criando estrutura de diretÃ³rios..."
mkdir -p {config,logs,backups,data,scripts,docs}
mkdir -p backups/{publications,executions,logs}
mkdir -p data/{temp,cache}
log_success "Estrutura de diretÃ³rios criada"

# Configurar arquivo .env para produÃ§Ã£o
log_info "Configurando ambiente de produÃ§Ã£o..."

if [ ! -f ".env" ]; then
    log_info "Criando arquivo .env..."
    
    # Gerar API Key segura
    api_key="scraper_$(date +%Y)_$(openssl rand -hex 32)"
    
    cat > .env << EOF
# ==========================================
# DJE Scraper - ConfiguraÃ§Ã£o de ProduÃ§Ã£o
# ==========================================
ENVIRONMENT=production

# API Configuration
API_BASE_URL=http://localhost:8000
SCRAPER_API_KEY=$api_key
API_TIMEOUT=60

# Browser Configuration
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=45000
BROWSER_USER_AGENT=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36

# Scraper Configuration
SCRAPER_TARGET_URL=https://dje.tjsp.jus.br/cdje/index.do
SCRAPER_MAX_RETRIES=5
SCRAPER_RETRY_DELAY=10
SCRAPER_MAX_PAGES=50
SCRAPER_SEARCH_TERMS=aposentadoria,benefÃ­cio,INSS,previdenciÃ¡rio

# Logging Configuration
LOG_LEVEL=INFO
LOG_DIR=./logs
LOG_ROTATION_DAYS=30
LOG_MAX_SIZE_MB=50

# Performance
SCRAPER_DELAY_BETWEEN_PAGES=3
SCRAPER_DELAY_BETWEEN_REQUESTS=2
SCRAPER_MAX_CONCURRENT_PAGES=2

# Backup Configuration
BACKUP_ENABLED=true
BACKUP_RETENTION_DAYS=90
BACKUP_COMPRESS=true

# Monitoring
ENABLE_METRICS=true
ENABLE_ALERTS=true
HEALTH_CHECK_INTERVAL=300

# Email Alerts (configurar se necessÃ¡rio)
# SMTP_HOST=smtp.gmail.com
# SMTP_PORT=587
# SMTP_USER=
# SMTP_PASS=
# ALERT_EMAILS=admin@empresa.com
EOF
    
    log_success "Arquivo .env criado com API Key: ${api_key:0:20}..."
else
    log_warning "Arquivo .env jÃ¡ existe - nÃ£o sobrescrevendo"
fi

# Instalar dependÃªncias Python
log_info "Instalando dependÃªncias Python..."
if [ -f "requirements.txt" ]; then
    python3 -m pip install --user -r requirements.txt
    log_success "DependÃªncias Python instaladas"
else
    log_warning "requirements.txt nÃ£o encontrado"
fi

# Instalar browsers do Playwright
log_info "Instalando browsers do Playwright..."
python3 -m playwright install chromium --with-deps
log_success "Browsers do Playwright instalados"

# Configurar systemd service
log_info "Configurando serviÃ§o systemd..."

sudo tee /etc/systemd/system/dje-scraper.service > /dev/null << EOF
[Unit]
Description=DJE Scraper Service
After=network.target
Wants=network.target

[Service]
Type=simple
User=$USER
Group=$USER
WorkingDirectory=$(pwd)
Environment=PATH=$(which python3):$PATH
ExecStart=$(which python3) src/main.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=dje-scraper

# Security settings
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=$(pwd)/logs $(pwd)/backups $(pwd)/data

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
log_success "ServiÃ§o systemd configurado"

# Configurar logrotate
log_info "Configurando rotaÃ§Ã£o de logs..."
sudo tee /etc/logrotate.d/dje-scraper > /dev/null << EOF
$(pwd)/logs/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 644 $USER $USER
    postrotate
        systemctl reload dje-scraper || true
    endscript
}
EOF

log_success "RotaÃ§Ã£o de logs configurada"

# Configurar cron para backup
log_info "Configurando backup automÃ¡tico..."
(crontab -l 2>/dev/null; echo "0 2 * * * cd $(pwd) && python3 scraper_cli.py backup logs --days-back 1") | crontab -
(crontab -l 2>/dev/null; echo "0 3 * * 0 cd $(pwd) && python3 scraper_cli.py backup cleanup --retention-days 90") | crontab -
log_success "Backup automÃ¡tico configurado"

# Configurar monitoramento
log_info "Configurando monitoramento..."
cat > scripts/health_check.sh << 'EOF'
#!/bin/bash
# Health check script para monitoramento externo

cd "$(dirname "$0")/.."
python3 scraper_cli.py test api --api-url http://localhost:8000

if [ $? -eq 0 ]; then
    echo "OK: Scraper funcionando"
    exit 0
else
    echo "CRITICAL: Scraper com problemas"
    exit 2
fi
EOF

chmod +x scripts/health_check.sh
log_success "Script de health check criado"

# Configurar firewall (UFW)
log_info "Configurando firewall..."
if command -v ufw >/dev/null 2>&1; then
    sudo ufw allow ssh
    sudo ufw allow 8000/tcp  # API
    sudo ufw --force enable
    log_success "Firewall configurado"
else
    log_warning "UFW nÃ£o encontrado - configure firewall manualmente"
fi

# OtimizaÃ§Ãµes de sistema
log_info "Aplicando otimizaÃ§Ãµes de sistema..."

# Aumentar limites de arquivo
echo "$USER soft nofile 65536" | sudo tee -a /etc/security/limits.conf
echo "$USER hard nofile 65536" | sudo tee -a /etc/security/limits.conf

# OtimizaÃ§Ãµes de rede
sudo tee -a /etc/sysctl.conf > /dev/null << EOF

# DJE Scraper optimizations
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.ipv4.tcp_window_scaling = 1
EOF

sudo sysctl -p
log_success "OtimizaÃ§Ãµes aplicadas"

# Teste de configuraÃ§Ã£o
log_info "Testando configuraÃ§Ã£o..."
python3 scraper_cli.py config show > /dev/null
if [ $? -eq 0 ]; then
    log_success "ConfiguraÃ§Ã£o vÃ¡lida"
else
    log_error "Problema na configuraÃ§Ã£o"
    exit 1
fi

# InstruÃ§Ãµes finais
echo ""
echo "ðŸŽ‰ ConfiguraÃ§Ã£o de produÃ§Ã£o concluÃ­da!"
echo "======================================"
echo ""
echo "ðŸ“‹ PrÃ³ximos passos:"
echo "1. Configurar a API JusCash (se ainda nÃ£o estiver rodando)"
echo "2. Ajustar SCRAPER_API_KEY na API para: $api_key"
echo "3. Iniciar o serviÃ§o: sudo systemctl start dje-scraper"
echo "4. Habilitar auto-start: sudo systemctl enable dje-scraper"
echo "5. Verificar logs: journalctl -fu dje-scraper"
echo ""
echo "ðŸ”§ Comandos Ãºteis:"
echo "   Status:    sudo systemctl status dje-scraper"
echo "   Parar:     sudo systemctl stop dje-scraper"
echo "   Reiniciar: sudo systemctl restart dje-scraper"
echo "   Logs:      journalctl -fu dje-scraper"
echo "   Health:    ./scripts/health_check.sh"
echo ""
echo "ðŸ“Š Monitoramento:"
echo "   Health Check: http://localhost:8000/health"
echo "   CLI Status:   python3 scraper_cli.py status"
echo "   Alertas:      python3 scraper_cli.py alerts list"