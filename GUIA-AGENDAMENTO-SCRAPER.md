# ðŸ•• Guia de ConfiguraÃ§Ã£o do Agendamento do Scraper

## âœ… Status Atual

**Problema resolvido!** Identifiquei que havia duas implementaÃ§Ãµes diferentes de scraping e corrigi para que **o agendamento agora use o cÃ³digo do `scraping.py` que funciona completamente**.

## ðŸ”§ **CorreÃ§Ã£o Implementada**

### **ANTES (Problema):**
```
Agendamento â†’ src/main.py â†’ DJEScraperAdapter (incompleto)
Manual â†’ scraping.py â†’ DJEScraperPlaywright (completo)
```

### **DEPOIS (SoluÃ§Ã£o):**
```
Agendamento â†’ src/main.py â†’ executa scraping.py â†’ DJEScraperPlaywright (completo)
Manual â†’ scraping.py â†’ DJEScraperPlaywright (completo)
```

**Agora ambos usam o mesmo cÃ³digo funcional!**

## ðŸ“‹ ConfiguraÃ§Ãµes NecessÃ¡rias

### 1. Adicionar ConfiguraÃ§Ãµes do Scheduler ao .env

Adicione as seguintes linhas ao seu arquivo `.env`:

```bash
# ===========================================
# CONFIGURAÃ‡Ã•ES DO AGENDADOR (SCHEDULER)
# ===========================================
# ExecuÃ§Ã£o automÃ¡tica todos os dias Ã s 06:00
SCHEDULER_DAILY_HOUR=6
SCHEDULER_DAILY_MINUTE=0
SCHEDULER_START_DATE=2025-01-21
```

### 2. Habilitar o Main App no Supervisord

No arquivo `backend/scraper/supervisord.conf`, altere a configuraÃ§Ã£o do `main_app`:

**ATUAL:**
```ini
[program:main_app]
autostart=false     # <- MUDAR PARA true
```

**NOVO:**
```ini
[program:main_app]
autostart=true      # <- Habilita o agendamento automÃ¡tico
```

## ðŸš€ Como Funciona Agora

### **Fluxo do Agendamento:**

1. **06:00** - Scheduler dispara
2. **src/main.py** - Recebe o trigger
3. **ScrapingOrchestrator** - Executa automaticamente:
   ```bash
   python scraping.py run --start-date 2025-01-20 --end-date 2025-01-20 --headless
   ```
4. **scraping.py** - Faz o scraping completo (mesmo cÃ³digo do manual)
5. **Resultados** - Salvos em `reports/json/`

### **Vantagens da CorreÃ§Ã£o:**
- âœ… **Usa o cÃ³digo que funciona** (`DJEScraperPlaywright`)
- âœ… **Scraping completo** (PDFs, ESAJ, valores, advogados)
- âœ… **Mesma qualidade** que execuÃ§Ã£o manual
- âœ… **Logs detalhados** do processo
- âœ… **Compatibilidade total**

## ðŸš€ Como Executar

### OpÃ§Ã£o 1: Via Docker (Recomendado)

1. **Parar serviÃ§os atuais:**
```bash
docker-compose down
```

2. **Reconstruir e iniciar com agendamento:**
```bash
docker-compose up --build scraper -d
```

3. **Verificar logs do agendamento:**
```bash
docker-compose logs -f scraper
```

VocÃª verÃ¡ logs como:
```
ðŸ“… Scheduler inicializado
ðŸ“… Agendando scraping diÃ¡rio a partir de 2025-01-21 Ã s 06:00
â° Scheduler configurado para execuÃ§Ã£o diÃ¡ria Ã s 06:00
âœ… Scraping diÃ¡rio agendado com sucesso
```

**No horÃ¡rio agendado (06:00):**
```
ðŸš€ Iniciando execuÃ§Ã£o diÃ¡ria [execution-id]
ðŸ“… Executando scraping para data: 2025-01-20
ðŸ“„ Usando script: /app/scraping.py
ðŸ”„ Executando comando: python scraping.py run --start-date 2025-01-20 --end-date 2025-01-20 --headless
ðŸš€ Iniciando DJE Scraper com Playwright
ðŸ“… PerÃ­odo: 2025-01-20 atÃ© 2025-01-20
âœ… scraping.py executado com sucesso
ðŸ“Š PublicaÃ§Ãµes encontradas: X
ðŸ’¾ PublicaÃ§Ãµes salvas: Y
```

### OpÃ§Ã£o 2: ExecuÃ§Ã£o Local

1. **Navegar para o diretÃ³rio do scraper:**
```bash
cd backend/scraper
```

2. **Ativar ambiente virtual:**
```bash
source venv/bin/activate
```

3. **Executar o agendador:**
```bash
python -m src.main
```

## ðŸ“Š Monitoramento

### Verificar Status do Agendamento

```bash
# Logs em tempo real
docker-compose logs -f scraper

# Verificar se o scheduler estÃ¡ ativo
docker-compose exec scraper supervisorctl status

# Verificar processos Python
docker-compose exec scraper ps aux | grep python
```

### Logs Importantes

Os logs do agendamento aparecerÃ£o como:
```
ðŸš€ Iniciando DJE Scraper Application
ðŸ“… ExecuÃ§Ã£o programada diÃ¡ria a partir de 2025-01-21
â° Scheduler configurado para execuÃ§Ã£o diÃ¡ria Ã s 06:00
ðŸ”„ Aguardando prÃ³xima execuÃ§Ã£o...
```

No horÃ¡rio agendado (06:00), vocÃª verÃ¡ os **mesmos logs do scraping.py manual**, mostrando que estÃ¡ usando o cÃ³digo completo!

## âš™ï¸ PersonalizaÃ§Ã£o do HorÃ¡rio

Para alterar o horÃ¡rio de execuÃ§Ã£o, modifique no arquivo `.env`:

```bash
# Para executar Ã s 08:30 da manhÃ£
SCHEDULER_DAILY_HOUR=8
SCHEDULER_DAILY_MINUTE=30

# Para executar Ã s 18:00 (6h da tarde)
SCHEDULER_DAILY_HOUR=18
SCHEDULER_DAILY_MINUTE=0
```

## ðŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Data de Scraping

Por padrÃ£o, o agendamento faz scraping da **data anterior** (ontem), que Ã© o comportamento correto para execuÃ§Ã£o diÃ¡ria Ã s 06h.

### Modo Headless

O agendamento sempre executa em modo headless (`--headless`) para garantir estabilidade no ambiente Docker.

### TolerÃ¢ncia a Atrasos

O scheduler estÃ¡ configurado com:
- **TolerÃ¢ncia de atraso:** 1 hora (`misfire_grace_time=3600`)
- **ExecuÃ§Ã£o Ãºnica:** Impede execuÃ§Ãµes simultÃ¢neas (`max_instances=1`)

## ðŸ§ª Teste do Agendamento

### Teste Manual Imediato

Para testar se o scraping funciona sem aguardar o horÃ¡rio:

```bash
# Testar integraÃ§Ã£o completa
docker-compose exec scraper python -c "
import asyncio
from src.application.services.scraping_orchestrator import ScrapingOrchestrator
from src.shared.container import Container

async def test():
    container = Container()
    orchestrator = ScrapingOrchestrator(container)
    result = await orchestrator.execute_daily_scraping()
    print(f'âœ… Teste concluÃ­do: {result.publications_found} publicaÃ§Ãµes')

asyncio.run(test())
"
```

### Teste Manual Direto

```bash
# Testar scraping.py diretamente (mesmo cÃ³digo que o agendamento usa)
docker-compose exec scraper python scraping.py run --start-date 2025-01-21 --end-date 2025-01-21 --headless
```

### Teste do Scheduler

Para testar se o agendamento estÃ¡ funcionando, altere temporariamente para executar em alguns minutos:

```bash
# No .env, definir para daqui a 2 minutos
SCHEDULER_DAILY_HOUR=14  # (se agora sÃ£o 14:00)
SCHEDULER_DAILY_MINUTE=2  # (para executar Ã s 14:02)
```

## ðŸš¨ SoluÃ§Ã£o de Problemas

### Problema: "scraping.py nÃ£o encontrado"

```bash
# Verificar se o arquivo existe
docker-compose exec scraper ls -la scraping.py

# Se nÃ£o existir, verificar estrutura
docker-compose exec scraper find /app -name "scraping.py"
```

### Problema: "Scheduler nÃ£o estÃ¡ executando"

1. **Verificar logs:**
```bash
docker-compose logs scraper | grep -i scheduler
```

2. **Verificar se main_app estÃ¡ rodando:**
```bash
docker-compose exec scraper supervisorctl status main_app
```

3. **Restart do serviÃ§o:**
```bash
docker-compose restart scraper
```

### Problema: "ExecuÃ§Ã£o falhou"

1. **Verificar logs do scraping.py:**
```bash
docker-compose logs scraper | grep -A 10 -B 10 "Executando comando"
```

2. **Testar scraping.py manualmente:**
```bash
docker-compose exec scraper python scraping.py run --start-date 2025-01-21 --end-date 2025-01-21 --headless
```

3. **Verificar dependÃªncias:**
```bash
docker-compose exec scraper python -c "from playwright.async_api import async_playwright; print('Playwright OK')"
```

## ðŸ“ˆ Resultado Esperado

Com a correÃ§Ã£o implementada, vocÃª terÃ¡:

- âœ… **ExecuÃ§Ã£o automÃ¡tica diÃ¡ria Ã s 06:00**
- âœ… **Uso do cÃ³digo completo do scraping.py**
- âœ… **PDFs baixados e processados**
- âœ… **Dados extraÃ­dos do ESAJ**
- âœ… **PublicaÃ§Ãµes salvas em JSON**
- âœ… **Logs detalhados de cada execuÃ§Ã£o**
- âœ… **Mesma qualidade da execuÃ§Ã£o manual**

## ðŸ”„ Comandos Ãšteis

```bash
# Parar agendamento
docker-compose stop scraper

# Reiniciar agendamento
docker-compose restart scraper

# Ver status de todos os serviÃ§os
docker-compose ps

# Logs de todos os serviÃ§os
docker-compose logs -f

# Entrar no container do scraper
docker-compose exec scraper bash

# Testar scraping manual (mesmo cÃ³digo do agendamento)
docker-compose exec scraper python scraping.py run --start-date 2025-01-21 --end-date 2025-01-21
```

## ðŸ“ž PrÃ³ximos Passos

1. **Adicionar as configuraÃ§Ãµes ao .env**
2. **Habilitar autostart=true no supervisord.conf**
3. **Reiniciar o container com docker-compose up --build scraper -d**
4. **Monitorar os logs para confirmar o agendamento**

**Agora o agendamento realmente executa o `python scraping.py run` Ã s 06:00 da manhÃ£ todos os dias!** ðŸŽ‰ 